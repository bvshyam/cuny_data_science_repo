{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Resume Chatbot\n",
    "\n",
    "## Web Analytics Final project\n",
    "\n",
    "This is part of the final project on 620 Web Analytics course. Objective of this project is to create a resume chatbot for Shyam BV. This will provide most of the information about me.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we will build a deep neural network with that functions as part of an end-to-end machine translation pipeline which also uses text processing libraries. A question will be asked, an answer will be provided by bot. Also bot's ability to respond to new questions.\n",
    "\n",
    "- **Dataset Creation** - Here we will create a dataset with various text inputs\n",
    "- **Preprocess** - You'll convert text to sequence of integers with various preprocessing.\n",
    "- **Models** - A Deep neural network model will be created. By creating a word embedding, this model will be able to learn new words.\n",
    "- **Prediction** - Run the model and generate responses from bot.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "To perform this deep learning chatbot, we need to train the model with lots of data. Unfortunately my resume is not in format of QA(question answering type). So we need to generate lot of data with some existing data for this model.\n",
    " \n",
    "Here is a sample data which [I found](\n",
    "https://raw.githubusercontent.com/bvshyam/make_yourself_a_bot/master/workspace-watson.json). This will be me base to create a dataset.\n",
    "\n",
    "Over the period, due to many interactions, we will gather more data and our bot will get better over time.\n",
    "\n",
    "## Dataset creation\n",
    "\n",
    "As mentioned earlier, need create the initial dataset. That will be performed in  data_creation.ipynb\n",
    "\n",
    "## Neural Network\n",
    "\n",
    "As the model will be created using deep neural networks, we will be using Keras library with Tensorflow backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import GRU, Input, Dense, TimeDistributed, SimpleRNN, LSTM\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Dropout, Bidirectional, RepeatVector\n",
    "from keras.layers.embeddings import Embedding\n",
    "import collections\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer,text_to_word_sequence\n",
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "from autocorrect import spell\n",
    "from scipy.spatial import distance\n",
    "import time\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dataset has been created with dataset_creation.ipynb and some manual effort to correct it.Now we will load it and perform further pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tell me 5 positive things about you</td>\n",
       "      <td>Tricky question detecting. Waiting for next qu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tell me your strengths</td>\n",
       "      <td>Tricky question detecting. Waiting for next qu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tell us Unique Selling Points</td>\n",
       "      <td>Tricky question detecting. Waiting for next qu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are you good at ?</td>\n",
       "      <td>Tricky question detecting. Waiting for next qu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are your professional strengths ?</td>\n",
       "      <td>Tricky question detecting. Waiting for next qu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Question  \\\n",
       "0     Tell me 5 positive things about you   \n",
       "1                  Tell me your strengths   \n",
       "2           Tell us Unique Selling Points   \n",
       "3                  What are you good at ?   \n",
       "4  What are your professional strengths ?   \n",
       "\n",
       "                                              Answer  \n",
       "0  Tricky question detecting. Waiting for next qu...  \n",
       "1  Tricky question detecting. Waiting for next qu...  \n",
       "2  Tricky question detecting. Waiting for next qu...  \n",
       "3  Tricky question detecting. Waiting for next qu...  \n",
       "4  Tricky question detecting. Waiting for next qu...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a pandas dataframe for the input data\n",
    "\n",
    "df = pd.read_csv('./data/final_qa_data.csv',sep=',')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loaded\n"
     ]
    }
   ],
   "source": [
    "# Loading dataset to numpy array\n",
    "\n",
    "question_sentences = df.Question.values\n",
    "answer_sentences = df.Answer.values\n",
    "\n",
    "print('Dataset Loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files\n",
    "\n",
    "In each line, the first part is question and then the next part is answers. Lets see sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question Line 1:  Tell me 5 positive things about you\n",
      "Answer Line 1:  Tricky question detecting. Waiting for next question...\n",
      "Question Line 2:  Tell me your strengths\n",
      "Answer Line 2:  Tricky question detecting. Waiting for next question...\n"
     ]
    }
   ],
   "source": [
    "for sample_i in range(2):\n",
    "    print('Question Line {}:  {}'.format(sample_i + 1, question_sentences[sample_i]))\n",
    "    print('Answer Line {}:  {}'.format(sample_i + 1, answer_sentences[sample_i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From looking at the sentences, we need to perform some pre-processing. Below are some of them.\n",
    "\n",
    "- Complete string lower case. Parent vocab has only lower case characters.\n",
    "- Separate symbols and text. Else it will be considered as unique word.\n",
    "- Preserve the line ending.\n",
    "- Also replace some of the junk characters.\n",
    "- Replace won't, wouldn't etc into proper words.\n",
    "\n",
    "\n",
    "### Vocabulary\n",
    "\n",
    "Also need to build a word vocablary for the questions and answers. This will be our knowledge base. All the new questions will be stored and the vocabulary will be updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_cleaning(texts):\n",
    "    \n",
    "    text_cleaned = []\n",
    "    \n",
    "    text_cleaned = [' '.join(nltk.tokenize.word_tokenize(word.lower(),preserve_line=True)) for word in texts]\n",
    "\n",
    "    return text_cleaned\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Clean and store in the same variables.\n",
    "\n",
    "question_sentences = preprocess_cleaning(question_sentences)\n",
    "answer_sentences = preprocess_cleaning(answer_sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nex we will analyze the overall dataset and draw some inference out of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1124 words in questions.\n",
      "288 unique English words.\n",
      "10 Most common words in the questions dataset:\n",
      "\"you\" \"?\" \"what\" \"do\" \"are\" \"your\" \"how\" \"i\" \"'s\" \"tell\"\n",
      "\n",
      "3361 words in answers.\n",
      "292 unique English words.\n",
      "10 Most common words in the answers dataset:\n",
      "\"i\" \"you\" \":\" \"to\" \",\" \"my\" \"...\" \"a\" \"and\" \".\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "question_words_counter = collections.Counter([word for sentence in question_sentences for word in sentence.split()])\n",
    "answer_words_counter = collections.Counter([word for sentence in answer_sentences for word in sentence.split()])\n",
    "\n",
    "print('{} words in questions.'.format(len([word for sentence in question_sentences for word in sentence.split()])))\n",
    "print('{} unique English words.'.format(len(question_words_counter)))\n",
    "print('10 Most common words in the questions dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*question_words_counter.most_common(10)))[0]) + '\"')\n",
    "print()\n",
    "print('{} words in answers.'.format(len([word for sentence in answer_sentences for word in sentence.split()])))\n",
    "print('{} unique English words.'.format(len(answer_words_counter)))\n",
    "print('10 Most common words in the answers dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*answer_words_counter.most_common(10)))[0]) + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('you', 110),\n",
       " ('?', 102),\n",
       " ('what', 66),\n",
       " ('do', 38),\n",
       " ('are', 36),\n",
       " ('your', 31),\n",
       " ('how', 27),\n",
       " ('i', 19),\n",
       " (\"'s\", 19),\n",
       " ('tell', 17),\n",
       " ('me', 17),\n",
       " ('about', 17),\n",
       " ('to', 16),\n",
       " ('a', 15),\n",
       " ('is', 15),\n",
       " ('can', 13),\n",
       " ('work', 13),\n",
       " ('the', 13),\n",
       " ('have', 12),\n",
       " ('where', 10)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Most common top 20 word.\n",
    "\n",
    "question_words_counter.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess\n",
    "\n",
    "As a next step we will perform below ones.\n",
    "1. Tokenize the words into ids\n",
    "2. Add padding to make all the sequences the same length.\n",
    "\n",
    "\n",
    "### Tokenize (IMPLEMENTATION)\n",
    "For a neural network to predict on text data, it first has to be turned into data it can understand. \n",
    "\n",
    "A word level model uses word ids that generate text predictions for each word. Also we will use the pre-trained word embedding called as [Glove](https://nlp.stanford.edu/projects/glove/)\n",
    "\n",
    "Turn each sentence into a sequence of words ids using Keras's [`Tokenizer`](https://keras.io/preprocessing/text/#tokenizer) function. Using this function we will tokenize `question_sentences` and `answer_sentences` in the cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tell me 5 positive things about you', 'tell me your strengths', 'tell us unique selling points'] \n",
      "\n",
      "{'tell': 1, 'me': 2, '5': 3, 'positive': 4, 'things': 5, 'about': 6, 'you': 7, 'your': 8, 'strengths': 9, 'us': 10, 'unique': 11, 'selling': 12, 'points': 13}\n",
      "\n",
      "Sequence 1 in x\n",
      "  Input:  tell me 5 positive things about you\n",
      "  Output: [1, 2, 3, 4, 5, 6, 7]\n",
      "Sequence 2 in x\n",
      "  Input:  tell me your strengths\n",
      "  Output: [1, 2, 8, 9]\n",
      "Sequence 3 in x\n",
      "  Input:  tell us unique selling points\n",
      "  Output: [1, 10, 11, 12, 13]\n"
     ]
    }
   ],
   "source": [
    "def tokenize(x):\n",
    "    \"\"\"\n",
    "    Tokenize x\n",
    "    param x: List of sentences/strings to be tokenized\n",
    "    return: Tuple of (tokenized x data, tokenizer used to tokenize x)\n",
    "    \"\"\"\n",
    "    # convert to nltk tokenizer to preserve the symbols and tokenize\n",
    "    x = [' '.join(nltk.tokenize.word_tokenize(word.lower(),preserve_line=True)) for word in x]\n",
    "\n",
    "    #Use tokenizer from Keras\n",
    "    tokenizer = Tokenizer(num_words=None, filters=\"\", lower=True, split=\" \")\n",
    "    tokenizer.fit_on_texts(x)\n",
    "    return tokenizer.texts_to_sequences(x), tokenizer\n",
    "\n",
    "\n",
    "# Tokenize sample output\n",
    "\n",
    "text_sentences = question_sentences[:3]\n",
    "text_tokenized, text_tokenizer = tokenize(text_sentences)\n",
    "print(text_sentences,\"\\n\")\n",
    "print(text_tokenizer.word_index)\n",
    "print()\n",
    "\n",
    "for sample_i, (sent, token_sent) in enumerate(zip(text_sentences, text_tokenized)):\n",
    "    print('Sequence {} in x'.format(sample_i + 1))\n",
    "    print('  Input:  {}'.format(sent))\n",
    "    print('  Output: {}'.format(token_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding\n",
    "\n",
    "When batching the sequence of word ids together, each sequence needs to be the same length.  Since sentences are dynamic in length, we can add padding to the end of the sequences to make them the same length.\n",
    "\n",
    "Make sure all the English sequences have the same length and all the French sequences have the same length by adding padding to the **end** of each sequence using Keras's [`pad_sequences`](https://keras.io/preprocessing/sequence/#pad_sequences) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 1 in x\n",
      "  Input:  [1 2 3 4 5 6 7]\n",
      "  Output: [1 2 3 4 5 6 7]\n",
      "Sequence 2 in x\n",
      "  Input:  [1 2 8 9]\n",
      "  Output: [1 2 8 9 0 0 0]\n",
      "Sequence 3 in x\n",
      "  Input:  [ 1 10 11 12 13]\n",
      "  Output: [ 1 10 11 12 13  0  0]\n"
     ]
    }
   ],
   "source": [
    "def pad(x, length=None):\n",
    "    \"\"\"\n",
    "    Pad x\n",
    "    param x: List of sequences.\n",
    "    param length: Length to pad the sequence to.  If None, use length of longest sequence in x.\n",
    "    return: Padded numpy array of sequences\n",
    "    \"\"\"\n",
    "    if length==None:\n",
    "        maxLenX = 0\n",
    "        for sequence in x:\n",
    "            if len(sequence) > maxLenX:\n",
    "                maxLenX = len(sequence)\n",
    "        \n",
    "        padded = pad_sequences(sequences=x,maxlen=maxLenX, dtype='int32', padding='post', truncating='post', value=0)\n",
    "\n",
    "    else:\n",
    "        padded = pad_sequences(sequences=x,maxlen=length, dtype='int32', padding='post', truncating='post', value=0)\n",
    "    return padded\n",
    "\n",
    "\n",
    "# Pad Tokenized output\n",
    "test_pad = pad(text_tokenized)\n",
    "for sample_i, (token_sent, pad_sent) in enumerate(zip(text_tokenized, test_pad)):\n",
    "    print('Sequence {} in x'.format(sample_i + 1))\n",
    "    print('  Input:  {}'.format(np.array(token_sent)))\n",
    "    print('  Output: {}'.format(pad_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Pipeline\n",
    "\n",
    "As a next step we will call the above functions for the questions and answers and store those tokenizer and outputs for further usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preprocessed\n"
     ]
    }
   ],
   "source": [
    "def preprocess(x, y):\n",
    "    \"\"\"\n",
    "    Preprocess x and y\n",
    "    :param x: Feature List of sentences\n",
    "    :param y: Feature output List of sentences\n",
    "    :return: Tuple of (Preprocessed x, Preprocessed y, x tokenizer, y tokenizer)\n",
    "    \"\"\"\n",
    "    preprocess_x, x_tk = tokenize(x)\n",
    "    preprocess_y, y_tk = tokenize(y)\n",
    "\n",
    "    preprocess_x = pad(preprocess_x)\n",
    "    preprocess_y = pad(preprocess_y)\n",
    "        \n",
    "    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n",
    "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
    "\n",
    "    return preprocess_x, preprocess_y, x_tk, y_tk\n",
    "\n",
    "preproc_question_sentences, preproc_answer_sentences, question_tokenizer, answer_tokenizer =\\\n",
    "    preprocess(question_sentences, answer_sentences)\n",
    "\n",
    "print('Data Preprocessed')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove Embedding \n",
    "\n",
    "As a next step we will try Glove word embedding. Here we want to convert the preprocessed padded word sentences to a word vector with dimentions. By performing this, we know the relationship between the words. \n",
    "\n",
    "For this project, we have used a simple word embedding which is of 100 dimensions. Means each word is converted to a 100 dimension vector. That will be one of the input to Embedding layer.\n",
    "\n",
    "We will assign the word embedding vector to each vocablary in the questions layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 400000 word vectors\n"
     ]
    }
   ],
   "source": [
    "#word embeddings from glove\n",
    "\n",
    "embeddings_index = dict()\n",
    "\n",
    "with open('./data/glove.6B.100d.txt',encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word]= coefs\n",
    "\n",
    "print('loaded %s word vectors' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create an encoder matrix with all the vocablary in questions.\n",
    "\n",
    "num_question_tokens = len(question_tokenizer.word_index)+1\n",
    "\n",
    "#Create a dummy matrix to store all the word embeddings\n",
    "encoder_embedding_matrix = np.zeros((num_question_tokens, 100))\n",
    "\n",
    "# List to find nearest words for unknown words in current vocab list\n",
    "unknown_helper_list = []\n",
    "\n",
    "for word,i in question_tokenizer.word_index.items():\n",
    "    \n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    unknown_helper_list.append(word)\n",
    "\n",
    "    if embedding_vector is not None:\n",
    "        encoder_embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "#Final matrix is the collection of vectors for the complete questions vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Implementation\n",
    "\n",
    "As a next step, we will build a deep neural network model. This will be a series of different layers. We will specify some hyper parameters and then use it to train our model.\n",
    "\n",
    "Below are the different layers of deep neural network\n",
    "\n",
    "1. Embedding layer which acts as input layer and gets input from embeddings vector.\n",
    "2. Create a Bidirectional LSTM layer.\n",
    "3. Add some random dropout neurons. Repeat Step 2 and 3.\n",
    "4. Create an output layer with sigmoid activation function.\n",
    "5. Finally compile the model with optimizer and accuracy metrics.\n",
    "6. Fit the model with training questions and answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Learning rate for optimizer\n",
    "learning_rate = 0.01\n",
    "# Hidden layers for LSTM layers\n",
    "hidden_dim = 256\n",
    "# Word embedding vector length\n",
    "embedding_vector_length = 100\n",
    "# Number of epochs for complete training\n",
    "epochs =1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Reshaping the questions input to work with a basic RNN with output shape\n",
    "tmp_x = pad(preproc_question_sentences, preproc_answer_sentences.shape[1])\n",
    "tmp_x = tmp_x.reshape((-1, preproc_answer_sentences.shape[-2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Assign to the variables\n",
    "input_shape = tmp_x.shape\n",
    "output_sequence_length = preproc_answer_sentences.shape[1]\n",
    "question_vocab_size = len(question_tokenizer.word_index)+1\n",
    "answer_vocab_size= len(answer_tokenizer.word_index)+1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ids Back to Text\n",
    "\n",
    "The neural network will be translating the input to words ids, which isn't the final form we want.  We want the answers for that question.  Below functions  will bridge the gab between the logits from the neural network to the answers. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 39, 100)           28900     \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 39, 512)           731136    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 39, 512)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 39, 512)           1574912   \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 39, 512)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 39, 512)           1574912   \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 39, 512)           0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 39, 293)           150309    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 39, 293)           0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 39, 293)           0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 39, 293)           86142     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 39, 293)           0         \n",
      "=================================================================\n",
      "Total params: 4,146,311\n",
      "Trainable params: 4,117,411\n",
      "Non-trainable params: 28,900\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Load models\n",
    "model = load_model('./saved_models/resume_chatbot_save_3000.hdf5')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def similar_word(unknown_word_dim):\n",
    "    \"\"\"\n",
    "    To find the similar if a typed word is not available in questions vocab. \n",
    "    Here we are finding the nearest word using euclidean distance \n",
    "    and perform the approximate word  which is similar to it.\n",
    "    :param unknown_word_dim: unknown word entered in the text\n",
    "    \"\"\"\n",
    "    all_distance = []\n",
    "\n",
    "    for known_word in encoder_embedding_matrix:\n",
    "        all_distance.append(distance.euclidean(unknown_word_dim,known_word))\n",
    "    \n",
    "    #Get the minimum distance using argsort\n",
    "    return(unknown_helper_list[np.array(all_distance).argsort()[:1][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_test(raw_word, question_tokenizer):\n",
    "    \"\"\"\n",
    "    Preprocess the text which is entered by user. We need to remove and clean \n",
    "    the text before we predict the answer.\n",
    "    :param raw_word: Raw sentence entered by the user.\n",
    "    :param question_tokenizer: Question tokenizer for vocab\n",
    "    \"\"\"\n",
    "    \n",
    "    # Cleaning the text\n",
    "    l1 = ['won’t','won\\'t','wouldn’t','wouldn\\'t','’m', '’re', '’ve', '’ll', '’s','’d', 'n’t', '\\'m', '\\'re', '\\'ve', '\\'ll', '\\'s', '\\'d', 'can\\'t', 'n\\'t', 'B: ', 'A: ', ',', ';', '.', '?', '!', ':', '. ?', ',   .', '. ,', 'EOS', 'BOS', 'eos', 'bos']\n",
    "    l2 = ['will not','will not','would not','would not',' am', ' are', ' have', ' will', ' is', ' had', ' not', ' am', ' are', ' have', ' will', ' is', ' had', 'can not', ' not', '', '', ' ,', ' ;', ' .', ' ?', ' !', ' :', '? ', '.', ',', '', '', '', '']\n",
    "\n",
    "    raw_word = raw_word.lower()\n",
    "\n",
    "    for j, term in enumerate(l1):\n",
    "        raw_word = raw_word.replace(term,l2[j])\n",
    "       \n",
    "    for j in range(30):\n",
    "        raw_word = raw_word.replace('. .', '')\n",
    "        raw_word = raw_word.replace('.  .', '')\n",
    "        raw_word = raw_word.replace('..', '')\n",
    "        raw_word = raw_word.replace('...', '')\n",
    "        \n",
    "    for j in range(5):\n",
    "        raw_word = raw_word.replace('  ', ' ')\n",
    " \n",
    "    #Spell checker and call similar words function\n",
    "\n",
    "    final_corrected_words  = []\n",
    "    \n",
    "    for text in nltk.tokenize.word_tokenize(raw_word.lower(),preserve_line=True):\n",
    "        text = text.lower()\n",
    "        #Spell checker changes the symbols. So passing only strings.\n",
    "        if text not in '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n':\n",
    "            #Spell checker\n",
    "            text = spell(text).lower()\n",
    "            \n",
    "            # Finding unknown similar words from the vocab\n",
    "            if text not in question_tokenizer.word_index.keys():\n",
    "                \n",
    "                try:\n",
    "                    final_corrected_words.append(similar_word(embeddings_index[text]))\n",
    "                except:\n",
    "                    final_corrected_words.append('?')\n",
    "            else:\n",
    "                final_corrected_words.append(text)\n",
    "        else:\n",
    "           \n",
    "            if text not in question_tokenizer.word_index.keys():\n",
    "                final_corrected_words.append(similar_word(embeddings_index[text]))\n",
    "            else:\n",
    "                final_corrected_words.append(text)\n",
    "            \n",
    "    #print(' '.join(final_corrected_words))\n",
    "    return ' '.join(final_corrected_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Hi ! please type your name.\n",
      "\n",
      "user: Shyam\n",
      "Chatbot: hi , Shyam ! My name is Jammy AI.\n",
      "\n",
      "How are you?\n",
      "i 'm perfectly fine. the servers are paid and in case of network failure i 'll automatically be available from a different region .               \n",
      "\n",
      "what can you do?\n",
      "think of me as shyam at an interview. ask me `` interview '' related questions. what books i read , what have i been doing lately or to tell you a joke .      \n",
      "\n",
      "What have you worked?\n",
      "think open source projects. you can see them on github : https : //github.com/bvshyam/. this bot is also an achievement : )                 \n",
      "\n",
      "connect with shyam\n",
      "i have no words for old : )                               \n",
      "\n",
      "how old are you?\n",
      "... i 'm 31 years old : )                               \n",
      "\n",
      "exit\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "que = ''\n",
    "last_query  = ' '\n",
    "last_last_query = ''\n",
    "text = ' '\n",
    "last_text = ''\n",
    "name_of_computer = 'Jammy AI'\n",
    "\n",
    "# Open files to save the conversation for further training:\n",
    "\n",
    "name = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "qf = open('./session_data/'+name+'.txt', 'w')\n",
    "\n",
    "\n",
    "def final_predictions(model, x, y, x_tk, y_tk):\n",
    "    \"\"\"\n",
    "    Gets predictions using the final model\n",
    "    :param x: Preprocessed English data\n",
    "    :param y: Preprocessed French data\n",
    "    :param x_tk: Questions tokenizer\n",
    "    :param y_tk: Answers tokenizer\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Create a answer dictionary\n",
    "    y_id_to_word = {value: key for key, value in y_tk.word_index.items()}\n",
    "    y_id_to_word[0] = '<PAD>'\n",
    "    \n",
    "\n",
    "    print('Chatbot: Hi ! please type your name.\\n')\n",
    "    name = input('user: ')\n",
    "    print('Chatbot: hi , ' + name +' ! My name is ' + name_of_computer + '.\\n') \n",
    "\n",
    "    \n",
    "    while(True):\n",
    "        \n",
    "        que = input()\n",
    "        \n",
    "        qf.write(\"Question typed:\" + que + '\\n')\n",
    "        \n",
    "        if que =='exit':\n",
    "            break\n",
    "        else:\n",
    "            #Preprocess the text\n",
    "            que = preprocess_test(que,x_tk)\n",
    "            #print(que)\n",
    "        sentence =que\n",
    "        \n",
    "        qf.write(\"Question interpreted:\" + que + '\\n')\n",
    "        \n",
    "        #sentence = [x_tk.word_index[word] for word in text_to_word_sequence(sentence,filters='')]\n",
    "        \n",
    "        sentence = [x_tk.word_index[word] for word in nltk.tokenize.word_tokenize(sentence.lower())]\n",
    "        \n",
    "        #Convert to padded sequence\n",
    "        sentence = pad_sequences([sentence], maxlen=x.shape[-1], padding='post')\n",
    "        sentences = np.array([sentence[0], x[0]])\n",
    "        \n",
    "        #print(sentences.shape)\n",
    "        \n",
    "        tmp_sentences = pad(sentences, y.shape[1])\n",
    "        tmp_sentences = tmp_sentences.reshape((-1, y.shape[-2]))\n",
    "\n",
    "        #print(tmp_sentences)\n",
    "        predictions = model.predict(tmp_sentences, len(tmp_sentences))\n",
    "\n",
    "        #print('Sample 1:')\n",
    "        prediction_text = ' '.join([y_id_to_word[np.argmax(x)] for x in predictions[0]]).replace('<PAD>','')\n",
    "        \n",
    "        qf.write(\"Answer by bot:\" + prediction_text + '\\n')\n",
    "        print(prediction_text+'\\n')\n",
    "\n",
    "    qf.close()\n",
    "\n",
    "final_predictions(model,preproc_question_sentences, preproc_answer_sentences, question_tokenizer, answer_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Summary\n",
    "\n",
    "### Learnings\n",
    "\n",
    "1. We have successfully created a dataset with the existing data sources with automated and manual effort.\n",
    "2. Preprocessed the dataset and performed tokenization, padding, cleaning symbols, creating word dictinary.\n",
    "3. Loaded an pretrained word embeddings matrix(GLove) and used it to convert the vocab to a word embeddings 100 dimention matrix.\n",
    "4. Create a deep neural network model and input questions and answers to train the neural network.\n",
    "5. As the training takes longer time, we will run the training on a 8 GB GPU+ memory with 8 core, 30 GB RAM. It took ~2 hours to run 2500 epochs.\n",
    "6. Next step is to predict the new sentances which are typed. Lot of preprocessing needs to be performed on user entered text.\n",
    "7. Clean any symbols and tokenize the user input text. Also convert it to vector format.\n",
    "8. Finally all the input text, interpreted text and response text is stored in text file with date stamp for future training.\n",
    "\n",
    "### Flask App:\n",
    "\n",
    "1. There is also an flask app which was created for this interaction with the user. It is shown in the video and code.\n",
    "\n",
    "### Potential exploration and development:\n",
    "\n",
    "Although this chatbot is providing some answers, there is a lot of space for improvements. Some of them are .\n",
    "\n",
    "1. Chatbot response in a proper grammatically correct way.\n",
    "2. Perform more preprocessing on the input text and finding correct match words for the unknown words.\n",
    "3. Create a sequence to sequence encoder-decoder model using bidirectional RNN. That will provide better results. I have tried to implement it. But require some  more time for deep exploration. \n",
    "4. Better UI for the chatbot. Currently UI is very simple and have minor issues in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
