{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Artificial Intelligence Nanodegree\n",
    "## Machine Translation Project\n",
    "In this notebook, sections that end with **'(IMPLEMENTATION)'** in the header indicate that the following blocks of code will require additional functionality which you must provide. Please be sure to read the instructions carefully!\n",
    "\n",
    "## Introduction\n",
    "In this notebook, you will build a deep neural network that functions as part of an end-to-end machine translation pipeline. Your completed pipeline will accept English text as input and return the French translation.\n",
    "\n",
    "- **Preprocess** - You'll convert text to sequence of integers.\n",
    "- **Models** Create models which accepts a sequence of integers as input and returns a probability distribution over possible translations. After learning about the basic types of neural networks that are often used for machine translation, you will engage in your own investigations, to design your own model!\n",
    "- **Prediction** Run the model on English text.\n",
    "\n",
    "## Dataset\n",
    "We begin by investigating the dataset that will be used to train and evaluate your pipeline.  The most common datasets used for machine translation are from [WMT](http://www.statmt.org/).  However, that will take a long time to train a neural network on.  We'll be using a dataset we created for this project that contains a small vocabulary.  You'll be able to train your model in a reasonable time with this dataset.\n",
    "### Load Data\n",
    "The data is located in `data/small_vocab_en` and `data/small_vocab_fr`. The `small_vocab_en` file contains English sentences with their French translations in the `small_vocab_fr` file. Load the English and French data from these files from running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import GRU, Input, Dense, TimeDistributed, SimpleRNN, LSTM\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Dropout, Bidirectional, RepeatVector\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "df = pd.read_csv('./data/jokes.csv',sep=',',nrows=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loaded\n"
     ]
    }
   ],
   "source": [
    "#english_sentences = [word.lower() for sent in df.Question.values for word in nltk.tokenize.word_tokenize(sent)]\n",
    "#french_sentences = [' '.join(word.lower()) for sent in df.Answer.values for word in nltk.tokenize.word_tokenize(sent)]\n",
    "english_sentences = df.Question.values\n",
    "french_sentences = df.Answer.values\n",
    "\n",
    "print('Dataset Loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(french_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files\n",
    "Each line in `small_vocab_en` contains an English sentence with the respective translation in each line of `small_vocab_fr`.  View the first two lines from each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small_vocab_en Line 1:  Did you hear about the Native American man that drank 200 cups of tea?\n",
      "small_vocab_fr Line 1:  He nearly drown in his own tea pee.\n",
      "small_vocab_en Line 2:  What's the best anti diarrheal prescription?\n",
      "small_vocab_fr Line 2:  Mycheexarphlexin\n"
     ]
    }
   ],
   "source": [
    "for sample_i in range(2):\n",
    "    print('small_vocab_en Line {}:  {}'.format(sample_i + 1, english_sentences[sample_i]))\n",
    "    print('small_vocab_fr Line {}:  {}'.format(sample_i + 1, french_sentences[sample_i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From looking at the sentences, you can see they have been preprocessed already.  The puncuations have been delimited using spaces. All the text have been converted to lowercase.  This should save you some time, but the text requires more preprocessing.\n",
    "### Vocabulary\n",
    "The complexity of the problem is determined by the complexity of the vocabulary.  A more complex vocabulary is a more complex problem.  Let's look at the complexity of the dataset we'll be working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8850 English words.\n",
      "2706 unique English words.\n",
      "10 Most common words in the English dataset:\n",
      "\"a\" \"the\" \"What\" \"do\" \"you\" \"Why\" \"to\" \"call\" \"and\" \"did\"\n",
      "\n",
      "6173 French words.\n",
      "2763 unique French words.\n",
      "10 Most common words in the French dataset:\n",
      "\"the\" \"a\" \"to\" \"A\" \"Because\" \"I\" \"and\" \"it\" \"you\" \"in\"\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "\n",
    "english_words_counter = collections.Counter([word for sentence in english_sentences for word in sentence.split()])\n",
    "french_words_counter = collections.Counter([word for sentence in french_sentences for word in sentence.split()])\n",
    "\n",
    "print('{} English words.'.format(len([word for sentence in english_sentences for word in sentence.split()])))\n",
    "print('{} unique English words.'.format(len(english_words_counter)))\n",
    "print('10 Most common words in the English dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*english_words_counter.most_common(10)))[0]) + '\"')\n",
    "print()\n",
    "print('{} French words.'.format(len([word for sentence in french_sentences for word in sentence.split()])))\n",
    "print('{} unique French words.'.format(len(french_words_counter)))\n",
    "print('10 Most common words in the French dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*french_words_counter.most_common(10)))[0]) + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 471),\n",
       " ('the', 450),\n",
       " ('What', 403),\n",
       " ('do', 312),\n",
       " ('you', 310),\n",
       " ('Why', 189),\n",
       " ('to', 152),\n",
       " ('call', 147),\n",
       " ('and', 143),\n",
       " ('did', 140),\n",
       " ('in', 128),\n",
       " (\"What's\", 105),\n",
       " ('How', 101),\n",
       " ('is', 96),\n",
       " ('of', 95),\n",
       " ('does', 85),\n",
       " ('say', 75),\n",
       " ('between', 70),\n",
       " ('difference', 64),\n",
       " ('when', 61)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_words_counter.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, _Alice's Adventures in Wonderland_ contains 2,766 unique words of a total of 15,500 words.\n",
    "## Preprocess\n",
    "For this project, you won't use text data as input to your model. Instead, you'll convert the text into sequences of integers using the following preprocess methods:\n",
    "1. Tokenize the words into ids\n",
    "2. Add padding to make all the sequences the same length.\n",
    "\n",
    "Time to start preprocessing the data...\n",
    "### Tokenize (IMPLEMENTATION)\n",
    "For a neural network to predict on text data, it first has to be turned into data it can understand. Text data like \"dog\" is a sequence of ASCII character encodings.  Since a neural network is a series of multiplication and addition operations, the input data needs to be number(s).\n",
    "\n",
    "We can turn each character into a number or each word into a number.  These are called character and word ids, respectively.  Character ids are used for character level models that generate text predictions for each character.  A word level model uses word ids that generate text predictions for each word.  Word level models tend to learn better, since they are lower in complexity, so we'll use those.\n",
    "\n",
    "Turn each sentence into a sequence of words ids using Keras's [`Tokenizer`](https://keras.io/preprocessing/text/#tokenizer) function. Use this function to tokenize `english_sentences` and `french_sentences` in the cell below.\n",
    "\n",
    "Running the cell will run `tokenize` on sample data and show output for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'.': 1, 'quick': 2, 'a': 3, 'The': 4, 'brown': 5, 'fox': 6, 'jumps': 7, 'over': 8, 'the': 9, 'lazy': 10, 'dog': 11, 'By': 12, 'Jove': 13, ',': 14, 'my': 15, 'study': 16, 'of': 17, 'lexicography': 18, 'won': 19, 'prize': 20, 'This': 21, 'is': 22, 'short': 23, 'sentence': 24}\n",
      "\n",
      "Sequence 1 in x\n",
      "  Input:  The quick brown fox jumps over the lazy dog .\n",
      "  Output: [4, 2, 5, 6, 7, 8, 9, 10, 11, 1]\n",
      "Sequence 2 in x\n",
      "  Input:  By Jove , my quick study of lexicography won a prize .\n",
      "  Output: [12, 13, 14, 15, 2, 16, 17, 18, 19, 3, 20, 1]\n",
      "Sequence 3 in x\n",
      "  Input:  This is a short sentence .\n",
      "  Output: [21, 22, 3, 23, 24, 1]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n",
    "def tokenize(x):\n",
    "    \"\"\"\n",
    "    Tokenize x\n",
    "    :param x: List of sentences/strings to be tokenized\n",
    "    :return: Tuple of (tokenized x data, tokenizer used to tokenize x)\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=None, filters=\"\", lower=False, split=\" \")\n",
    "    tokenizer.fit_on_texts(x)\n",
    "    return tokenizer.texts_to_sequences(x), tokenizer\n",
    "\n",
    "\n",
    "# Tokenize Example output\n",
    "text_sentences = [\n",
    "    'The quick brown fox jumps over the lazy dog .',\n",
    "    'By Jove , my quick study of lexicography won a prize .',\n",
    "    'This is a short sentence .']\n",
    "text_tokenized, text_tokenizer = tokenize(text_sentences)\n",
    "print(text_tokenizer.word_index)\n",
    "print()\n",
    "for sample_i, (sent, token_sent) in enumerate(zip(text_sentences, text_tokenized)):\n",
    "    print('Sequence {} in x'.format(sample_i + 1))\n",
    "    print('  Input:  {}'.format(sent))\n",
    "    print('  Output: {}'.format(token_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding (IMPLEMENTATION)\n",
    "When batching the sequence of word ids together, each sequence needs to be the same length.  Since sentences are dynamic in length, we can add padding to the end of the sequences to make them the same length.\n",
    "\n",
    "Make sure all the English sequences have the same length and all the French sequences have the same length by adding padding to the **end** of each sequence using Keras's [`pad_sequences`](https://keras.io/preprocessing/sequence/#pad_sequences) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 1 in x\n",
      "  Input:  [ 4  2  5  6  7  8  9 10 11  1]\n",
      "  Output: [ 4  2  5  6  7  8  9 10 11  1  0  0  0  0  0]\n",
      "Sequence 2 in x\n",
      "  Input:  [12 13 14 15  2 16 17 18 19  3 20  1]\n",
      "  Output: [12 13 14 15  2 16 17 18 19  3 20  1  0  0  0]\n",
      "Sequence 3 in x\n",
      "  Input:  [21 22  3 23 24  1]\n",
      "  Output: [21 22  3 23 24  1  0  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "def pad(x, length=15):\n",
    "    \"\"\"\n",
    "    Pad x\n",
    "    :param x: List of sequences.\n",
    "    :param length: Length to pad the sequence to.  If None, use length of longest sequence in x.\n",
    "    :return: Padded numpy array of sequences\n",
    "    \"\"\"\n",
    "    if length==None:\n",
    "        maxLenX = 0\n",
    "        for sequence in x:\n",
    "            if len(sequence) > maxLenX:\n",
    "                maxLenX = len(sequence)\n",
    "        \n",
    "        padded = pad_sequences(sequences=x,maxlen=maxLenX, dtype='int32', padding='post', truncating='post', value=0)\n",
    "\n",
    "    else:\n",
    "        padded = pad_sequences(sequences=x,maxlen=length, dtype='int32', padding='post', truncating='post', value=0)\n",
    "    return padded\n",
    "\n",
    "\n",
    "# Pad Tokenized output\n",
    "test_pad = pad(text_tokenized)\n",
    "for sample_i, (token_sent, pad_sent) in enumerate(zip(text_tokenized, test_pad)):\n",
    "    print('Sequence {} in x'.format(sample_i + 1))\n",
    "    print('  Input:  {}'.format(np.array(token_sent)))\n",
    "    print('  Output: {}'.format(pad_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Pipeline\n",
    "Your focus for this project is to build neural network architecture, so we won't ask you to create a preprocess pipeline.  Instead, we've provided you with the implementation of the `preprocess` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 15)\n",
      "Data Preprocessed\n"
     ]
    }
   ],
   "source": [
    "def preprocess(x, y):\n",
    "    \"\"\"\n",
    "    Preprocess x and y\n",
    "    :param x: Feature List of sentences\n",
    "    :param y: Label List of sentences\n",
    "    :return: Tuple of (Preprocessed x, Preprocessed y, x tokenizer, y tokenizer)\n",
    "    \"\"\"\n",
    "    preprocess_x, x_tk = tokenize(x)\n",
    "    preprocess_y, y_tk = tokenize(y)\n",
    "\n",
    "    preprocess_x = pad(preprocess_x)\n",
    "    preprocess_y = pad(preprocess_y)\n",
    "    \n",
    "    print(preprocess_y.shape)\n",
    "    \n",
    "    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n",
    "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
    "\n",
    "    return preprocess_x, preprocess_y, x_tk, y_tk\n",
    "\n",
    "preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer =\\\n",
    "    preprocess(english_sentences, french_sentences)\n",
    "\n",
    "print('Data Preprocessed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 400000 word vectors\n"
     ]
    }
   ],
   "source": [
    "# Embedding matrix\n",
    "\n",
    "#word embeddings from glove\n",
    "\n",
    "embeddings_index = dict()\n",
    "\n",
    "with open('./data/glove.6B.100d.txt',encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word]= coefs\n",
    "\n",
    "print('loaded %s word vectors' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder matrix\n",
    "num_english_tokens = len(english_tokenizer.word_index)+1\n",
    "\n",
    "encoder_embedding_matrix = np.zeros((num_english_tokens, 100))\n",
    "\n",
    "for word,i in english_tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        encoder_embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.27085999,  0.044006  , -0.02026   , ..., -0.4923    ,\n",
       "         0.63687003,  0.23642001],\n",
       "       [-0.038194  , -0.24487001,  0.72812003, ..., -0.1459    ,\n",
       "         0.82779998,  0.27061999],\n",
       "       ..., \n",
       "       [ 0.061859  ,  0.10992   ,  0.27384001, ..., -0.20844001,\n",
       "         0.10268   , -0.17731   ],\n",
       "       [ 0.38402   ,  0.11894   , -0.11175   , ...,  0.20793   ,\n",
       "         0.37232   , -0.011673  ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "hidden_dim = 256\n",
    "embedding_vector_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Reshaping the input to work with a basic RNN\n",
    "tmp_x = pad(preproc_english_sentences, preproc_french_sentences.shape[1])\n",
    "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_shape =tmp_x.shape\n",
    "output_sequence_length =preproc_french_sentences.shape[1]\n",
    "english_vocab_size =len(english_tokenizer.word_index)+1\n",
    "french_vocab_size=len(french_tokenizer.word_index)+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 15, 100)           270700    \n",
      "_________________________________________________________________\n",
      "bidirectional_10 (Bidirectio (None, 15, 512)           731136    \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 15, 512)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_11 (Bidirectio (None, 15, 512)           1574912   \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 15, 512)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_12 (Bidirectio (None, 15, 512)           1574912   \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 15, 512)           0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 15, 2764)          1417932   \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 15, 2764)          0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 15, 2764)          0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 15, 2764)          7642460   \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 15, 2764)          0         \n",
      "=================================================================\n",
      "Total params: 13,212,052\n",
      "Trainable params: 12,941,352\n",
      "Non-trainable params: 270,700\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/500\n",
      "800/800 [==============================] - ETA: 1:50 - loss: 8.2440 - acc: 5.2083e-0 - ETA: 59s - loss: 7.2492 - acc: 0.3063    - ETA: 36s - loss: 6.8057 - acc: 0.41 - ETA: 22s - loss: 6.4245 - acc: 0.47 - ETA: 11s - loss: 6.2444 - acc: 0.51 - ETA: 2s - loss: 6.2936 - acc: 0.5282 - 59s 74ms/step - loss: 6.2823 - acc: 0.5322 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 2/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.0362 - acc: 0.62 - ETA: 27s - loss: 6.1328 - acc: 0.61 - ETA: 21s - loss: 6.1565 - acc: 0.61 - ETA: 14s - loss: 6.0824 - acc: 0.62 - ETA: 8s - loss: 6.0715 - acc: 0.6233 - ETA: 1s - loss: 5.9942 - acc: 0.628 - 45s 56ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 3/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 5.8431 - acc: 0.63 - ETA: 28s - loss: 6.1328 - acc: 0.61 - ETA: 22s - loss: 6.0838 - acc: 0.62 - ETA: 15s - loss: 6.0068 - acc: 0.62 - ETA: 8s - loss: 5.9640 - acc: 0.6300 - ETA: 1s - loss: 5.9537 - acc: 0.630 - 46s 58ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 4/500\n",
      "800/800 [==============================] - ETA: 37s - loss: 7.0436 - acc: 0.56 - ETA: 30s - loss: 6.2881 - acc: 0.60 - ETA: 23s - loss: 6.1593 - acc: 0.61 - ETA: 16s - loss: 6.1118 - acc: 0.62 - ETA: 9s - loss: 6.1168 - acc: 0.6205 - ETA: 1s - loss: 6.0222 - acc: 0.626 - 50s 63ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 5/500\n",
      "800/800 [==============================] - ETA: 35s - loss: 6.1873 - acc: 0.61 - ETA: 28s - loss: 6.1202 - acc: 0.62 - ETA: 22s - loss: 5.8627 - acc: 0.63 - ETA: 15s - loss: 5.9397 - acc: 0.63 - ETA: 8s - loss: 6.0513 - acc: 0.6246 - ETA: 1s - loss: 5.9942 - acc: 0.628 - 46s 57ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 6/500\n",
      "800/800 [==============================] - ETA: 37s - loss: 5.7340 - acc: 0.64 - ETA: 29s - loss: 5.9859 - acc: 0.62 - ETA: 23s - loss: 6.1006 - acc: 0.62 - ETA: 16s - loss: 6.0215 - acc: 0.62 - ETA: 9s - loss: 6.0211 - acc: 0.6265 - ETA: 1s - loss: 6.0460 - acc: 0.624 - 48s 60ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 7/500\n",
      "800/800 [==============================] - ETA: 36s - loss: 6.6406 - acc: 0.58 - ETA: 29s - loss: 6.3678 - acc: 0.60 - ETA: 22s - loss: 6.1510 - acc: 0.61 - ETA: 15s - loss: 6.0551 - acc: 0.62 - ETA: 8s - loss: 5.9859 - acc: 0.6286 - ETA: 1s - loss: 5.9635 - acc: 0.630 - 47s 59ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 8/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 5.8347 - acc: 0.63 - ETA: 27s - loss: 6.1034 - acc: 0.62 - ETA: 21s - loss: 6.0950 - acc: 0.62 - ETA: 14s - loss: 6.0803 - acc: 0.62 - ETA: 8s - loss: 6.0161 - acc: 0.6268 - ETA: 1s - loss: 6.0096 - acc: 0.627 - 45s 57ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 9/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 6.1454 - acc: 0.61 - ETA: 27s - loss: 6.2629 - acc: 0.61 - ETA: 21s - loss: 5.9327 - acc: 0.63 - ETA: 14s - loss: 6.1055 - acc: 0.62 - ETA: 8s - loss: 6.0379 - acc: 0.6254 - ETA: 1s - loss: 6.0068 - acc: 0.627 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 10/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 5.6417 - acc: 0.65 - ETA: 28s - loss: 5.7508 - acc: 0.64 - ETA: 21s - loss: 6.0026 - acc: 0.62 - ETA: 15s - loss: 6.0614 - acc: 0.62 - ETA: 8s - loss: 6.0513 - acc: 0.6246 - ETA: 1s - loss: 6.0502 - acc: 0.624 - 46s 57ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 11/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.6081 - acc: 0.65 - ETA: 27s - loss: 5.5577 - acc: 0.65 - ETA: 20s - loss: 5.7732 - acc: 0.64 - ETA: 14s - loss: 5.9061 - acc: 0.63 - ETA: 8s - loss: 5.8834 - acc: 0.6350 - ETA: 1s - loss: 5.9747 - acc: 0.629 - 44s 56ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 12/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 5.9523 - acc: 0.63 - ETA: 27s - loss: 5.8851 - acc: 0.63 - ETA: 21s - loss: 5.8907 - acc: 0.63 - ETA: 14s - loss: 6.0593 - acc: 0.62 - ETA: 8s - loss: 6.0396 - acc: 0.6253 - ETA: 1s - loss: 6.0110 - acc: 0.627 - 45s 56ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 13/500\n",
      "800/800 [==============================] - ETA: 35s - loss: 5.5745 - acc: 0.65 - ETA: 28s - loss: 5.7172 - acc: 0.64 - ETA: 21s - loss: 5.8012 - acc: 0.64 - ETA: 14s - loss: 5.8473 - acc: 0.63 - ETA: 8s - loss: 5.9355 - acc: 0.6318 - ETA: 1s - loss: 5.9900 - acc: 0.628 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 14/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 6.4224 - acc: 0.60 - ETA: 27s - loss: 6.4518 - acc: 0.59 - ETA: 20s - loss: 6.1090 - acc: 0.62 - ETA: 14s - loss: 6.0131 - acc: 0.62 - ETA: 7s - loss: 6.0211 - acc: 0.6265 - ETA: 1s - loss: 5.9719 - acc: 0.629 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 15/500\n",
      "800/800 [==============================] - ETA: 31s - loss: 5.9523 - acc: 0.63 - ETA: 26s - loss: 5.7760 - acc: 0.64 - ETA: 20s - loss: 5.8963 - acc: 0.63 - ETA: 13s - loss: 5.9859 - acc: 0.62 - ETA: 7s - loss: 6.0312 - acc: 0.6258 - ETA: 1s - loss: 5.9928 - acc: 0.628 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 16/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.1454 - acc: 0.61 - ETA: 25s - loss: 6.3468 - acc: 0.60 - ETA: 20s - loss: 6.1034 - acc: 0.62 - ETA: 14s - loss: 6.0278 - acc: 0.62 - ETA: 7s - loss: 6.0832 - acc: 0.6226 - ETA: 1s - loss: 6.0138 - acc: 0.626 - 42s 53ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 17/500\n",
      "800/800 [==============================] - ETA: 35s - loss: 6.2041 - acc: 0.61 - ETA: 27s - loss: 6.1621 - acc: 0.61 - ETA: 21s - loss: 6.0754 - acc: 0.62 - ETA: 14s - loss: 5.9355 - acc: 0.63 - ETA: 7s - loss: 6.0530 - acc: 0.6245 - ETA: 1s - loss: 6.0040 - acc: 0.627 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.1537 - acc: 0.61 - ETA: 27s - loss: 5.9313 - acc: 0.63 - ETA: 20s - loss: 6.0026 - acc: 0.62 - ETA: 14s - loss: 6.0551 - acc: 0.62 - ETA: 7s - loss: 6.0765 - acc: 0.6230 - ETA: 1s - loss: 6.0264 - acc: 0.626 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 19/500\n",
      "800/800 [==============================] - ETA: 30s - loss: 6.2377 - acc: 0.61 - ETA: 26s - loss: 6.1957 - acc: 0.61 - ETA: 20s - loss: 5.9047 - acc: 0.63 - ETA: 14s - loss: 5.9019 - acc: 0.63 - ETA: 7s - loss: 5.9741 - acc: 0.6294 - ETA: 1s - loss: 6.0278 - acc: 0.626 - 43s 53ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 20/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 6.3300 - acc: 0.60 - ETA: 27s - loss: 5.9607 - acc: 0.63 - ETA: 20s - loss: 6.0614 - acc: 0.62 - ETA: 14s - loss: 6.0425 - acc: 0.62 - ETA: 7s - loss: 6.0043 - acc: 0.6275 - ETA: 1s - loss: 5.9831 - acc: 0.628 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 21/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 5.6333 - acc: 0.65 - ETA: 27s - loss: 5.8515 - acc: 0.63 - ETA: 20s - loss: 5.8627 - acc: 0.63 - ETA: 14s - loss: 5.9880 - acc: 0.62 - ETA: 7s - loss: 5.9758 - acc: 0.6293 - ETA: 1s - loss: 5.9635 - acc: 0.630 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 22/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.8096 - acc: 0.63 - ETA: 26s - loss: 5.9397 - acc: 0.63 - ETA: 20s - loss: 6.1090 - acc: 0.62 - ETA: 14s - loss: 6.0530 - acc: 0.62 - ETA: 7s - loss: 5.9842 - acc: 0.6288 - ETA: 1s - loss: 6.0012 - acc: 0.627 - 43s 53ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 23/500\n",
      "800/800 [==============================] - ETA: 35s - loss: 5.9355 - acc: 0.63 - ETA: 26s - loss: 5.9817 - acc: 0.62 - ETA: 20s - loss: 6.2657 - acc: 0.61 - ETA: 14s - loss: 6.0887 - acc: 0.62 - ETA: 7s - loss: 6.0245 - acc: 0.6262 - ETA: 1s - loss: 5.9439 - acc: 0.631 - 43s 53ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 24/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 6.5567 - acc: 0.59 - ETA: 27s - loss: 6.2083 - acc: 0.61 - ETA: 20s - loss: 6.1873 - acc: 0.61 - ETA: 14s - loss: 5.9166 - acc: 0.63 - ETA: 8s - loss: 6.0010 - acc: 0.6277 - ETA: 1s - loss: 6.0628 - acc: 0.623 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 25/500\n",
      "800/800 [==============================] - ETA: 37s - loss: 6.3133 - acc: 0.60 - ETA: 31s - loss: 6.0110 - acc: 0.62 - ETA: 23s - loss: 6.2041 - acc: 0.61 - ETA: 16s - loss: 6.1558 - acc: 0.61 - ETA: 8s - loss: 6.0379 - acc: 0.6254 - ETA: 1s - loss: 6.0166 - acc: 0.626 - 47s 59ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 26/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.3468 - acc: 0.60 - ETA: 26s - loss: 6.1831 - acc: 0.61 - ETA: 20s - loss: 6.2237 - acc: 0.61 - ETA: 14s - loss: 6.2398 - acc: 0.61 - ETA: 7s - loss: 6.1151 - acc: 0.6206 - ETA: 1s - loss: 6.0376 - acc: 0.625 - 44s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 27/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 5.9607 - acc: 0.63 - ETA: 27s - loss: 5.8683 - acc: 0.63 - ETA: 21s - loss: 5.8767 - acc: 0.63 - ETA: 14s - loss: 5.9838 - acc: 0.62 - ETA: 8s - loss: 6.0748 - acc: 0.6231 - ETA: 1s - loss: 6.0208 - acc: 0.626 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 28/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 6.1286 - acc: 0.61 - ETA: 28s - loss: 5.8599 - acc: 0.63 - ETA: 21s - loss: 5.9747 - acc: 0.62 - ETA: 14s - loss: 6.0467 - acc: 0.62 - ETA: 8s - loss: 6.0664 - acc: 0.6236 - ETA: 1s - loss: 6.0348 - acc: 0.625 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 29/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.4811 - acc: 0.59 - ETA: 27s - loss: 6.2083 - acc: 0.61 - ETA: 20s - loss: 5.9803 - acc: 0.62 - ETA: 14s - loss: 5.9376 - acc: 0.63 - ETA: 8s - loss: 5.9237 - acc: 0.6325 - ETA: 1s - loss: 5.9649 - acc: 0.629 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 30/500\n",
      "800/800 [==============================] - ETA: 31s - loss: 5.7928 - acc: 0.64 - ETA: 26s - loss: 5.7340 - acc: 0.64 - ETA: 20s - loss: 5.7592 - acc: 0.64 - ETA: 14s - loss: 5.9733 - acc: 0.62 - ETA: 7s - loss: 5.9556 - acc: 0.6305 - ETA: 1s - loss: 6.0306 - acc: 0.625 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 31/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 5.9775 - acc: 0.62 - ETA: 27s - loss: 6.0026 - acc: 0.62 - ETA: 21s - loss: 5.9159 - acc: 0.63 - ETA: 14s - loss: 5.9628 - acc: 0.63 - ETA: 8s - loss: 6.0211 - acc: 0.6265 - ETA: 1s - loss: 5.9593 - acc: 0.630 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 32/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 6.3300 - acc: 0.60 - ETA: 28s - loss: 6.2923 - acc: 0.60 - ETA: 21s - loss: 5.8655 - acc: 0.63 - ETA: 14s - loss: 5.9166 - acc: 0.63 - ETA: 8s - loss: 5.9825 - acc: 0.6289 - ETA: 1s - loss: 5.9859 - acc: 0.628 - 45s 56ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 33/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.0614 - acc: 0.62 - ETA: 27s - loss: 6.1412 - acc: 0.61 - ETA: 20s - loss: 6.3188 - acc: 0.60 - ETA: 14s - loss: 6.1957 - acc: 0.61 - ETA: 8s - loss: 6.0799 - acc: 0.6228 - ETA: 1s - loss: 6.0138 - acc: 0.626 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 34/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.2797 - acc: 0.61 - ETA: 26s - loss: 6.1034 - acc: 0.62 - ETA: 20s - loss: 5.9495 - acc: 0.63 - ETA: 14s - loss: 6.1055 - acc: 0.62 - ETA: 7s - loss: 6.0547 - acc: 0.6244 - ETA: 1s - loss: 5.9914 - acc: 0.628 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 35/500\n",
      "800/800 [==============================] - ETA: 36s - loss: 6.1537 - acc: 0.61 - ETA: 27s - loss: 6.2503 - acc: 0.61 - ETA: 21s - loss: 6.0838 - acc: 0.62 - ETA: 14s - loss: 6.0656 - acc: 0.62 - ETA: 8s - loss: 5.9439 - acc: 0.6312 - ETA: 1s - loss: 6.0054 - acc: 0.627 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 36/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 5.9271 - acc: 0.63 - ETA: 27s - loss: 5.8222 - acc: 0.63 - ETA: 20s - loss: 5.8683 - acc: 0.63 - ETA: 14s - loss: 5.8935 - acc: 0.63 - ETA: 8s - loss: 5.8767 - acc: 0.6354 - ETA: 1s - loss: 5.9453 - acc: 0.631 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 37/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.1957 - acc: 0.61 - ETA: 26s - loss: 6.1286 - acc: 0.61 - ETA: 20s - loss: 5.9803 - acc: 0.62 - ETA: 14s - loss: 5.9565 - acc: 0.63 - ETA: 7s - loss: 5.9304 - acc: 0.6321 - ETA: 1s - loss: 5.9719 - acc: 0.629 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 38/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 5.9271 - acc: 0.63 - ETA: 27s - loss: 6.1747 - acc: 0.61 - ETA: 21s - loss: 6.1537 - acc: 0.61 - ETA: 15s - loss: 6.1307 - acc: 0.61 - ETA: 8s - loss: 6.0094 - acc: 0.6272 - ETA: 1s - loss: 6.0068 - acc: 0.627 - 45s 56ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 39/500\n",
      "800/800 [==============================] - ETA: 35s - loss: 6.0950 - acc: 0.62 - ETA: 27s - loss: 5.8767 - acc: 0.63 - ETA: 21s - loss: 5.9187 - acc: 0.63 - ETA: 14s - loss: 5.9082 - acc: 0.63 - ETA: 8s - loss: 5.8902 - acc: 0.6346 - ETA: 1s - loss: 6.0138 - acc: 0.626 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 40/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - ETA: 32s - loss: 5.6417 - acc: 0.65 - ETA: 27s - loss: 5.8809 - acc: 0.63 - ETA: 20s - loss: 5.8991 - acc: 0.63 - ETA: 14s - loss: 5.9942 - acc: 0.62 - ETA: 8s - loss: 5.9338 - acc: 0.6319 - ETA: 1s - loss: 5.9831 - acc: 0.628 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 41/500\n",
      "800/800 [==============================] - ETA: 31s - loss: 6.2797 - acc: 0.61 - ETA: 25s - loss: 6.1831 - acc: 0.61 - ETA: 20s - loss: 5.9159 - acc: 0.63 - ETA: 14s - loss: 6.0194 - acc: 0.62 - ETA: 7s - loss: 6.0278 - acc: 0.6260 - ETA: 1s - loss: 6.0208 - acc: 0.626 - 43s 53ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 42/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 6.2629 - acc: 0.61 - ETA: 27s - loss: 5.9942 - acc: 0.62 - ETA: 21s - loss: 6.0110 - acc: 0.62 - ETA: 14s - loss: 6.0047 - acc: 0.62 - ETA: 8s - loss: 5.9825 - acc: 0.6289 - ETA: 1s - loss: 6.0138 - acc: 0.626 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 43/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 5.7004 - acc: 0.64 - ETA: 27s - loss: 6.0026 - acc: 0.62 - ETA: 21s - loss: 5.7424 - acc: 0.64 - ETA: 14s - loss: 5.8263 - acc: 0.63 - ETA: 8s - loss: 5.9002 - acc: 0.6340 - ETA: 1s - loss: 6.0208 - acc: 0.626 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 44/500\n",
      "800/800 [==============================] - ETA: 31s - loss: 5.8935 - acc: 0.63 - ETA: 27s - loss: 5.6627 - acc: 0.64 - ETA: 20s - loss: 5.8991 - acc: 0.63 - ETA: 14s - loss: 5.9900 - acc: 0.62 - ETA: 8s - loss: 5.9707 - acc: 0.6296 - ETA: 1s - loss: 6.0628 - acc: 0.623 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 45/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 5.7424 - acc: 0.64 - ETA: 27s - loss: 6.1705 - acc: 0.61 - ETA: 21s - loss: 6.0894 - acc: 0.62 - ETA: 14s - loss: 6.0929 - acc: 0.62 - ETA: 8s - loss: 6.0312 - acc: 0.6258 - ETA: 1s - loss: 5.9845 - acc: 0.628 - 44s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 46/500\n",
      "800/800 [==============================] - ETA: 35s - loss: 5.6081 - acc: 0.65 - ETA: 27s - loss: 5.9103 - acc: 0.63 - ETA: 21s - loss: 5.8319 - acc: 0.63 - ETA: 14s - loss: 5.7592 - acc: 0.64 - ETA: 7s - loss: 5.8129 - acc: 0.6394 - ETA: 1s - loss: 6.0166 - acc: 0.626 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 47/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 5.8012 - acc: 0.64 - ETA: 27s - loss: 6.0824 - acc: 0.62 - ETA: 21s - loss: 5.9831 - acc: 0.62 - ETA: 15s - loss: 5.9292 - acc: 0.63 - ETA: 8s - loss: 5.9909 - acc: 0.6283 - ETA: 1s - loss: 6.0278 - acc: 0.626 - 45s 56ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 48/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 5.9355 - acc: 0.63 - ETA: 26s - loss: 6.2377 - acc: 0.61 - ETA: 20s - loss: 6.1593 - acc: 0.61 - ETA: 14s - loss: 6.0551 - acc: 0.62 - ETA: 7s - loss: 5.9405 - acc: 0.6315 - ETA: 1s - loss: 5.9803 - acc: 0.629 - 44s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 49/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.8851 - acc: 0.63 - ETA: 26s - loss: 5.9984 - acc: 0.62 - ETA: 20s - loss: 5.9075 - acc: 0.63 - ETA: 14s - loss: 5.9942 - acc: 0.62 - ETA: 7s - loss: 6.0765 - acc: 0.6230 - ETA: 1s - loss: 5.9803 - acc: 0.629 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 50/500\n",
      "800/800 [==============================] - ETA: 35s - loss: 6.0866 - acc: 0.62 - ETA: 28s - loss: 6.0824 - acc: 0.62 - ETA: 21s - loss: 5.9495 - acc: 0.63 - ETA: 14s - loss: 6.0005 - acc: 0.62 - ETA: 8s - loss: 6.0564 - acc: 0.6243 - ETA: 1s - loss: 6.0208 - acc: 0.626 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 51/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.9355 - acc: 0.63 - ETA: 27s - loss: 6.0194 - acc: 0.62 - ETA: 20s - loss: 6.0558 - acc: 0.62 - ETA: 14s - loss: 5.9796 - acc: 0.62 - ETA: 8s - loss: 6.0597 - acc: 0.6241 - ETA: 1s - loss: 6.0600 - acc: 0.624 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 52/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.5157 - acc: 0.65 - ETA: 27s - loss: 5.4696 - acc: 0.66 - ETA: 20s - loss: 5.6613 - acc: 0.64 - ETA: 14s - loss: 5.9061 - acc: 0.63 - ETA: 7s - loss: 5.9590 - acc: 0.6303 - ETA: 1s - loss: 6.0236 - acc: 0.626 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 53/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 5.4234 - acc: 0.66 - ETA: 26s - loss: 5.8012 - acc: 0.64 - ETA: 20s - loss: 5.8487 - acc: 0.63 - ETA: 14s - loss: 5.8263 - acc: 0.63 - ETA: 7s - loss: 5.9002 - acc: 0.6340 - ETA: 1s - loss: 5.9383 - acc: 0.631 - 43s 53ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 54/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 5.7844 - acc: 0.64 - ETA: 27s - loss: 5.9607 - acc: 0.63 - ETA: 21s - loss: 5.8963 - acc: 0.63 - ETA: 14s - loss: 5.9397 - acc: 0.63 - ETA: 8s - loss: 6.0178 - acc: 0.6267 - ETA: 1s - loss: 6.0040 - acc: 0.627 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 55/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.7928 - acc: 0.64 - ETA: 26s - loss: 5.9355 - acc: 0.63 - ETA: 20s - loss: 6.0418 - acc: 0.62 - ETA: 14s - loss: 5.8326 - acc: 0.63 - ETA: 7s - loss: 5.9019 - acc: 0.6339 - ETA: 1s - loss: 5.9886 - acc: 0.628 - 43s 53ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 56/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.7676 - acc: 0.64 - ETA: 25s - loss: 5.9061 - acc: 0.63 - ETA: 20s - loss: 6.0418 - acc: 0.62 - ETA: 14s - loss: 6.0593 - acc: 0.62 - ETA: 7s - loss: 5.9187 - acc: 0.6328 - ETA: 1s - loss: 6.0208 - acc: 0.626 - 42s 53ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 57/500\n",
      "800/800 [==============================] - ETA: 35s - loss: 5.5997 - acc: 0.65 - ETA: 27s - loss: 5.9900 - acc: 0.62 - ETA: 20s - loss: 5.9131 - acc: 0.63 - ETA: 14s - loss: 5.8494 - acc: 0.63 - ETA: 8s - loss: 5.9271 - acc: 0.6323 - ETA: 1s - loss: 6.0124 - acc: 0.627 - 44s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 58/500\n",
      "800/800 [==============================] - ETA: 31s - loss: 6.1789 - acc: 0.61 - ETA: 26s - loss: 5.9942 - acc: 0.62 - ETA: 20s - loss: 5.9691 - acc: 0.62 - ETA: 14s - loss: 6.0593 - acc: 0.62 - ETA: 7s - loss: 6.0161 - acc: 0.6268 - ETA: 1s - loss: 6.0600 - acc: 0.624 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 59/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.5157 - acc: 0.65 - ETA: 26s - loss: 5.8347 - acc: 0.63 - ETA: 20s - loss: 6.0446 - acc: 0.62 - ETA: 14s - loss: 5.9103 - acc: 0.63 - ETA: 7s - loss: 5.9942 - acc: 0.6281 - ETA: 1s - loss: 6.0096 - acc: 0.627 - 43s 53ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 60/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 5.8851 - acc: 0.63 - ETA: 27s - loss: 6.0824 - acc: 0.62 - ETA: 20s - loss: 6.1230 - acc: 0.62 - ETA: 14s - loss: 6.0614 - acc: 0.62 - ETA: 7s - loss: 6.0513 - acc: 0.6246 - ETA: 1s - loss: 6.0110 - acc: 0.627 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 61/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.6501 - acc: 0.64 - ETA: 27s - loss: 5.8347 - acc: 0.63 - ETA: 20s - loss: 5.9243 - acc: 0.63 - ETA: 14s - loss: 5.9544 - acc: 0.63 - ETA: 8s - loss: 6.0060 - acc: 0.6274 - ETA: 1s - loss: 6.0250 - acc: 0.626 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 62/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - ETA: 32s - loss: 6.0614 - acc: 0.62 - ETA: 26s - loss: 6.2041 - acc: 0.61 - ETA: 20s - loss: 5.9747 - acc: 0.62 - ETA: 14s - loss: 5.9649 - acc: 0.62 - ETA: 7s - loss: 5.9691 - acc: 0.6297 - ETA: 1s - loss: 6.0278 - acc: 0.626 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 63/500\n",
      "800/800 [==============================] - ETA: 35s - loss: 6.3804 - acc: 0.60 - ETA: 26s - loss: 6.1370 - acc: 0.61 - ETA: 20s - loss: 6.0894 - acc: 0.62 - ETA: 14s - loss: 6.1621 - acc: 0.61 - ETA: 7s - loss: 6.1773 - acc: 0.6168 - ETA: 1s - loss: 6.0502 - acc: 0.624 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 64/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 6.2293 - acc: 0.61 - ETA: 26s - loss: 6.0026 - acc: 0.62 - ETA: 20s - loss: 5.8543 - acc: 0.63 - ETA: 14s - loss: 5.8515 - acc: 0.63 - ETA: 7s - loss: 5.8331 - acc: 0.6381 - ETA: 1s - loss: 5.9341 - acc: 0.631 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 65/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 5.4990 - acc: 0.65 - ETA: 26s - loss: 5.8347 - acc: 0.63 - ETA: 20s - loss: 5.5661 - acc: 0.65 - ETA: 14s - loss: 5.7865 - acc: 0.64 - ETA: 7s - loss: 5.8952 - acc: 0.6343 - ETA: 1s - loss: 5.9439 - acc: 0.631 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 66/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 5.8767 - acc: 0.63 - ETA: 27s - loss: 5.5535 - acc: 0.65 - ETA: 21s - loss: 5.8739 - acc: 0.63 - ETA: 14s - loss: 5.9124 - acc: 0.63 - ETA: 8s - loss: 5.9506 - acc: 0.6308 - ETA: 1s - loss: 6.0068 - acc: 0.627 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 67/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.8431 - acc: 0.63 - ETA: 27s - loss: 6.0026 - acc: 0.62 - ETA: 20s - loss: 5.8012 - acc: 0.64 - ETA: 14s - loss: 5.9355 - acc: 0.63 - ETA: 7s - loss: 6.0329 - acc: 0.6257 - ETA: 1s - loss: 6.0082 - acc: 0.627 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 68/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.3888 - acc: 0.60 - ETA: 27s - loss: 6.2083 - acc: 0.61 - ETA: 20s - loss: 6.3188 - acc: 0.60 - ETA: 14s - loss: 6.3195 - acc: 0.60 - ETA: 8s - loss: 6.0261 - acc: 0.6261 - ETA: 1s - loss: 5.9887 - acc: 0.628 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 69/500\n",
      "800/800 [==============================] - ETA: 31s - loss: 6.2461 - acc: 0.61 - ETA: 26s - loss: 6.1663 - acc: 0.61 - ETA: 20s - loss: 6.0194 - acc: 0.62 - ETA: 14s - loss: 5.8473 - acc: 0.63 - ETA: 7s - loss: 5.8683 - acc: 0.6359 - ETA: 1s - loss: 6.0152 - acc: 0.626 - 43s 53ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 70/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 6.1705 - acc: 0.61 - ETA: 26s - loss: 6.1747 - acc: 0.61 - ETA: 20s - loss: 6.0250 - acc: 0.62 - ETA: 14s - loss: 5.9187 - acc: 0.63 - ETA: 7s - loss: 5.9153 - acc: 0.6330 - ETA: 1s - loss: 5.9425 - acc: 0.631 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 71/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 6.0782 - acc: 0.62 - ETA: 27s - loss: 6.1034 - acc: 0.62 - ETA: 20s - loss: 6.0082 - acc: 0.62 - ETA: 14s - loss: 5.9271 - acc: 0.63 - ETA: 7s - loss: 5.9758 - acc: 0.6293 - ETA: 1s - loss: 6.0012 - acc: 0.627 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 72/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 5.9271 - acc: 0.63 - ETA: 26s - loss: 5.8096 - acc: 0.63 - ETA: 20s - loss: 5.9103 - acc: 0.63 - ETA: 14s - loss: 5.9900 - acc: 0.62 - ETA: 7s - loss: 5.9170 - acc: 0.6329 - ETA: 1s - loss: 5.9607 - acc: 0.630 - 43s 53ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 73/500\n",
      "800/800 [==============================] - ETA: 35s - loss: 6.2629 - acc: 0.61 - ETA: 27s - loss: 6.3133 - acc: 0.60 - ETA: 21s - loss: 6.1062 - acc: 0.62 - ETA: 14s - loss: 6.0929 - acc: 0.62 - ETA: 8s - loss: 6.0228 - acc: 0.6264 - ETA: 1s - loss: 5.9523 - acc: 0.630 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 74/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 6.4560 - acc: 0.59 - ETA: 27s - loss: 5.9691 - acc: 0.62 - ETA: 20s - loss: 6.0558 - acc: 0.62 - ETA: 14s - loss: 5.9334 - acc: 0.63 - ETA: 7s - loss: 6.0396 - acc: 0.6253 - ETA: 1s - loss: 5.9761 - acc: 0.629 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 75/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.9859 - acc: 0.62 - ETA: 27s - loss: 5.9481 - acc: 0.63 - ETA: 20s - loss: 5.9019 - acc: 0.63 - ETA: 14s - loss: 5.8746 - acc: 0.63 - ETA: 8s - loss: 5.9741 - acc: 0.6294 - ETA: 1s - loss: 5.9691 - acc: 0.629 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 76/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 6.1118 - acc: 0.62 - ETA: 26s - loss: 5.5367 - acc: 0.65 - ETA: 20s - loss: 5.8012 - acc: 0.64 - ETA: 14s - loss: 6.0341 - acc: 0.62 - ETA: 8s - loss: 6.0782 - acc: 0.6229 - ETA: 1s - loss: 5.9998 - acc: 0.627 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 77/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 6.2125 - acc: 0.61 - ETA: 27s - loss: 6.1286 - acc: 0.61 - ETA: 20s - loss: 5.9299 - acc: 0.63 - ETA: 14s - loss: 5.9817 - acc: 0.62 - ETA: 7s - loss: 5.9540 - acc: 0.6306 - ETA: 1s - loss: 6.0418 - acc: 0.625 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 78/500\n",
      "800/800 [==============================] - ETA: 31s - loss: 6.3049 - acc: 0.60 - ETA: 27s - loss: 6.4602 - acc: 0.59 - ETA: 20s - loss: 6.2713 - acc: 0.61 - ETA: 14s - loss: 6.0908 - acc: 0.62 - ETA: 8s - loss: 6.0564 - acc: 0.6243 - ETA: 1s - loss: 5.9677 - acc: 0.629 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 79/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 6.1957 - acc: 0.61 - ETA: 31s - loss: 5.9900 - acc: 0.62 - ETA: 23s - loss: 5.9439 - acc: 0.63 - ETA: 15s - loss: 6.0257 - acc: 0.62 - ETA: 8s - loss: 6.1319 - acc: 0.6196 - ETA: 1s - loss: 6.0852 - acc: 0.622 - 45s 57ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 80/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.7760 - acc: 0.64 - ETA: 26s - loss: 6.0236 - acc: 0.62 - ETA: 20s - loss: 5.9467 - acc: 0.63 - ETA: 14s - loss: 5.9271 - acc: 0.63 - ETA: 7s - loss: 5.9674 - acc: 0.6298 - ETA: 1s - loss: 5.9733 - acc: 0.629 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 81/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 5.8599 - acc: 0.63 - ETA: 26s - loss: 5.9145 - acc: 0.63 - ETA: 20s - loss: 5.8655 - acc: 0.63 - ETA: 14s - loss: 5.9040 - acc: 0.63 - ETA: 7s - loss: 5.8465 - acc: 0.6373 - ETA: 1s - loss: 6.0082 - acc: 0.627 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 82/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.6668 - acc: 0.64 - ETA: 27s - loss: 5.8263 - acc: 0.63 - ETA: 20s - loss: 6.0502 - acc: 0.62 - ETA: 14s - loss: 6.0068 - acc: 0.62 - ETA: 7s - loss: 6.0161 - acc: 0.6268 - ETA: 1s - loss: 6.0404 - acc: 0.625 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 83/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.0950 - acc: 0.62 - ETA: 26s - loss: 5.9103 - acc: 0.63 - ETA: 20s - loss: 5.8263 - acc: 0.63 - ETA: 14s - loss: 5.9418 - acc: 0.63 - ETA: 7s - loss: 6.0026 - acc: 0.6276 - ETA: 1s - loss: 5.9984 - acc: 0.627 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 84/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - ETA: 34s - loss: 6.6658 - acc: 0.58 - ETA: 27s - loss: 6.3091 - acc: 0.60 - ETA: 20s - loss: 6.0978 - acc: 0.62 - ETA: 14s - loss: 6.0908 - acc: 0.62 - ETA: 7s - loss: 6.1135 - acc: 0.6207 - ETA: 1s - loss: 6.0488 - acc: 0.624 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 85/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.6249 - acc: 0.65 - ETA: 27s - loss: 6.0446 - acc: 0.62 - ETA: 20s - loss: 6.1174 - acc: 0.62 - ETA: 14s - loss: 6.0866 - acc: 0.62 - ETA: 7s - loss: 6.0832 - acc: 0.6226 - ETA: 1s - loss: 6.0264 - acc: 0.626 - 43s 53ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 86/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.9439 - acc: 0.63 - ETA: 26s - loss: 5.7802 - acc: 0.64 - ETA: 20s - loss: 5.7480 - acc: 0.64 - ETA: 14s - loss: 5.8536 - acc: 0.63 - ETA: 7s - loss: 5.9623 - acc: 0.6301 - ETA: 1s - loss: 5.9523 - acc: 0.630 - 43s 53ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 87/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 5.4822 - acc: 0.65 - ETA: 27s - loss: 5.3730 - acc: 0.66 - ETA: 20s - loss: 5.5801 - acc: 0.65 - ETA: 14s - loss: 5.6668 - acc: 0.64 - ETA: 8s - loss: 5.8969 - acc: 0.6342 - ETA: 1s - loss: 5.9327 - acc: 0.631 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 88/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.6742 - acc: 0.58 - ETA: 26s - loss: 6.1789 - acc: 0.61 - ETA: 20s - loss: 5.9607 - acc: 0.63 - ETA: 14s - loss: 6.0551 - acc: 0.62 - ETA: 7s - loss: 5.9724 - acc: 0.6295 - ETA: 1s - loss: 5.9900 - acc: 0.628 - 43s 53ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 89/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 6.1118 - acc: 0.62 - ETA: 27s - loss: 6.0572 - acc: 0.62 - ETA: 21s - loss: 5.9327 - acc: 0.63 - ETA: 14s - loss: 5.9229 - acc: 0.63 - ETA: 8s - loss: 5.9758 - acc: 0.6293 - ETA: 1s - loss: 5.9593 - acc: 0.630 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 90/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 5.7088 - acc: 0.64 - ETA: 26s - loss: 6.0530 - acc: 0.62 - ETA: 20s - loss: 5.8543 - acc: 0.63 - ETA: 14s - loss: 5.9628 - acc: 0.63 - ETA: 7s - loss: 6.0194 - acc: 0.6266 - ETA: 1s - loss: 6.0096 - acc: 0.627 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 91/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.6333 - acc: 0.65 - ETA: 26s - loss: 5.9271 - acc: 0.63 - ETA: 20s - loss: 6.0222 - acc: 0.62 - ETA: 14s - loss: 6.0950 - acc: 0.62 - ETA: 7s - loss: 6.0413 - acc: 0.6252 - ETA: 1s - loss: 5.9411 - acc: 0.631 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 92/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.0698 - acc: 0.62 - ETA: 26s - loss: 5.9439 - acc: 0.63 - ETA: 20s - loss: 5.9859 - acc: 0.62 - ETA: 14s - loss: 5.9166 - acc: 0.63 - ETA: 7s - loss: 5.8851 - acc: 0.6349 - ETA: 1s - loss: 5.9411 - acc: 0.631 - 43s 53ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 93/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 6.3720 - acc: 0.60 - ETA: 27s - loss: 6.1663 - acc: 0.61 - ETA: 20s - loss: 6.1454 - acc: 0.61 - ETA: 14s - loss: 6.2041 - acc: 0.61 - ETA: 8s - loss: 6.1000 - acc: 0.6216 - ETA: 1s - loss: 6.0236 - acc: 0.626 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 94/500\n",
      "800/800 [==============================] - ETA: 31s - loss: 6.0530 - acc: 0.62 - ETA: 27s - loss: 5.9817 - acc: 0.62 - ETA: 21s - loss: 6.0530 - acc: 0.62 - ETA: 14s - loss: 5.8998 - acc: 0.63 - ETA: 8s - loss: 5.9304 - acc: 0.6321 - ETA: 1s - loss: 5.9733 - acc: 0.629 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 95/500\n",
      "800/800 [==============================] - ETA: 31s - loss: 6.3384 - acc: 0.60 - ETA: 26s - loss: 6.1957 - acc: 0.61 - ETA: 20s - loss: 6.0698 - acc: 0.62 - ETA: 13s - loss: 5.9838 - acc: 0.62 - ETA: 7s - loss: 5.9724 - acc: 0.6295 - ETA: 1s - loss: 5.9467 - acc: 0.631 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 96/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 5.6333 - acc: 0.65 - ETA: 27s - loss: 6.0320 - acc: 0.62 - ETA: 21s - loss: 6.2713 - acc: 0.61 - ETA: 14s - loss: 6.1454 - acc: 0.61 - ETA: 8s - loss: 5.9825 - acc: 0.6289 - ETA: 1s - loss: 5.9900 - acc: 0.628 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 97/500\n",
      "800/800 [==============================] - ETA: 35s - loss: 6.4811 - acc: 0.59 - ETA: 27s - loss: 6.0698 - acc: 0.62 - ETA: 20s - loss: 6.0586 - acc: 0.62 - ETA: 14s - loss: 5.9565 - acc: 0.63 - ETA: 8s - loss: 5.9791 - acc: 0.6291 - ETA: 1s - loss: 6.0054 - acc: 0.627 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 98/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.3216 - acc: 0.60 - ETA: 26s - loss: 6.1873 - acc: 0.61 - ETA: 20s - loss: 5.9215 - acc: 0.63 - ETA: 14s - loss: 5.9145 - acc: 0.63 - ETA: 7s - loss: 5.9472 - acc: 0.6310 - ETA: 1s - loss: 5.9970 - acc: 0.628 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 99/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 6.6155 - acc: 0.58 - ETA: 27s - loss: 6.2083 - acc: 0.61 - ETA: 20s - loss: 6.1537 - acc: 0.61 - ETA: 14s - loss: 6.0551 - acc: 0.62 - ETA: 7s - loss: 6.0329 - acc: 0.6257 - ETA: 1s - loss: 5.9509 - acc: 0.630 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 100/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 5.8599 - acc: 0.63 - ETA: 27s - loss: 5.8222 - acc: 0.63 - ETA: 20s - loss: 5.9243 - acc: 0.63 - ETA: 14s - loss: 5.8725 - acc: 0.63 - ETA: 8s - loss: 5.9859 - acc: 0.6286 - ETA: 1s - loss: 5.9733 - acc: 0.629 - 44s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 101/500\n",
      "800/800 [==============================] - ETA: 31s - loss: 6.8085 - acc: 0.57 - ETA: 26s - loss: 6.1999 - acc: 0.61 - ETA: 20s - loss: 6.0530 - acc: 0.62 - ETA: 14s - loss: 6.0404 - acc: 0.62 - ETA: 7s - loss: 6.0564 - acc: 0.6243 - ETA: 1s - loss: 6.0068 - acc: 0.627 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 102/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.7424 - acc: 0.64 - ETA: 27s - loss: 5.9355 - acc: 0.63 - ETA: 20s - loss: 5.8319 - acc: 0.63 - ETA: 14s - loss: 5.9082 - acc: 0.63 - ETA: 7s - loss: 5.9842 - acc: 0.6288 - ETA: 1s - loss: 5.9956 - acc: 0.628 - 43s 53ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 103/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 5.4990 - acc: 0.65 - ETA: 27s - loss: 5.7382 - acc: 0.64 - ETA: 20s - loss: 5.7844 - acc: 0.64 - ETA: 14s - loss: 5.9103 - acc: 0.63 - ETA: 7s - loss: 5.9590 - acc: 0.6303 - ETA: 1s - loss: 5.9425 - acc: 0.631 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 104/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.7844 - acc: 0.64 - ETA: 27s - loss: 5.9145 - acc: 0.63 - ETA: 20s - loss: 5.9327 - acc: 0.63 - ETA: 14s - loss: 5.8473 - acc: 0.63 - ETA: 7s - loss: 6.0866 - acc: 0.6224 - ETA: 1s - loss: 6.0026 - acc: 0.627 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 105/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.5073 - acc: 0.65 - ETA: 27s - loss: 6.1621 - acc: 0.61 - ETA: 20s - loss: 6.1062 - acc: 0.62 - ETA: 14s - loss: 6.1810 - acc: 0.61 - ETA: 7s - loss: 6.0060 - acc: 0.6274 - ETA: 1s - loss: 6.0236 - acc: 0.626 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 106/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - ETA: 34s - loss: 5.5493 - acc: 0.65 - ETA: 26s - loss: 5.8473 - acc: 0.63 - ETA: 21s - loss: 5.9607 - acc: 0.63 - ETA: 14s - loss: 5.9607 - acc: 0.63 - ETA: 7s - loss: 5.8952 - acc: 0.6343 - ETA: 1s - loss: 5.9859 - acc: 0.628 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 107/500\n",
      "800/800 [==============================] - ETA: 35s - loss: 6.1621 - acc: 0.61 - ETA: 27s - loss: 6.1202 - acc: 0.62 - ETA: 20s - loss: 5.9970 - acc: 0.62 - ETA: 14s - loss: 5.9607 - acc: 0.63 - ETA: 8s - loss: 5.9120 - acc: 0.6332 - ETA: 1s - loss: 5.9551 - acc: 0.630 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 108/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.5483 - acc: 0.59 - ETA: 27s - loss: 6.3384 - acc: 0.60 - ETA: 20s - loss: 6.1342 - acc: 0.61 - ETA: 14s - loss: 6.0488 - acc: 0.62 - ETA: 8s - loss: 6.0732 - acc: 0.6232 - ETA: 1s - loss: 6.0306 - acc: 0.625 - 44s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 109/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 6.0278 - acc: 0.62 - ETA: 26s - loss: 6.2335 - acc: 0.61 - ETA: 20s - loss: 6.0390 - acc: 0.62 - ETA: 14s - loss: 5.9963 - acc: 0.62 - ETA: 7s - loss: 5.9674 - acc: 0.6298 - ETA: 1s - loss: 6.0096 - acc: 0.627 - 44s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 110/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 5.9691 - acc: 0.62 - ETA: 27s - loss: 6.3426 - acc: 0.60 - ETA: 20s - loss: 5.9914 - acc: 0.62 - ETA: 14s - loss: 6.0089 - acc: 0.62 - ETA: 7s - loss: 6.0614 - acc: 0.6240 - ETA: 1s - loss: 6.0292 - acc: 0.626 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 111/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.4476 - acc: 0.60 - ETA: 27s - loss: 6.3510 - acc: 0.60 - ETA: 20s - loss: 6.3188 - acc: 0.60 - ETA: 14s - loss: 6.1978 - acc: 0.61 - ETA: 7s - loss: 6.0631 - acc: 0.6239 - ETA: 1s - loss: 6.0572 - acc: 0.624 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 112/500\n",
      "800/800 [==============================] - ETA: 31s - loss: 5.6333 - acc: 0.65 - ETA: 26s - loss: 6.1202 - acc: 0.62 - ETA: 20s - loss: 5.8991 - acc: 0.63 - ETA: 13s - loss: 5.8305 - acc: 0.63 - ETA: 7s - loss: 6.0127 - acc: 0.6270 - ETA: 1s - loss: 5.9928 - acc: 0.628 - 43s 53ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 113/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 6.5063 - acc: 0.59 - ETA: 26s - loss: 6.3342 - acc: 0.60 - ETA: 20s - loss: 5.9691 - acc: 0.62 - ETA: 14s - loss: 5.9775 - acc: 0.62 - ETA: 7s - loss: 5.9187 - acc: 0.6328 - ETA: 1s - loss: 5.9299 - acc: 0.632 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 114/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 6.1202 - acc: 0.62 - ETA: 27s - loss: 6.2671 - acc: 0.61 - ETA: 20s - loss: 6.0894 - acc: 0.62 - ETA: 14s - loss: 6.0698 - acc: 0.62 - ETA: 7s - loss: 6.0312 - acc: 0.6258 - ETA: 1s - loss: 5.9761 - acc: 0.629 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 115/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.9607 - acc: 0.63 - ETA: 26s - loss: 5.8515 - acc: 0.63 - ETA: 20s - loss: 6.0530 - acc: 0.62 - ETA: 14s - loss: 5.9271 - acc: 0.63 - ETA: 7s - loss: 5.9489 - acc: 0.6309 - ETA: 1s - loss: 5.9803 - acc: 0.629 - 43s 53ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 116/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 6.2041 - acc: 0.61 - ETA: 27s - loss: 5.7424 - acc: 0.64 - ETA: 20s - loss: 5.8375 - acc: 0.63 - ETA: 14s - loss: 5.9103 - acc: 0.63 - ETA: 7s - loss: 5.9707 - acc: 0.6296 - ETA: 1s - loss: 6.0264 - acc: 0.626 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 117/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 6.4979 - acc: 0.59 - ETA: 27s - loss: 6.0488 - acc: 0.62 - ETA: 20s - loss: 5.9859 - acc: 0.62 - ETA: 14s - loss: 5.9334 - acc: 0.63 - ETA: 7s - loss: 6.0010 - acc: 0.6277 - ETA: 1s - loss: 6.0068 - acc: 0.627 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 118/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.1296 - acc: 0.68 - ETA: 27s - loss: 5.7424 - acc: 0.64 - ETA: 20s - loss: 5.8963 - acc: 0.63 - ETA: 14s - loss: 6.0215 - acc: 0.62 - ETA: 8s - loss: 6.0765 - acc: 0.6230 - ETA: 1s - loss: 6.0054 - acc: 0.627 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 119/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 5.6752 - acc: 0.64 - ETA: 27s - loss: 5.8473 - acc: 0.63 - ETA: 20s - loss: 5.9803 - acc: 0.62 - ETA: 14s - loss: 6.0089 - acc: 0.62 - ETA: 7s - loss: 5.9523 - acc: 0.6307 - ETA: 1s - loss: 5.9551 - acc: 0.630 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 120/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 5.5409 - acc: 0.65 - ETA: 26s - loss: 5.8977 - acc: 0.63 - ETA: 20s - loss: 6.0474 - acc: 0.62 - ETA: 14s - loss: 5.9942 - acc: 0.62 - ETA: 7s - loss: 6.0010 - acc: 0.6277 - ETA: 1s - loss: 6.0404 - acc: 0.625 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 121/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.0110 - acc: 0.62 - ETA: 26s - loss: 6.0152 - acc: 0.62 - ETA: 20s - loss: 6.0838 - acc: 0.62 - ETA: 14s - loss: 6.1537 - acc: 0.61 - ETA: 7s - loss: 6.0413 - acc: 0.6252 - ETA: 1s - loss: 6.0152 - acc: 0.626 - 43s 53ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 122/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 6.6239 - acc: 0.58 - ETA: 27s - loss: 6.3846 - acc: 0.60 - ETA: 21s - loss: 6.1761 - acc: 0.61 - ETA: 14s - loss: 5.9754 - acc: 0.62 - ETA: 8s - loss: 5.9657 - acc: 0.6299 - ETA: 1s - loss: 5.9803 - acc: 0.629 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 123/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 6.3636 - acc: 0.60 - ETA: 26s - loss: 5.8347 - acc: 0.63 - ETA: 20s - loss: 5.8347 - acc: 0.63 - ETA: 14s - loss: 5.9586 - acc: 0.63 - ETA: 7s - loss: 5.9439 - acc: 0.6312 - ETA: 1s - loss: 6.0152 - acc: 0.626 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 124/500\n",
      "800/800 [==============================] - ETA: 31s - loss: 5.8599 - acc: 0.63 - ETA: 27s - loss: 6.0026 - acc: 0.62 - ETA: 20s - loss: 5.9942 - acc: 0.62 - ETA: 14s - loss: 5.8956 - acc: 0.63 - ETA: 7s - loss: 5.9942 - acc: 0.6281 - ETA: 1s - loss: 6.0460 - acc: 0.624 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 125/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.2209 - acc: 0.61 - ETA: 26s - loss: 6.0698 - acc: 0.62 - ETA: 20s - loss: 6.0110 - acc: 0.62 - ETA: 14s - loss: 6.0257 - acc: 0.62 - ETA: 7s - loss: 5.9993 - acc: 0.6278 - ETA: 1s - loss: 6.0040 - acc: 0.627 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 126/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.2713 - acc: 0.61 - ETA: 26s - loss: 5.9019 - acc: 0.63 - ETA: 20s - loss: 5.9914 - acc: 0.62 - ETA: 14s - loss: 5.8662 - acc: 0.63 - ETA: 7s - loss: 5.8868 - acc: 0.6348 - ETA: 1s - loss: 5.9803 - acc: 0.629 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 127/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.0446 - acc: 0.62 - ETA: 27s - loss: 5.9775 - acc: 0.62 - ETA: 20s - loss: 6.0474 - acc: 0.62 - ETA: 14s - loss: 6.1307 - acc: 0.61 - ETA: 8s - loss: 6.0732 - acc: 0.6232 - ETA: 1s - loss: 6.0334 - acc: 0.625 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 128/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - ETA: 32s - loss: 6.1370 - acc: 0.61 - ETA: 26s - loss: 6.0026 - acc: 0.62 - ETA: 20s - loss: 6.0698 - acc: 0.62 - ETA: 13s - loss: 5.9523 - acc: 0.63 - ETA: 7s - loss: 6.1135 - acc: 0.6207 - ETA: 1s - loss: 6.0586 - acc: 0.624 - 43s 53ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 129/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 6.5063 - acc: 0.59 - ETA: 27s - loss: 6.1454 - acc: 0.61 - ETA: 21s - loss: 5.9803 - acc: 0.62 - ETA: 14s - loss: 6.0509 - acc: 0.62 - ETA: 8s - loss: 6.0110 - acc: 0.6271 - ETA: 1s - loss: 5.9747 - acc: 0.629 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 130/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 6.2545 - acc: 0.61 - ETA: 27s - loss: 5.9439 - acc: 0.63 - ETA: 20s - loss: 6.1873 - acc: 0.61 - ETA: 14s - loss: 6.0992 - acc: 0.62 - ETA: 7s - loss: 6.1235 - acc: 0.6201 - ETA: 1s - loss: 6.0320 - acc: 0.625 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 131/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.4486 - acc: 0.66 - ETA: 27s - loss: 5.4612 - acc: 0.66 - ETA: 20s - loss: 5.7984 - acc: 0.64 - ETA: 14s - loss: 5.9796 - acc: 0.62 - ETA: 7s - loss: 6.0261 - acc: 0.6261 - ETA: 1s - loss: 5.9817 - acc: 0.628 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 132/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 5.4402 - acc: 0.66 - ETA: 26s - loss: 5.6920 - acc: 0.64 - ETA: 20s - loss: 5.8431 - acc: 0.63 - ETA: 14s - loss: 5.7508 - acc: 0.64 - ETA: 7s - loss: 5.8549 - acc: 0.6368 - ETA: 1s - loss: 5.9117 - acc: 0.633 - 43s 53ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 133/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 5.9775 - acc: 0.62 - ETA: 27s - loss: 6.0656 - acc: 0.62 - ETA: 20s - loss: 5.9551 - acc: 0.63 - ETA: 14s - loss: 6.0173 - acc: 0.62 - ETA: 7s - loss: 6.0026 - acc: 0.6276 - ETA: 1s - loss: 5.9397 - acc: 0.631 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 134/500\n",
      "800/800 [==============================] - ETA: 31s - loss: 6.1370 - acc: 0.61 - ETA: 27s - loss: 6.0110 - acc: 0.62 - ETA: 20s - loss: 6.3412 - acc: 0.60 - ETA: 14s - loss: 6.2566 - acc: 0.61 - ETA: 7s - loss: 6.0799 - acc: 0.6228 - ETA: 1s - loss: 6.0166 - acc: 0.626 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 135/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.2125 - acc: 0.61 - ETA: 26s - loss: 6.1579 - acc: 0.61 - ETA: 20s - loss: 6.1258 - acc: 0.62 - ETA: 14s - loss: 6.0824 - acc: 0.62 - ETA: 7s - loss: 6.0429 - acc: 0.6251 - ETA: 1s - loss: 5.9900 - acc: 0.628 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 136/500\n",
      "800/800 [==============================] - ETA: 35s - loss: 5.6752 - acc: 0.64 - ETA: 27s - loss: 6.1034 - acc: 0.62 - ETA: 20s - loss: 5.9914 - acc: 0.62 - ETA: 14s - loss: 5.7697 - acc: 0.64 - ETA: 7s - loss: 5.8818 - acc: 0.6351 - ETA: 1s - loss: 5.9537 - acc: 0.630 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 137/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 6.1537 - acc: 0.61 - ETA: 26s - loss: 6.0152 - acc: 0.62 - ETA: 20s - loss: 6.0222 - acc: 0.62 - ETA: 14s - loss: 5.9754 - acc: 0.62 - ETA: 7s - loss: 5.9758 - acc: 0.6293 - ETA: 1s - loss: 5.9621 - acc: 0.630 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 138/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.3468 - acc: 0.60 - ETA: 27s - loss: 6.2293 - acc: 0.61 - ETA: 20s - loss: 6.1006 - acc: 0.62 - ETA: 14s - loss: 6.0845 - acc: 0.62 - ETA: 8s - loss: 6.1605 - acc: 0.6178 - ETA: 1s - loss: 6.0404 - acc: 0.625 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 139/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 6.4728 - acc: 0.59 - ETA: 27s - loss: 6.2335 - acc: 0.61 - ETA: 20s - loss: 6.1565 - acc: 0.61 - ETA: 14s - loss: 6.0719 - acc: 0.62 - ETA: 7s - loss: 6.1202 - acc: 0.6203 - ETA: 1s - loss: 6.0376 - acc: 0.625 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 140/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 6.0950 - acc: 0.62 - ETA: 27s - loss: 6.0026 - acc: 0.62 - ETA: 20s - loss: 5.7984 - acc: 0.64 - ETA: 14s - loss: 5.9166 - acc: 0.63 - ETA: 7s - loss: 6.0413 - acc: 0.6252 - ETA: 1s - loss: 5.9649 - acc: 0.629 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 141/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.2713 - acc: 0.61 - ETA: 26s - loss: 6.2713 - acc: 0.61 - ETA: 20s - loss: 6.1733 - acc: 0.61 - ETA: 13s - loss: 6.0488 - acc: 0.62 - ETA: 7s - loss: 6.0026 - acc: 0.6276 - ETA: 1s - loss: 5.9831 - acc: 0.628 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 142/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.3384 - acc: 0.60 - ETA: 26s - loss: 6.3888 - acc: 0.60 - ETA: 20s - loss: 6.1649 - acc: 0.61 - ETA: 14s - loss: 6.1684 - acc: 0.61 - ETA: 7s - loss: 6.1722 - acc: 0.6171 - ETA: 1s - loss: 6.0278 - acc: 0.626 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 143/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.9523 - acc: 0.63 - ETA: 27s - loss: 6.1328 - acc: 0.61 - ETA: 20s - loss: 6.0502 - acc: 0.62 - ETA: 14s - loss: 5.9439 - acc: 0.63 - ETA: 7s - loss: 6.0547 - acc: 0.6244 - ETA: 1s - loss: 6.0124 - acc: 0.627 - 43s 53ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 144/500\n",
      "800/800 [==============================] - ETA: 31s - loss: 5.4906 - acc: 0.65 - ETA: 26s - loss: 5.7886 - acc: 0.64 - ETA: 20s - loss: 6.0446 - acc: 0.62 - ETA: 14s - loss: 5.9838 - acc: 0.62 - ETA: 7s - loss: 6.0463 - acc: 0.6249 - ETA: 1s - loss: 5.9914 - acc: 0.628 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 145/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 6.1454 - acc: 0.61 - ETA: 26s - loss: 5.9649 - acc: 0.62 - ETA: 20s - loss: 5.9467 - acc: 0.63 - ETA: 14s - loss: 6.0467 - acc: 0.62 - ETA: 7s - loss: 5.9875 - acc: 0.6285 - ETA: 1s - loss: 6.0152 - acc: 0.626 - 43s 53ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 146/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 5.2639 - acc: 0.67 - ETA: 27s - loss: 5.4906 - acc: 0.65 - ETA: 20s - loss: 5.8487 - acc: 0.63 - ETA: 14s - loss: 5.9250 - acc: 0.63 - ETA: 7s - loss: 6.0295 - acc: 0.6259 - ETA: 1s - loss: 5.9425 - acc: 0.631 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 147/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.1118 - acc: 0.62 - ETA: 26s - loss: 5.7802 - acc: 0.64 - ETA: 20s - loss: 5.9355 - acc: 0.63 - ETA: 14s - loss: 5.9691 - acc: 0.62 - ETA: 7s - loss: 6.0664 - acc: 0.6236 - ETA: 1s - loss: 6.0124 - acc: 0.627 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 148/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.9691 - acc: 0.62 - ETA: 26s - loss: 5.9607 - acc: 0.63 - ETA: 20s - loss: 5.9607 - acc: 0.63 - ETA: 14s - loss: 5.9187 - acc: 0.63 - ETA: 7s - loss: 5.9590 - acc: 0.6303 - ETA: 1s - loss: 6.0068 - acc: 0.627 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 149/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 5.9103 - acc: 0.63 - ETA: 26s - loss: 5.9481 - acc: 0.63 - ETA: 20s - loss: 5.8627 - acc: 0.63 - ETA: 14s - loss: 5.7487 - acc: 0.64 - ETA: 7s - loss: 5.8683 - acc: 0.6359 - ETA: 1s - loss: 5.9621 - acc: 0.630 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 150/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - ETA: 31s - loss: 6.6826 - acc: 0.58 - ETA: 27s - loss: 6.2965 - acc: 0.60 - ETA: 20s - loss: 6.1761 - acc: 0.61 - ETA: 14s - loss: 6.0950 - acc: 0.62 - ETA: 8s - loss: 6.1017 - acc: 0.6215 - ETA: 1s - loss: 5.9956 - acc: 0.628 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 151/500\n",
      "800/800 [==============================] - ETA: 31s - loss: 5.6501 - acc: 0.64 - ETA: 26s - loss: 5.8054 - acc: 0.63 - ETA: 20s - loss: 5.7872 - acc: 0.64 - ETA: 14s - loss: 6.0257 - acc: 0.62 - ETA: 7s - loss: 6.0228 - acc: 0.6264 - ETA: 1s - loss: 5.9733 - acc: 0.629 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 152/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 6.0026 - acc: 0.62 - ETA: 26s - loss: 6.1202 - acc: 0.62 - ETA: 20s - loss: 6.0306 - acc: 0.62 - ETA: 14s - loss: 6.1055 - acc: 0.62 - ETA: 7s - loss: 6.0329 - acc: 0.6257 - ETA: 1s - loss: 5.9928 - acc: 0.628 - 43s 53ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 153/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 6.1873 - acc: 0.61 - ETA: 27s - loss: 6.3804 - acc: 0.60 - ETA: 20s - loss: 6.1230 - acc: 0.62 - ETA: 14s - loss: 6.0866 - acc: 0.62 - ETA: 7s - loss: 6.0480 - acc: 0.6248 - ETA: 1s - loss: 6.0194 - acc: 0.626 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 154/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.2797 - acc: 0.61 - ETA: 27s - loss: 6.0992 - acc: 0.62 - ETA: 20s - loss: 5.9551 - acc: 0.63 - ETA: 14s - loss: 6.0929 - acc: 0.62 - ETA: 7s - loss: 6.0094 - acc: 0.6272 - ETA: 1s - loss: 5.9411 - acc: 0.631 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 155/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 6.0782 - acc: 0.62 - ETA: 26s - loss: 5.8935 - acc: 0.63 - ETA: 20s - loss: 5.7312 - acc: 0.64 - ETA: 14s - loss: 5.8599 - acc: 0.63 - ETA: 7s - loss: 5.9271 - acc: 0.6323 - ETA: 1s - loss: 6.0418 - acc: 0.625 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 156/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.0362 - acc: 0.62 - ETA: 26s - loss: 6.0530 - acc: 0.62 - ETA: 20s - loss: 5.8515 - acc: 0.63 - ETA: 14s - loss: 6.1139 - acc: 0.62 - ETA: 7s - loss: 6.0362 - acc: 0.6255 - ETA: 1s - loss: 6.0292 - acc: 0.626 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 157/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 6.5315 - acc: 0.59 - ETA: 27s - loss: 6.1034 - acc: 0.62 - ETA: 20s - loss: 5.8963 - acc: 0.63 - ETA: 14s - loss: 5.8201 - acc: 0.63 - ETA: 8s - loss: 5.8683 - acc: 0.6359 - ETA: 1s - loss: 6.0026 - acc: 0.627 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 158/500\n",
      "800/800 [==============================] - ETA: 31s - loss: 5.8012 - acc: 0.64 - ETA: 25s - loss: 6.0026 - acc: 0.62 - ETA: 20s - loss: 6.2657 - acc: 0.61 - ETA: 14s - loss: 6.1663 - acc: 0.61 - ETA: 7s - loss: 6.1067 - acc: 0.6211 - ETA: 1s - loss: 6.0236 - acc: 0.626 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 159/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 6.2713 - acc: 0.61 - ETA: 26s - loss: 6.0950 - acc: 0.62 - ETA: 20s - loss: 5.8851 - acc: 0.63 - ETA: 14s - loss: 6.1454 - acc: 0.61 - ETA: 7s - loss: 6.1051 - acc: 0.6212 - ETA: 1s - loss: 6.0012 - acc: 0.627 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 160/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 5.9271 - acc: 0.63 - ETA: 27s - loss: 6.0656 - acc: 0.62 - ETA: 20s - loss: 5.9942 - acc: 0.62 - ETA: 14s - loss: 6.0614 - acc: 0.62 - ETA: 7s - loss: 6.1537 - acc: 0.6182 - ETA: 1s - loss: 5.9998 - acc: 0.627 - 43s 53ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 161/500\n",
      "800/800 [==============================] - ETA: 31s - loss: 6.1789 - acc: 0.61 - ETA: 26s - loss: 6.0110 - acc: 0.62 - ETA: 20s - loss: 6.1398 - acc: 0.61 - ETA: 14s - loss: 6.0089 - acc: 0.62 - ETA: 7s - loss: 5.9758 - acc: 0.6293 - ETA: 1s - loss: 6.0026 - acc: 0.627 - 43s 53ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 162/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 6.3133 - acc: 0.60 - ETA: 27s - loss: 5.9145 - acc: 0.63 - ETA: 20s - loss: 5.7900 - acc: 0.64 - ETA: 14s - loss: 6.0047 - acc: 0.62 - ETA: 8s - loss: 6.1051 - acc: 0.6212 - ETA: 1s - loss: 6.0222 - acc: 0.626 - 45s 56ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 163/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 5.7760 - acc: 0.64 - ETA: 27s - loss: 5.9565 - acc: 0.63 - ETA: 20s - loss: 5.8096 - acc: 0.63 - ETA: 14s - loss: 5.8494 - acc: 0.63 - ETA: 7s - loss: 5.8985 - acc: 0.6341 - ETA: 1s - loss: 6.0194 - acc: 0.626 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 164/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.9859 - acc: 0.62 - ETA: 26s - loss: 5.7886 - acc: 0.64 - ETA: 21s - loss: 5.5381 - acc: 0.65 - ETA: 14s - loss: 5.6962 - acc: 0.64 - ETA: 8s - loss: 5.8633 - acc: 0.6362 - ETA: 1s - loss: 5.9928 - acc: 0.628 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 165/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.2713 - acc: 0.61 - ETA: 26s - loss: 6.0866 - acc: 0.62 - ETA: 20s - loss: 5.9243 - acc: 0.63 - ETA: 14s - loss: 5.8893 - acc: 0.63 - ETA: 8s - loss: 5.9254 - acc: 0.6324 - ETA: 1s - loss: 5.9761 - acc: 0.629 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 166/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 5.9691 - acc: 0.62 - ETA: 27s - loss: 5.8935 - acc: 0.63 - ETA: 20s - loss: 5.9355 - acc: 0.63 - ETA: 14s - loss: 6.0404 - acc: 0.62 - ETA: 7s - loss: 6.0110 - acc: 0.6271 - ETA: 1s - loss: 6.0040 - acc: 0.627 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 167/500\n",
      "800/800 [==============================] - ETA: 31s - loss: 6.3888 - acc: 0.60 - ETA: 26s - loss: 6.1202 - acc: 0.62 - ETA: 20s - loss: 6.0894 - acc: 0.62 - ETA: 14s - loss: 5.9250 - acc: 0.63 - ETA: 7s - loss: 5.9607 - acc: 0.6302 - ETA: 1s - loss: 6.0404 - acc: 0.625 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 168/500\n",
      "800/800 [==============================] - ETA: 36s - loss: 6.0194 - acc: 0.62 - ETA: 30s - loss: 6.1454 - acc: 0.61 - ETA: 22s - loss: 6.2349 - acc: 0.61 - ETA: 15s - loss: 5.8704 - acc: 0.63 - ETA: 8s - loss: 5.9556 - acc: 0.6305 - ETA: 1s - loss: 5.9831 - acc: 0.628 - 46s 58ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 169/500\n",
      "800/800 [==============================] - ETA: 31s - loss: 5.8767 - acc: 0.63 - ETA: 25s - loss: 6.1957 - acc: 0.61 - ETA: 20s - loss: 6.0278 - acc: 0.62 - ETA: 14s - loss: 6.0719 - acc: 0.62 - ETA: 7s - loss: 6.1689 - acc: 0.6173 - ETA: 1s - loss: 6.0670 - acc: 0.623 - 42s 53ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 170/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 5.8683 - acc: 0.63 - ETA: 27s - loss: 5.9817 - acc: 0.62 - ETA: 20s - loss: 5.9635 - acc: 0.63 - ETA: 14s - loss: 5.9208 - acc: 0.63 - ETA: 7s - loss: 5.9472 - acc: 0.6310 - ETA: 1s - loss: 5.9747 - acc: 0.629 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 171/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.3384 - acc: 0.60 - ETA: 26s - loss: 6.1160 - acc: 0.62 - ETA: 20s - loss: 6.1314 - acc: 0.61 - ETA: 14s - loss: 6.0761 - acc: 0.62 - ETA: 7s - loss: 6.0631 - acc: 0.6239 - ETA: 1s - loss: 6.0110 - acc: 0.627 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 172/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - ETA: 32s - loss: 6.9932 - acc: 0.56 - ETA: 26s - loss: 6.4392 - acc: 0.60 - ETA: 20s - loss: 6.2377 - acc: 0.61 - ETA: 14s - loss: 5.9880 - acc: 0.62 - ETA: 8s - loss: 5.9540 - acc: 0.6306 - ETA: 1s - loss: 6.0376 - acc: 0.625 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 173/500\n",
      "800/800 [==============================] - ETA: 35s - loss: 6.5483 - acc: 0.59 - ETA: 27s - loss: 6.4392 - acc: 0.60 - ETA: 20s - loss: 6.3244 - acc: 0.60 - ETA: 14s - loss: 6.2755 - acc: 0.61 - ETA: 8s - loss: 6.1067 - acc: 0.6211 - ETA: 1s - loss: 5.9928 - acc: 0.628 - 44s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 174/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 6.8925 - acc: 0.57 - ETA: 27s - loss: 6.2881 - acc: 0.60 - ETA: 20s - loss: 6.1314 - acc: 0.61 - ETA: 14s - loss: 6.0215 - acc: 0.62 - ETA: 7s - loss: 6.0026 - acc: 0.6276 - ETA: 1s - loss: 5.9998 - acc: 0.627 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 175/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 5.8431 - acc: 0.63 - ETA: 28s - loss: 5.9229 - acc: 0.63 - ETA: 21s - loss: 6.1426 - acc: 0.61 - ETA: 14s - loss: 6.1328 - acc: 0.61 - ETA: 8s - loss: 6.0329 - acc: 0.6257 - ETA: 1s - loss: 6.0068 - acc: 0.627 - 45s 56ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 176/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 6.5819 - acc: 0.59 - ETA: 27s - loss: 6.1663 - acc: 0.61 - ETA: 21s - loss: 6.0894 - acc: 0.62 - ETA: 15s - loss: 6.0257 - acc: 0.62 - ETA: 8s - loss: 6.0312 - acc: 0.6258 - ETA: 1s - loss: 6.0040 - acc: 0.627 - 46s 57ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 177/500\n",
      "800/800 [==============================] - ETA: 35s - loss: 5.4318 - acc: 0.66 - ETA: 28s - loss: 5.8389 - acc: 0.63 - ETA: 21s - loss: 6.0978 - acc: 0.62 - ETA: 14s - loss: 6.0761 - acc: 0.62 - ETA: 8s - loss: 6.0983 - acc: 0.6217 - ETA: 1s - loss: 5.9579 - acc: 0.630 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 178/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 6.1537 - acc: 0.61 - ETA: 27s - loss: 5.8222 - acc: 0.63 - ETA: 22s - loss: 5.9103 - acc: 0.63 - ETA: 15s - loss: 6.0005 - acc: 0.62 - ETA: 8s - loss: 6.1235 - acc: 0.6201 - ETA: 1s - loss: 5.9789 - acc: 0.629 - 47s 59ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 179/500\n",
      "800/800 [==============================] - ETA: 38s - loss: 5.9523 - acc: 0.63 - ETA: 31s - loss: 5.9439 - acc: 0.63 - ETA: 23s - loss: 5.9019 - acc: 0.63 - ETA: 15s - loss: 5.8431 - acc: 0.63 - ETA: 8s - loss: 5.9069 - acc: 0.6335 - ETA: 1s - loss: 5.9914 - acc: 0.628 - 46s 58ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 180/500\n",
      "800/800 [==============================] - ETA: 36s - loss: 5.5577 - acc: 0.65 - ETA: 28s - loss: 5.9984 - acc: 0.62 - ETA: 21s - loss: 6.0334 - acc: 0.62 - ETA: 14s - loss: 6.1894 - acc: 0.61 - ETA: 8s - loss: 6.0866 - acc: 0.6224 - ETA: 1s - loss: 6.0026 - acc: 0.627 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 181/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.1957 - acc: 0.61 - ETA: 27s - loss: 6.1034 - acc: 0.62 - ETA: 20s - loss: 6.0614 - acc: 0.62 - ETA: 14s - loss: 5.9628 - acc: 0.63 - ETA: 7s - loss: 5.9556 - acc: 0.6305 - ETA: 1s - loss: 6.0222 - acc: 0.626 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 182/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 6.1705 - acc: 0.61 - ETA: 26s - loss: 5.9649 - acc: 0.62 - ETA: 20s - loss: 5.9523 - acc: 0.63 - ETA: 14s - loss: 5.8809 - acc: 0.63 - ETA: 7s - loss: 5.9288 - acc: 0.6322 - ETA: 1s - loss: 6.0418 - acc: 0.625 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 183/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 6.0950 - acc: 0.62 - ETA: 27s - loss: 6.1244 - acc: 0.62 - ETA: 20s - loss: 6.1146 - acc: 0.62 - ETA: 14s - loss: 5.9145 - acc: 0.63 - ETA: 7s - loss: 5.9187 - acc: 0.6328 - ETA: 1s - loss: 5.9970 - acc: 0.628 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 184/500\n",
      "800/800 [==============================] - ETA: 31s - loss: 5.6752 - acc: 0.64 - ETA: 26s - loss: 6.1454 - acc: 0.61 - ETA: 20s - loss: 6.1957 - acc: 0.61 - ETA: 14s - loss: 6.1349 - acc: 0.61 - ETA: 8s - loss: 6.0110 - acc: 0.6271 - ETA: 1s - loss: 6.0236 - acc: 0.626 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 185/500\n",
      "800/800 [==============================] - ETA: 31s - loss: 6.1286 - acc: 0.61 - ETA: 26s - loss: 5.8641 - acc: 0.63 - ETA: 20s - loss: 6.0502 - acc: 0.62 - ETA: 14s - loss: 5.9502 - acc: 0.63 - ETA: 7s - loss: 5.9472 - acc: 0.6310 - ETA: 1s - loss: 5.9831 - acc: 0.628 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 186/500\n",
      "800/800 [==============================] - ETA: 35s - loss: 6.2461 - acc: 0.61 - ETA: 28s - loss: 6.0782 - acc: 0.62 - ETA: 21s - loss: 6.0222 - acc: 0.62 - ETA: 14s - loss: 6.0236 - acc: 0.62 - ETA: 8s - loss: 5.9825 - acc: 0.6289 - ETA: 1s - loss: 5.9789 - acc: 0.629 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 187/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.8599 - acc: 0.63 - ETA: 27s - loss: 5.7676 - acc: 0.64 - ETA: 20s - loss: 5.8096 - acc: 0.63 - ETA: 14s - loss: 5.9712 - acc: 0.62 - ETA: 8s - loss: 5.9791 - acc: 0.6291 - ETA: 1s - loss: 6.0138 - acc: 0.626 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 188/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.7088 - acc: 0.64 - ETA: 26s - loss: 5.9733 - acc: 0.62 - ETA: 20s - loss: 6.0530 - acc: 0.62 - ETA: 14s - loss: 6.0320 - acc: 0.62 - ETA: 7s - loss: 6.0127 - acc: 0.6270 - ETA: 1s - loss: 5.9705 - acc: 0.629 - 43s 53ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 189/500\n",
      "800/800 [==============================] - ETA: 35s - loss: 6.2209 - acc: 0.61 - ETA: 27s - loss: 5.9900 - acc: 0.62 - ETA: 21s - loss: 5.9635 - acc: 0.63 - ETA: 14s - loss: 6.0089 - acc: 0.62 - ETA: 8s - loss: 6.0715 - acc: 0.6233 - ETA: 1s - loss: 6.0278 - acc: 0.626 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 190/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 5.4066 - acc: 0.66 - ETA: 27s - loss: 5.8096 - acc: 0.63 - ETA: 20s - loss: 6.1062 - acc: 0.62 - ETA: 14s - loss: 6.1726 - acc: 0.61 - ETA: 7s - loss: 6.0547 - acc: 0.6244 - ETA: 1s - loss: 6.0642 - acc: 0.623 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 191/500\n",
      "800/800 [==============================] - ETA: 31s - loss: 5.9187 - acc: 0.63 - ETA: 26s - loss: 5.9355 - acc: 0.63 - ETA: 20s - loss: 6.0362 - acc: 0.62 - ETA: 13s - loss: 6.1139 - acc: 0.62 - ETA: 7s - loss: 6.0614 - acc: 0.6240 - ETA: 1s - loss: 6.0628 - acc: 0.623 - 43s 53ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 192/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 5.4318 - acc: 0.66 - ETA: 27s - loss: 5.6585 - acc: 0.64 - ETA: 20s - loss: 5.8627 - acc: 0.63 - ETA: 14s - loss: 5.7865 - acc: 0.64 - ETA: 7s - loss: 5.8985 - acc: 0.6341 - ETA: 1s - loss: 6.0166 - acc: 0.626 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 193/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 6.2377 - acc: 0.61 - ETA: 27s - loss: 6.6197 - acc: 0.58 - ETA: 20s - loss: 6.2293 - acc: 0.61 - ETA: 14s - loss: 6.3342 - acc: 0.60 - ETA: 7s - loss: 6.1521 - acc: 0.6183 - ETA: 1s - loss: 5.9900 - acc: 0.628 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 194/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - ETA: 32s - loss: 6.1789 - acc: 0.61 - ETA: 27s - loss: 6.0152 - acc: 0.62 - ETA: 20s - loss: 5.9887 - acc: 0.62 - ETA: 14s - loss: 6.0236 - acc: 0.62 - ETA: 7s - loss: 6.0094 - acc: 0.6272 - ETA: 1s - loss: 5.9789 - acc: 0.629 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 195/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 6.6155 - acc: 0.58 - ETA: 26s - loss: 6.2419 - acc: 0.61 - ETA: 20s - loss: 6.1510 - acc: 0.61 - ETA: 14s - loss: 6.0131 - acc: 0.62 - ETA: 8s - loss: 5.9472 - acc: 0.6310 - ETA: 1s - loss: 6.0082 - acc: 0.627 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 196/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 5.8767 - acc: 0.63 - ETA: 26s - loss: 6.0908 - acc: 0.62 - ETA: 20s - loss: 6.0054 - acc: 0.62 - ETA: 14s - loss: 6.0530 - acc: 0.62 - ETA: 7s - loss: 6.0799 - acc: 0.6228 - ETA: 1s - loss: 6.0222 - acc: 0.626 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 197/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 5.5241 - acc: 0.65 - ETA: 27s - loss: 5.9565 - acc: 0.63 - ETA: 20s - loss: 5.9914 - acc: 0.62 - ETA: 14s - loss: 5.9229 - acc: 0.63 - ETA: 7s - loss: 6.0228 - acc: 0.6264 - ETA: 1s - loss: 5.9831 - acc: 0.628 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 198/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.7592 - acc: 0.64 - ETA: 26s - loss: 5.9733 - acc: 0.62 - ETA: 20s - loss: 5.8935 - acc: 0.63 - ETA: 14s - loss: 6.0131 - acc: 0.62 - ETA: 8s - loss: 6.0547 - acc: 0.6244 - ETA: 1s - loss: 6.0124 - acc: 0.627 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 199/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 5.7760 - acc: 0.64 - ETA: 26s - loss: 6.0278 - acc: 0.62 - ETA: 20s - loss: 5.9215 - acc: 0.63 - ETA: 14s - loss: 5.8683 - acc: 0.63 - ETA: 7s - loss: 6.0564 - acc: 0.6243 - ETA: 1s - loss: 6.0040 - acc: 0.627 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 200/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 6.2965 - acc: 0.60 - ETA: 26s - loss: 6.2629 - acc: 0.61 - ETA: 20s - loss: 6.2797 - acc: 0.61 - ETA: 14s - loss: 6.0950 - acc: 0.62 - ETA: 7s - loss: 6.0026 - acc: 0.6276 - ETA: 1s - loss: 6.0236 - acc: 0.626 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 201/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.9859 - acc: 0.62 - ETA: 27s - loss: 5.9607 - acc: 0.63 - ETA: 20s - loss: 5.9551 - acc: 0.63 - ETA: 14s - loss: 6.0152 - acc: 0.62 - ETA: 8s - loss: 6.0043 - acc: 0.6275 - ETA: 1s - loss: 6.0166 - acc: 0.626 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 202/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 5.5241 - acc: 0.65 - ETA: 26s - loss: 5.8431 - acc: 0.63 - ETA: 20s - loss: 6.0698 - acc: 0.62 - ETA: 14s - loss: 5.9271 - acc: 0.63 - ETA: 7s - loss: 5.9825 - acc: 0.6289 - ETA: 1s - loss: 5.9900 - acc: 0.628 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 203/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 6.0530 - acc: 0.62 - ETA: 26s - loss: 5.8347 - acc: 0.63 - ETA: 20s - loss: 5.7200 - acc: 0.64 - ETA: 14s - loss: 5.9061 - acc: 0.63 - ETA: 7s - loss: 6.0530 - acc: 0.6245 - ETA: 1s - loss: 6.0026 - acc: 0.627 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 204/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.9691 - acc: 0.62 - ETA: 26s - loss: 6.0194 - acc: 0.62 - ETA: 20s - loss: 6.0838 - acc: 0.62 - ETA: 14s - loss: 6.0194 - acc: 0.62 - ETA: 7s - loss: 6.0144 - acc: 0.6269 - ETA: 1s - loss: 6.0404 - acc: 0.625 - 42s 53ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 205/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 6.0362 - acc: 0.62 - ETA: 26s - loss: 6.1034 - acc: 0.62 - ETA: 20s - loss: 6.1314 - acc: 0.61 - ETA: 14s - loss: 6.0614 - acc: 0.62 - ETA: 7s - loss: 6.0127 - acc: 0.6270 - ETA: 1s - loss: 5.9942 - acc: 0.628 - 43s 53ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 206/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 6.2125 - acc: 0.61 - ETA: 27s - loss: 5.9019 - acc: 0.63 - ETA: 20s - loss: 5.9747 - acc: 0.62 - ETA: 14s - loss: 6.0257 - acc: 0.62 - ETA: 8s - loss: 6.0413 - acc: 0.6252 - ETA: 1s - loss: 6.0026 - acc: 0.627 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 207/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.3300 - acc: 0.60 - ETA: 27s - loss: 6.3091 - acc: 0.60 - ETA: 20s - loss: 6.2069 - acc: 0.61 - ETA: 14s - loss: 6.0698 - acc: 0.62 - ETA: 7s - loss: 6.0597 - acc: 0.6241 - ETA: 1s - loss: 6.0418 - acc: 0.625 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 208/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.0782 - acc: 0.62 - ETA: 26s - loss: 5.7172 - acc: 0.64 - ETA: 20s - loss: 6.0502 - acc: 0.62 - ETA: 14s - loss: 6.1223 - acc: 0.62 - ETA: 8s - loss: 6.0564 - acc: 0.6243 - ETA: 1s - loss: 5.9803 - acc: 0.629 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 209/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 5.5913 - acc: 0.65 - ETA: 27s - loss: 5.9019 - acc: 0.63 - ETA: 20s - loss: 6.1649 - acc: 0.61 - ETA: 14s - loss: 6.0446 - acc: 0.62 - ETA: 7s - loss: 6.0043 - acc: 0.6275 - ETA: 1s - loss: 6.0152 - acc: 0.626 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 210/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.9187 - acc: 0.63 - ETA: 27s - loss: 5.8012 - acc: 0.64 - ETA: 20s - loss: 6.0222 - acc: 0.62 - ETA: 14s - loss: 6.1454 - acc: 0.61 - ETA: 7s - loss: 6.1924 - acc: 0.6158 - ETA: 1s - loss: 6.0096 - acc: 0.627 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 211/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.2797 - acc: 0.61 - ETA: 26s - loss: 6.0026 - acc: 0.62 - ETA: 20s - loss: 5.9914 - acc: 0.62 - ETA: 14s - loss: 5.9649 - acc: 0.62 - ETA: 7s - loss: 6.0782 - acc: 0.6229 - ETA: 1s - loss: 5.9803 - acc: 0.629 - 43s 53ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 212/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 6.2209 - acc: 0.61 - ETA: 27s - loss: 6.1747 - acc: 0.61 - ETA: 20s - loss: 5.9775 - acc: 0.62 - ETA: 14s - loss: 5.9334 - acc: 0.63 - ETA: 8s - loss: 5.9892 - acc: 0.6284 - ETA: 1s - loss: 6.0208 - acc: 0.626 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 213/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 6.0026 - acc: 0.62 - ETA: 27s - loss: 6.0026 - acc: 0.62 - ETA: 20s - loss: 6.0558 - acc: 0.62 - ETA: 14s - loss: 5.9880 - acc: 0.62 - ETA: 7s - loss: 5.9875 - acc: 0.6285 - ETA: 1s - loss: 5.9453 - acc: 0.631 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 214/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.3552 - acc: 0.60 - ETA: 27s - loss: 6.1747 - acc: 0.61 - ETA: 20s - loss: 6.0670 - acc: 0.62 - ETA: 14s - loss: 5.9628 - acc: 0.63 - ETA: 7s - loss: 5.9338 - acc: 0.6319 - ETA: 1s - loss: 5.9649 - acc: 0.629 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 215/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 6.2377 - acc: 0.61 - ETA: 27s - loss: 6.3216 - acc: 0.60 - ETA: 20s - loss: 6.2517 - acc: 0.61 - ETA: 14s - loss: 6.0950 - acc: 0.62 - ETA: 8s - loss: 6.1252 - acc: 0.6200 - ETA: 1s - loss: 5.9928 - acc: 0.628 - 44s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 216/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - ETA: 34s - loss: 6.1370 - acc: 0.61 - ETA: 27s - loss: 6.2125 - acc: 0.61 - ETA: 20s - loss: 6.0418 - acc: 0.62 - ETA: 14s - loss: 6.0530 - acc: 0.62 - ETA: 7s - loss: 5.9959 - acc: 0.6280 - ETA: 1s - loss: 5.9900 - acc: 0.628 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 217/500\n",
      "800/800 [==============================] - ETA: 31s - loss: 5.6165 - acc: 0.65 - ETA: 26s - loss: 5.9439 - acc: 0.63 - ETA: 20s - loss: 5.8851 - acc: 0.63 - ETA: 14s - loss: 5.9754 - acc: 0.62 - ETA: 7s - loss: 5.9623 - acc: 0.6301 - ETA: 1s - loss: 6.0012 - acc: 0.627 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 218/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.5325 - acc: 0.65 - ETA: 26s - loss: 5.7424 - acc: 0.64 - ETA: 20s - loss: 5.7704 - acc: 0.64 - ETA: 14s - loss: 5.7760 - acc: 0.64 - ETA: 7s - loss: 5.9640 - acc: 0.6300 - ETA: 1s - loss: 5.9789 - acc: 0.629 - 43s 53ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 219/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 6.7414 - acc: 0.58 - ETA: 27s - loss: 6.1202 - acc: 0.62 - ETA: 20s - loss: 6.0110 - acc: 0.62 - ETA: 14s - loss: 5.9754 - acc: 0.62 - ETA: 7s - loss: 6.0631 - acc: 0.6239 - ETA: 1s - loss: 6.0292 - acc: 0.626 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 220/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.6501 - acc: 0.64 - ETA: 26s - loss: 5.6333 - acc: 0.65 - ETA: 20s - loss: 5.7788 - acc: 0.64 - ETA: 14s - loss: 5.9880 - acc: 0.62 - ETA: 7s - loss: 5.9321 - acc: 0.6320 - ETA: 1s - loss: 5.9914 - acc: 0.628 - 44s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 221/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 5.8180 - acc: 0.63 - ETA: 26s - loss: 6.0194 - acc: 0.62 - ETA: 20s - loss: 6.0782 - acc: 0.62 - ETA: 14s - loss: 5.9838 - acc: 0.62 - ETA: 7s - loss: 6.0379 - acc: 0.6254 - ETA: 1s - loss: 6.0124 - acc: 0.627 - 43s 53ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 222/500\n",
      "800/800 [==============================] - ETA: 35s - loss: 5.9942 - acc: 0.62 - ETA: 27s - loss: 5.9397 - acc: 0.63 - ETA: 21s - loss: 5.9635 - acc: 0.63 - ETA: 14s - loss: 6.0698 - acc: 0.62 - ETA: 7s - loss: 5.9859 - acc: 0.6286 - ETA: 1s - loss: 5.9761 - acc: 0.629 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 223/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.9691 - acc: 0.62 - ETA: 27s - loss: 6.1076 - acc: 0.62 - ETA: 20s - loss: 6.0138 - acc: 0.62 - ETA: 14s - loss: 5.9502 - acc: 0.63 - ETA: 7s - loss: 5.9304 - acc: 0.6321 - ETA: 1s - loss: 6.0264 - acc: 0.626 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 224/500\n",
      "800/800 [==============================] - ETA: 31s - loss: 5.5829 - acc: 0.65 - ETA: 27s - loss: 5.9103 - acc: 0.63 - ETA: 20s - loss: 5.8935 - acc: 0.63 - ETA: 14s - loss: 5.8410 - acc: 0.63 - ETA: 7s - loss: 5.8297 - acc: 0.6383 - ETA: 1s - loss: 5.9859 - acc: 0.628 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 225/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.7582 - acc: 0.58 - ETA: 26s - loss: 6.3510 - acc: 0.60 - ETA: 20s - loss: 6.2545 - acc: 0.61 - ETA: 14s - loss: 6.0425 - acc: 0.62 - ETA: 7s - loss: 5.9909 - acc: 0.6283 - ETA: 1s - loss: 6.0376 - acc: 0.625 - 42s 53ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 226/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 6.0614 - acc: 0.62 - ETA: 27s - loss: 5.9271 - acc: 0.63 - ETA: 20s - loss: 5.9215 - acc: 0.63 - ETA: 14s - loss: 5.9691 - acc: 0.62 - ETA: 8s - loss: 5.9942 - acc: 0.6281 - ETA: 1s - loss: 5.9956 - acc: 0.628 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 227/500\n",
      "800/800 [==============================] - ETA: 31s - loss: 5.8935 - acc: 0.63 - ETA: 26s - loss: 5.8305 - acc: 0.63 - ETA: 20s - loss: 5.9215 - acc: 0.63 - ETA: 13s - loss: 6.0635 - acc: 0.62 - ETA: 7s - loss: 6.0681 - acc: 0.6235 - ETA: 1s - loss: 6.0180 - acc: 0.626 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 228/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.9439 - acc: 0.63 - ETA: 26s - loss: 5.9942 - acc: 0.62 - ETA: 20s - loss: 6.0698 - acc: 0.62 - ETA: 14s - loss: 6.0089 - acc: 0.62 - ETA: 7s - loss: 5.8969 - acc: 0.6342 - ETA: 1s - loss: 5.9635 - acc: 0.630 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 229/500\n",
      "800/800 [==============================] - ETA: 35s - loss: 6.2293 - acc: 0.61 - ETA: 27s - loss: 6.3972 - acc: 0.60 - ETA: 21s - loss: 6.3916 - acc: 0.60 - ETA: 14s - loss: 6.3951 - acc: 0.60 - ETA: 8s - loss: 6.1705 - acc: 0.6172 - ETA: 1s - loss: 6.0348 - acc: 0.625 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 230/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.6658 - acc: 0.58 - ETA: 27s - loss: 6.1328 - acc: 0.61 - ETA: 20s - loss: 5.9803 - acc: 0.62 - ETA: 14s - loss: 6.0656 - acc: 0.62 - ETA: 7s - loss: 6.0899 - acc: 0.6222 - ETA: 1s - loss: 6.0026 - acc: 0.627 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 231/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.8012 - acc: 0.64 - ETA: 26s - loss: 6.0740 - acc: 0.62 - ETA: 20s - loss: 5.9635 - acc: 0.63 - ETA: 14s - loss: 5.9733 - acc: 0.62 - ETA: 7s - loss: 6.0413 - acc: 0.6252 - ETA: 1s - loss: 6.0180 - acc: 0.626 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 232/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 5.9103 - acc: 0.63 - ETA: 27s - loss: 6.1244 - acc: 0.62 - ETA: 20s - loss: 5.9187 - acc: 0.63 - ETA: 14s - loss: 5.9963 - acc: 0.62 - ETA: 7s - loss: 5.8700 - acc: 0.6358 - ETA: 1s - loss: 5.9803 - acc: 0.629 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 233/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.1883 - acc: 0.67 - ETA: 27s - loss: 5.8431 - acc: 0.63 - ETA: 20s - loss: 6.0502 - acc: 0.62 - ETA: 14s - loss: 6.0215 - acc: 0.62 - ETA: 7s - loss: 6.0295 - acc: 0.6259 - ETA: 1s - loss: 6.0586 - acc: 0.624 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 234/500\n",
      "800/800 [==============================] - ETA: 31s - loss: 6.3552 - acc: 0.60 - ETA: 26s - loss: 6.2587 - acc: 0.61 - ETA: 20s - loss: 6.0782 - acc: 0.62 - ETA: 14s - loss: 5.9733 - acc: 0.62 - ETA: 8s - loss: 5.9456 - acc: 0.6311 - ETA: 1s - loss: 5.9537 - acc: 0.630 - 45s 56ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 235/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 5.9439 - acc: 0.63 - ETA: 27s - loss: 5.8557 - acc: 0.63 - ETA: 21s - loss: 6.2041 - acc: 0.61 - ETA: 14s - loss: 6.0173 - acc: 0.62 - ETA: 7s - loss: 6.0077 - acc: 0.6273 - ETA: 1s - loss: 6.0026 - acc: 0.627 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 236/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 6.1370 - acc: 0.61 - ETA: 26s - loss: 5.9984 - acc: 0.62 - ETA: 20s - loss: 5.8767 - acc: 0.63 - ETA: 14s - loss: 5.8746 - acc: 0.63 - ETA: 7s - loss: 5.9607 - acc: 0.6302 - ETA: 1s - loss: 6.0236 - acc: 0.626 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 237/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.4728 - acc: 0.59 - ETA: 27s - loss: 6.2209 - acc: 0.61 - ETA: 20s - loss: 6.0054 - acc: 0.62 - ETA: 14s - loss: 5.9397 - acc: 0.63 - ETA: 7s - loss: 6.0497 - acc: 0.6247 - ETA: 1s - loss: 5.9970 - acc: 0.628 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 238/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - ETA: 33s - loss: 6.5231 - acc: 0.59 - ETA: 26s - loss: 6.2419 - acc: 0.61 - ETA: 20s - loss: 6.1929 - acc: 0.61 - ETA: 14s - loss: 6.1852 - acc: 0.61 - ETA: 8s - loss: 6.0295 - acc: 0.6259 - ETA: 1s - loss: 6.0320 - acc: 0.625 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 239/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 6.0362 - acc: 0.62 - ETA: 27s - loss: 6.1328 - acc: 0.61 - ETA: 20s - loss: 6.0950 - acc: 0.62 - ETA: 14s - loss: 6.0299 - acc: 0.62 - ETA: 8s - loss: 5.9304 - acc: 0.6321 - ETA: 1s - loss: 5.9537 - acc: 0.630 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 240/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.9859 - acc: 0.62 - ETA: 27s - loss: 5.7256 - acc: 0.64 - ETA: 20s - loss: 5.9607 - acc: 0.63 - ETA: 14s - loss: 6.0026 - acc: 0.62 - ETA: 7s - loss: 5.9674 - acc: 0.6298 - ETA: 1s - loss: 6.0054 - acc: 0.627 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 241/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.8263 - acc: 0.63 - ETA: 27s - loss: 5.4738 - acc: 0.66 - ETA: 21s - loss: 5.6920 - acc: 0.64 - ETA: 14s - loss: 5.8557 - acc: 0.63 - ETA: 8s - loss: 5.9439 - acc: 0.6312 - ETA: 1s - loss: 6.0264 - acc: 0.626 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 242/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.8096 - acc: 0.63 - ETA: 26s - loss: 6.1244 - acc: 0.62 - ETA: 20s - loss: 6.2573 - acc: 0.61 - ETA: 14s - loss: 6.0446 - acc: 0.62 - ETA: 7s - loss: 6.0513 - acc: 0.6246 - ETA: 1s - loss: 5.9900 - acc: 0.628 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 243/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 5.9439 - acc: 0.63 - ETA: 26s - loss: 5.8305 - acc: 0.63 - ETA: 20s - loss: 5.8431 - acc: 0.63 - ETA: 14s - loss: 5.8410 - acc: 0.63 - ETA: 7s - loss: 5.9405 - acc: 0.6315 - ETA: 1s - loss: 5.9887 - acc: 0.628 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 244/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 5.9523 - acc: 0.63 - ETA: 27s - loss: 5.9775 - acc: 0.62 - ETA: 20s - loss: 5.8571 - acc: 0.63 - ETA: 14s - loss: 5.9900 - acc: 0.62 - ETA: 7s - loss: 5.9456 - acc: 0.6311 - ETA: 1s - loss: 6.0110 - acc: 0.627 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 245/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 6.4979 - acc: 0.59 - ETA: 26s - loss: 6.1537 - acc: 0.61 - ETA: 20s - loss: 6.0306 - acc: 0.62 - ETA: 15s - loss: 6.1454 - acc: 0.61 - ETA: 8s - loss: 6.1286 - acc: 0.6198 - ETA: 1s - loss: 6.0040 - acc: 0.627 - 45s 56ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 246/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 6.0614 - acc: 0.62 - ETA: 26s - loss: 6.2167 - acc: 0.61 - ETA: 20s - loss: 6.0054 - acc: 0.62 - ETA: 14s - loss: 6.1013 - acc: 0.62 - ETA: 7s - loss: 6.0715 - acc: 0.6233 - ETA: 1s - loss: 6.0446 - acc: 0.625 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 247/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 6.0950 - acc: 0.62 - ETA: 27s - loss: 6.1537 - acc: 0.61 - ETA: 21s - loss: 6.3133 - acc: 0.60 - ETA: 14s - loss: 6.2923 - acc: 0.60 - ETA: 8s - loss: 6.0983 - acc: 0.6217 - ETA: 1s - loss: 6.0502 - acc: 0.624 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 248/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.8180 - acc: 0.63 - ETA: 26s - loss: 6.0320 - acc: 0.62 - ETA: 20s - loss: 5.7956 - acc: 0.64 - ETA: 14s - loss: 5.8872 - acc: 0.63 - ETA: 8s - loss: 5.9741 - acc: 0.6294 - ETA: 1s - loss: 6.0628 - acc: 0.623 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 249/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 6.0278 - acc: 0.62 - ETA: 26s - loss: 6.1789 - acc: 0.61 - ETA: 20s - loss: 6.1621 - acc: 0.61 - ETA: 14s - loss: 6.0341 - acc: 0.62 - ETA: 7s - loss: 5.9607 - acc: 0.6302 - ETA: 1s - loss: 5.9551 - acc: 0.630 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 250/500\n",
      "800/800 [==============================] - ETA: 35s - loss: 5.4402 - acc: 0.66 - ETA: 28s - loss: 5.8389 - acc: 0.63 - ETA: 21s - loss: 5.8711 - acc: 0.63 - ETA: 14s - loss: 5.7445 - acc: 0.64 - ETA: 8s - loss: 5.8666 - acc: 0.6360 - ETA: 1s - loss: 5.9691 - acc: 0.629 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 251/500\n",
      "800/800 [==============================] - ETA: 35s - loss: 5.9103 - acc: 0.63 - ETA: 28s - loss: 5.8389 - acc: 0.63 - ETA: 21s - loss: 5.9187 - acc: 0.63 - ETA: 15s - loss: 5.9061 - acc: 0.63 - ETA: 8s - loss: 5.9674 - acc: 0.6298 - ETA: 1s - loss: 5.9719 - acc: 0.629 - 45s 56ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 252/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 6.1537 - acc: 0.61 - ETA: 27s - loss: 6.2503 - acc: 0.61 - ETA: 21s - loss: 6.2097 - acc: 0.61 - ETA: 14s - loss: 6.1097 - acc: 0.62 - ETA: 8s - loss: 6.0664 - acc: 0.6236 - ETA: 1s - loss: 6.0152 - acc: 0.626 - 45s 56ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 253/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.9439 - acc: 0.63 - ETA: 27s - loss: 5.9061 - acc: 0.63 - ETA: 20s - loss: 5.8012 - acc: 0.64 - ETA: 14s - loss: 5.8117 - acc: 0.63 - ETA: 8s - loss: 5.9321 - acc: 0.6320 - ETA: 1s - loss: 5.9887 - acc: 0.628 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 254/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 6.3049 - acc: 0.60 - ETA: 27s - loss: 6.3174 - acc: 0.60 - ETA: 20s - loss: 6.2405 - acc: 0.61 - ETA: 14s - loss: 6.1181 - acc: 0.62 - ETA: 7s - loss: 6.0026 - acc: 0.6276 - ETA: 1s - loss: 6.0180 - acc: 0.626 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 255/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.8431 - acc: 0.63 - ETA: 27s - loss: 5.9271 - acc: 0.63 - ETA: 20s - loss: 5.8879 - acc: 0.63 - ETA: 14s - loss: 5.9838 - acc: 0.62 - ETA: 8s - loss: 5.9892 - acc: 0.6284 - ETA: 1s - loss: 6.0264 - acc: 0.626 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 256/500\n",
      "800/800 [==============================] - ETA: 31s - loss: 5.6668 - acc: 0.64 - ETA: 26s - loss: 5.7550 - acc: 0.64 - ETA: 20s - loss: 5.9775 - acc: 0.62 - ETA: 14s - loss: 5.9082 - acc: 0.63 - ETA: 7s - loss: 5.8549 - acc: 0.6368 - ETA: 1s - loss: 5.9998 - acc: 0.627 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 257/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 6.2881 - acc: 0.60 - ETA: 27s - loss: 5.9145 - acc: 0.63 - ETA: 20s - loss: 5.8599 - acc: 0.63 - ETA: 14s - loss: 5.9313 - acc: 0.63 - ETA: 7s - loss: 6.0127 - acc: 0.6270 - ETA: 1s - loss: 5.9928 - acc: 0.628 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 258/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 6.1202 - acc: 0.62 - ETA: 26s - loss: 5.9942 - acc: 0.62 - ETA: 20s - loss: 5.9970 - acc: 0.62 - ETA: 14s - loss: 6.1768 - acc: 0.61 - ETA: 7s - loss: 6.1051 - acc: 0.6213 - ETA: 1s - loss: 5.9984 - acc: 0.627 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 259/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.1454 - acc: 0.61 - ETA: 27s - loss: 6.1999 - acc: 0.61 - ETA: 20s - loss: 5.9439 - acc: 0.63 - ETA: 14s - loss: 5.9460 - acc: 0.63 - ETA: 7s - loss: 5.9758 - acc: 0.6293 - ETA: 1s - loss: 6.0180 - acc: 0.626 - 43s 53ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 260/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - ETA: 34s - loss: 6.4560 - acc: 0.59 - ETA: 26s - loss: 6.0908 - acc: 0.62 - ETA: 20s - loss: 5.9551 - acc: 0.63 - ETA: 14s - loss: 6.0992 - acc: 0.62 - ETA: 7s - loss: 6.0261 - acc: 0.6261 - ETA: 1s - loss: 6.0418 - acc: 0.625 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 261/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 5.9775 - acc: 0.62 - ETA: 26s - loss: 5.8431 - acc: 0.63 - ETA: 20s - loss: 6.1677 - acc: 0.61 - ETA: 14s - loss: 5.9166 - acc: 0.63 - ETA: 7s - loss: 5.9271 - acc: 0.6323 - ETA: 1s - loss: 6.0194 - acc: 0.626 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 262/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.3300 - acc: 0.60 - ETA: 28s - loss: 6.2965 - acc: 0.60 - ETA: 21s - loss: 6.0698 - acc: 0.62 - ETA: 14s - loss: 6.1642 - acc: 0.61 - ETA: 8s - loss: 6.0782 - acc: 0.6229 - ETA: 1s - loss: 6.0488 - acc: 0.624 - 44s 56ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 263/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.9271 - acc: 0.63 - ETA: 26s - loss: 5.7970 - acc: 0.64 - ETA: 20s - loss: 5.8963 - acc: 0.63 - ETA: 14s - loss: 5.9649 - acc: 0.62 - ETA: 7s - loss: 5.9237 - acc: 0.6325 - ETA: 1s - loss: 6.0292 - acc: 0.626 - 43s 53ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 264/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 5.6417 - acc: 0.65 - ETA: 27s - loss: 5.6878 - acc: 0.64 - ETA: 20s - loss: 5.6948 - acc: 0.64 - ETA: 14s - loss: 6.0425 - acc: 0.62 - ETA: 7s - loss: 6.0715 - acc: 0.6233 - ETA: 1s - loss: 6.0180 - acc: 0.626 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 265/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.3216 - acc: 0.60 - ETA: 27s - loss: 6.2377 - acc: 0.61 - ETA: 20s - loss: 5.9607 - acc: 0.63 - ETA: 14s - loss: 5.9859 - acc: 0.62 - ETA: 7s - loss: 6.0597 - acc: 0.6241 - ETA: 1s - loss: 5.9845 - acc: 0.628 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 266/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.8431 - acc: 0.63 - ETA: 26s - loss: 5.9271 - acc: 0.63 - ETA: 20s - loss: 6.0362 - acc: 0.62 - ETA: 14s - loss: 5.9439 - acc: 0.63 - ETA: 7s - loss: 5.9120 - acc: 0.6332 - ETA: 1s - loss: 5.9635 - acc: 0.630 - 42s 53ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 267/500\n",
      "800/800 [==============================] - ETA: 35s - loss: 5.6920 - acc: 0.64 - ETA: 27s - loss: 5.6123 - acc: 0.65 - ETA: 20s - loss: 5.8991 - acc: 0.63 - ETA: 14s - loss: 5.9817 - acc: 0.62 - ETA: 8s - loss: 5.8532 - acc: 0.6369 - ETA: 1s - loss: 5.9355 - acc: 0.631 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 268/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 5.5241 - acc: 0.65 - ETA: 27s - loss: 5.7550 - acc: 0.64 - ETA: 20s - loss: 5.8152 - acc: 0.63 - ETA: 14s - loss: 5.8431 - acc: 0.63 - ETA: 7s - loss: 5.9355 - acc: 0.6318 - ETA: 1s - loss: 6.0320 - acc: 0.625 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 269/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.0782 - acc: 0.62 - ETA: 27s - loss: 5.9565 - acc: 0.63 - ETA: 20s - loss: 5.8823 - acc: 0.63 - ETA: 14s - loss: 6.0803 - acc: 0.62 - ETA: 7s - loss: 5.9724 - acc: 0.6295 - ETA: 1s - loss: 5.9677 - acc: 0.629 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 270/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 6.2041 - acc: 0.61 - ETA: 26s - loss: 5.9019 - acc: 0.63 - ETA: 20s - loss: 5.7760 - acc: 0.64 - ETA: 14s - loss: 5.7613 - acc: 0.64 - ETA: 7s - loss: 5.8180 - acc: 0.6391 - ETA: 1s - loss: 5.9579 - acc: 0.630 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 271/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.2891 - acc: 0.67 - ETA: 27s - loss: 5.5031 - acc: 0.65 - ETA: 20s - loss: 5.7872 - acc: 0.64 - ETA: 14s - loss: 5.8494 - acc: 0.63 - ETA: 7s - loss: 5.8952 - acc: 0.6343 - ETA: 1s - loss: 5.9873 - acc: 0.628 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 272/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.0362 - acc: 0.62 - ETA: 26s - loss: 6.4308 - acc: 0.60 - ETA: 20s - loss: 6.2797 - acc: 0.61 - ETA: 14s - loss: 6.3363 - acc: 0.60 - ETA: 7s - loss: 6.1940 - acc: 0.6157 - ETA: 1s - loss: 6.0572 - acc: 0.624 - 43s 53ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 273/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 6.5651 - acc: 0.59 - ETA: 27s - loss: 6.1496 - acc: 0.61 - ETA: 21s - loss: 6.0978 - acc: 0.62 - ETA: 14s - loss: 6.0698 - acc: 0.62 - ETA: 8s - loss: 6.0413 - acc: 0.6252 - ETA: 1s - loss: 5.9928 - acc: 0.628 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 274/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 6.6490 - acc: 0.58 - ETA: 26s - loss: 6.3762 - acc: 0.60 - ETA: 20s - loss: 6.0166 - acc: 0.62 - ETA: 14s - loss: 6.0320 - acc: 0.62 - ETA: 7s - loss: 6.0077 - acc: 0.6273 - ETA: 1s - loss: 6.0068 - acc: 0.627 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 275/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.2881 - acc: 0.60 - ETA: 26s - loss: 6.2503 - acc: 0.61 - ETA: 20s - loss: 6.2181 - acc: 0.61 - ETA: 14s - loss: 6.0467 - acc: 0.62 - ETA: 7s - loss: 6.0043 - acc: 0.6275 - ETA: 1s - loss: 5.9831 - acc: 0.628 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 276/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 5.8263 - acc: 0.63 - ETA: 27s - loss: 5.7214 - acc: 0.64 - ETA: 21s - loss: 5.7060 - acc: 0.64 - ETA: 14s - loss: 5.7907 - acc: 0.64 - ETA: 8s - loss: 5.9623 - acc: 0.6301 - ETA: 1s - loss: 5.9313 - acc: 0.632 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 277/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 5.6752 - acc: 0.64 - ETA: 27s - loss: 5.8683 - acc: 0.63 - ETA: 20s - loss: 6.0110 - acc: 0.62 - ETA: 14s - loss: 6.0824 - acc: 0.62 - ETA: 7s - loss: 6.0681 - acc: 0.6235 - ETA: 1s - loss: 6.0068 - acc: 0.627 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 278/500\n",
      "800/800 [==============================] - ETA: 31s - loss: 6.2041 - acc: 0.61 - ETA: 26s - loss: 6.1621 - acc: 0.61 - ETA: 20s - loss: 5.9859 - acc: 0.62 - ETA: 14s - loss: 5.9984 - acc: 0.62 - ETA: 7s - loss: 5.9456 - acc: 0.6311 - ETA: 1s - loss: 6.0110 - acc: 0.627 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 279/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.4486 - acc: 0.66 - ETA: 26s - loss: 5.5367 - acc: 0.65 - ETA: 20s - loss: 5.9439 - acc: 0.63 - ETA: 13s - loss: 5.8473 - acc: 0.63 - ETA: 7s - loss: 5.8834 - acc: 0.6350 - ETA: 1s - loss: 5.9733 - acc: 0.629 - 42s 53ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 280/500\n",
      "800/800 [==============================] - ETA: 35s - loss: 6.1034 - acc: 0.62 - ETA: 27s - loss: 6.2503 - acc: 0.61 - ETA: 20s - loss: 6.1426 - acc: 0.61 - ETA: 14s - loss: 6.0173 - acc: 0.62 - ETA: 8s - loss: 5.9976 - acc: 0.6279 - ETA: 1s - loss: 6.0152 - acc: 0.626 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 281/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 6.0026 - acc: 0.62 - ETA: 26s - loss: 6.1663 - acc: 0.61 - ETA: 20s - loss: 6.1398 - acc: 0.61 - ETA: 14s - loss: 6.1726 - acc: 0.61 - ETA: 7s - loss: 6.1051 - acc: 0.6212 - ETA: 1s - loss: 6.0432 - acc: 0.625 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 282/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - ETA: 32s - loss: 5.5745 - acc: 0.65 - ETA: 26s - loss: 5.7634 - acc: 0.64 - ETA: 20s - loss: 5.7956 - acc: 0.64 - ETA: 14s - loss: 5.9670 - acc: 0.62 - ETA: 7s - loss: 6.0329 - acc: 0.6257 - ETA: 1s - loss: 5.9817 - acc: 0.628 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 283/500\n",
      "800/800 [==============================] - ETA: 36s - loss: 6.1454 - acc: 0.61 - ETA: 27s - loss: 6.1873 - acc: 0.61 - ETA: 21s - loss: 6.2265 - acc: 0.61 - ETA: 14s - loss: 6.0110 - acc: 0.62 - ETA: 8s - loss: 6.0228 - acc: 0.6264 - ETA: 1s - loss: 6.0054 - acc: 0.627 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 284/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 6.1621 - acc: 0.61 - ETA: 26s - loss: 6.1412 - acc: 0.61 - ETA: 20s - loss: 6.1733 - acc: 0.61 - ETA: 14s - loss: 5.9754 - acc: 0.62 - ETA: 7s - loss: 5.9976 - acc: 0.6279 - ETA: 1s - loss: 6.0404 - acc: 0.625 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 285/500\n",
      "800/800 [==============================] - ETA: 31s - loss: 6.5987 - acc: 0.59 - ETA: 27s - loss: 6.0572 - acc: 0.62 - ETA: 20s - loss: 6.2125 - acc: 0.61 - ETA: 14s - loss: 6.1831 - acc: 0.61 - ETA: 7s - loss: 6.0312 - acc: 0.6258 - ETA: 1s - loss: 6.0278 - acc: 0.626 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 286/500\n",
      "800/800 [==============================] - ETA: 31s - loss: 6.6910 - acc: 0.58 - ETA: 26s - loss: 6.3846 - acc: 0.60 - ETA: 20s - loss: 6.0894 - acc: 0.62 - ETA: 14s - loss: 6.1433 - acc: 0.61 - ETA: 7s - loss: 6.0832 - acc: 0.6226 - ETA: 1s - loss: 6.0012 - acc: 0.627 - 43s 53ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 287/500\n",
      "800/800 [==============================] - ETA: 35s - loss: 6.1957 - acc: 0.61 - ETA: 27s - loss: 5.9942 - acc: 0.62 - ETA: 21s - loss: 6.0082 - acc: 0.62 - ETA: 14s - loss: 6.0929 - acc: 0.62 - ETA: 8s - loss: 6.0631 - acc: 0.6239 - ETA: 1s - loss: 5.9914 - acc: 0.628 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 288/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.0362 - acc: 0.62 - ETA: 26s - loss: 5.9565 - acc: 0.63 - ETA: 20s - loss: 5.8711 - acc: 0.63 - ETA: 14s - loss: 5.8410 - acc: 0.63 - ETA: 8s - loss: 5.8482 - acc: 0.6372 - ETA: 1s - loss: 6.0040 - acc: 0.627 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 289/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.5913 - acc: 0.65 - ETA: 26s - loss: 5.9481 - acc: 0.63 - ETA: 20s - loss: 5.9635 - acc: 0.63 - ETA: 14s - loss: 5.9733 - acc: 0.62 - ETA: 7s - loss: 6.0228 - acc: 0.6264 - ETA: 1s - loss: 6.0362 - acc: 0.625 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 290/500\n",
      "800/800 [==============================] - ETA: 36s - loss: 5.5409 - acc: 0.65 - ETA: 27s - loss: 5.9859 - acc: 0.62 - ETA: 21s - loss: 6.0754 - acc: 0.62 - ETA: 14s - loss: 5.9691 - acc: 0.62 - ETA: 8s - loss: 6.0631 - acc: 0.6239 - ETA: 1s - loss: 6.0460 - acc: 0.624 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 291/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 6.7666 - acc: 0.58 - ETA: 27s - loss: 6.3720 - acc: 0.60 - ETA: 20s - loss: 6.2573 - acc: 0.61 - ETA: 14s - loss: 6.0908 - acc: 0.62 - ETA: 8s - loss: 6.0110 - acc: 0.6271 - ETA: 1s - loss: 6.0096 - acc: 0.627 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 292/500\n",
      "800/800 [==============================] - ETA: 31s - loss: 5.7004 - acc: 0.64 - ETA: 27s - loss: 5.8935 - acc: 0.63 - ETA: 20s - loss: 6.0698 - acc: 0.62 - ETA: 14s - loss: 6.0152 - acc: 0.62 - ETA: 8s - loss: 6.0345 - acc: 0.6256 - ETA: 1s - loss: 5.9998 - acc: 0.627 - 44s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 293/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.7592 - acc: 0.64 - ETA: 26s - loss: 5.9607 - acc: 0.63 - ETA: 20s - loss: 5.8655 - acc: 0.63 - ETA: 14s - loss: 5.9355 - acc: 0.63 - ETA: 7s - loss: 5.9859 - acc: 0.6286 - ETA: 1s - loss: 5.9565 - acc: 0.630 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 294/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 6.4224 - acc: 0.60 - ETA: 27s - loss: 6.1663 - acc: 0.61 - ETA: 20s - loss: 6.0978 - acc: 0.62 - ETA: 14s - loss: 6.0656 - acc: 0.62 - ETA: 7s - loss: 6.1017 - acc: 0.6215 - ETA: 1s - loss: 6.0236 - acc: 0.626 - 44s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 295/500\n",
      "800/800 [==============================] - ETA: 31s - loss: 5.2387 - acc: 0.67 - ETA: 26s - loss: 5.5115 - acc: 0.65 - ETA: 20s - loss: 5.6389 - acc: 0.65 - ETA: 14s - loss: 5.8243 - acc: 0.63 - ETA: 7s - loss: 5.9640 - acc: 0.6300 - ETA: 1s - loss: 6.0180 - acc: 0.626 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 296/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.9607 - acc: 0.63 - ETA: 26s - loss: 5.9733 - acc: 0.62 - ETA: 20s - loss: 5.9215 - acc: 0.63 - ETA: 14s - loss: 5.9775 - acc: 0.62 - ETA: 7s - loss: 5.8834 - acc: 0.6350 - ETA: 1s - loss: 5.9621 - acc: 0.630 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 297/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 5.8515 - acc: 0.63 - ETA: 27s - loss: 5.9733 - acc: 0.62 - ETA: 20s - loss: 6.0110 - acc: 0.62 - ETA: 14s - loss: 5.9292 - acc: 0.63 - ETA: 7s - loss: 5.9489 - acc: 0.6309 - ETA: 1s - loss: 5.9691 - acc: 0.629 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 298/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 5.7844 - acc: 0.64 - ETA: 27s - loss: 6.0152 - acc: 0.62 - ETA: 20s - loss: 6.0418 - acc: 0.62 - ETA: 14s - loss: 5.9963 - acc: 0.62 - ETA: 7s - loss: 6.0161 - acc: 0.6268 - ETA: 1s - loss: 6.0096 - acc: 0.627 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 299/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.5483 - acc: 0.59 - ETA: 26s - loss: 6.1579 - acc: 0.61 - ETA: 20s - loss: 5.9439 - acc: 0.63 - ETA: 14s - loss: 5.8431 - acc: 0.63 - ETA: 7s - loss: 5.9707 - acc: 0.6296 - ETA: 1s - loss: 6.0110 - acc: 0.627 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 300/500\n",
      "800/800 [==============================] - ETA: 35s - loss: 6.4644 - acc: 0.59 - ETA: 27s - loss: 5.9607 - acc: 0.63 - ETA: 21s - loss: 6.0978 - acc: 0.62 - ETA: 14s - loss: 6.1642 - acc: 0.61 - ETA: 7s - loss: 6.0060 - acc: 0.6274 - ETA: 1s - loss: 5.9873 - acc: 0.628 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 301/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 5.8012 - acc: 0.64 - ETA: 27s - loss: 5.7340 - acc: 0.64 - ETA: 20s - loss: 6.1985 - acc: 0.61 - ETA: 14s - loss: 6.0446 - acc: 0.62 - ETA: 7s - loss: 6.0580 - acc: 0.6242 - ETA: 1s - loss: 6.0418 - acc: 0.625 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 302/500\n",
      "800/800 [==============================] - ETA: 31s - loss: 6.5483 - acc: 0.59 - ETA: 27s - loss: 6.0698 - acc: 0.62 - ETA: 20s - loss: 5.8627 - acc: 0.63 - ETA: 14s - loss: 5.8305 - acc: 0.63 - ETA: 7s - loss: 5.8599 - acc: 0.6365 - ETA: 1s - loss: 5.9495 - acc: 0.630 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 303/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.2629 - acc: 0.61 - ETA: 26s - loss: 6.1496 - acc: 0.61 - ETA: 20s - loss: 5.9998 - acc: 0.62 - ETA: 14s - loss: 6.0887 - acc: 0.62 - ETA: 7s - loss: 6.0866 - acc: 0.6224 - ETA: 1s - loss: 5.9887 - acc: 0.628 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 304/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - ETA: 34s - loss: 5.9439 - acc: 0.63 - ETA: 27s - loss: 5.9817 - acc: 0.62 - ETA: 20s - loss: 5.9607 - acc: 0.63 - ETA: 14s - loss: 5.9942 - acc: 0.62 - ETA: 8s - loss: 6.1017 - acc: 0.6215 - ETA: 1s - loss: 6.0516 - acc: 0.624 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 305/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 6.6407 - acc: 0.58 - ETA: 27s - loss: 6.1831 - acc: 0.61 - ETA: 20s - loss: 6.0530 - acc: 0.62 - ETA: 14s - loss: 6.0635 - acc: 0.62 - ETA: 7s - loss: 6.0077 - acc: 0.6273 - ETA: 1s - loss: 6.0278 - acc: 0.626 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 306/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.0446 - acc: 0.62 - ETA: 26s - loss: 6.0908 - acc: 0.62 - ETA: 20s - loss: 6.0866 - acc: 0.62 - ETA: 14s - loss: 6.0299 - acc: 0.62 - ETA: 8s - loss: 5.9909 - acc: 0.6283 - ETA: 1s - loss: 5.9831 - acc: 0.628 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 307/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 6.1034 - acc: 0.62 - ETA: 26s - loss: 6.0908 - acc: 0.62 - ETA: 20s - loss: 6.2349 - acc: 0.61 - ETA: 14s - loss: 6.1579 - acc: 0.61 - ETA: 7s - loss: 6.0614 - acc: 0.6240 - ETA: 1s - loss: 6.0670 - acc: 0.623 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 308/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 5.3562 - acc: 0.66 - ETA: 26s - loss: 5.4444 - acc: 0.66 - ETA: 20s - loss: 5.4766 - acc: 0.66 - ETA: 14s - loss: 5.7697 - acc: 0.64 - ETA: 7s - loss: 5.8750 - acc: 0.6355 - ETA: 1s - loss: 6.0278 - acc: 0.626 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 309/500\n",
      "800/800 [==============================] - ETA: 31s - loss: 5.5745 - acc: 0.65 - ETA: 26s - loss: 5.8599 - acc: 0.63 - ETA: 20s - loss: 5.9663 - acc: 0.62 - ETA: 14s - loss: 6.0761 - acc: 0.62 - ETA: 7s - loss: 6.0631 - acc: 0.6239 - ETA: 1s - loss: 5.9928 - acc: 0.628 - 43s 53ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 310/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.3216 - acc: 0.60 - ETA: 26s - loss: 5.9649 - acc: 0.62 - ETA: 20s - loss: 5.7788 - acc: 0.64 - ETA: 14s - loss: 5.8536 - acc: 0.63 - ETA: 7s - loss: 5.7726 - acc: 0.6419 - ETA: 1s - loss: 5.9369 - acc: 0.631 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 311/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.7004 - acc: 0.64 - ETA: 27s - loss: 5.7508 - acc: 0.64 - ETA: 20s - loss: 5.9439 - acc: 0.63 - ETA: 14s - loss: 5.8851 - acc: 0.63 - ETA: 7s - loss: 6.0127 - acc: 0.6270 - ETA: 1s - loss: 6.0152 - acc: 0.626 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 312/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.0614 - acc: 0.62 - ETA: 27s - loss: 5.8767 - acc: 0.63 - ETA: 20s - loss: 6.0726 - acc: 0.62 - ETA: 14s - loss: 6.1223 - acc: 0.62 - ETA: 7s - loss: 6.1269 - acc: 0.6199 - ETA: 1s - loss: 6.0040 - acc: 0.627 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 313/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 5.9019 - acc: 0.63 - ETA: 26s - loss: 5.9817 - acc: 0.62 - ETA: 20s - loss: 6.0166 - acc: 0.62 - ETA: 14s - loss: 5.9942 - acc: 0.62 - ETA: 7s - loss: 5.9623 - acc: 0.6301 - ETA: 1s - loss: 5.9970 - acc: 0.628 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 314/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 5.7592 - acc: 0.64 - ETA: 27s - loss: 5.8389 - acc: 0.63 - ETA: 20s - loss: 5.7088 - acc: 0.64 - ETA: 14s - loss: 5.7109 - acc: 0.64 - ETA: 7s - loss: 5.8431 - acc: 0.6375 - ETA: 1s - loss: 5.9691 - acc: 0.629 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 315/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.2219 - acc: 0.67 - ETA: 27s - loss: 5.6081 - acc: 0.65 - ETA: 20s - loss: 5.7592 - acc: 0.64 - ETA: 14s - loss: 5.9355 - acc: 0.63 - ETA: 7s - loss: 5.8633 - acc: 0.6362 - ETA: 1s - loss: 6.0012 - acc: 0.627 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 316/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.8096 - acc: 0.63 - ETA: 26s - loss: 5.8851 - acc: 0.63 - ETA: 20s - loss: 5.7284 - acc: 0.64 - ETA: 14s - loss: 6.0173 - acc: 0.62 - ETA: 7s - loss: 5.9791 - acc: 0.6291 - ETA: 1s - loss: 6.0530 - acc: 0.624 - 43s 53ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 317/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 5.8347 - acc: 0.63 - ETA: 26s - loss: 5.7340 - acc: 0.64 - ETA: 20s - loss: 5.9019 - acc: 0.63 - ETA: 14s - loss: 6.0215 - acc: 0.62 - ETA: 7s - loss: 6.0261 - acc: 0.6261 - ETA: 1s - loss: 5.9845 - acc: 0.628 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 318/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.3888 - acc: 0.60 - ETA: 27s - loss: 6.3594 - acc: 0.60 - ETA: 20s - loss: 6.2517 - acc: 0.61 - ETA: 14s - loss: 6.0005 - acc: 0.62 - ETA: 8s - loss: 6.0345 - acc: 0.6256 - ETA: 1s - loss: 5.9956 - acc: 0.628 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 319/500\n",
      "800/800 [==============================] - ETA: 31s - loss: 6.0530 - acc: 0.62 - ETA: 26s - loss: 5.8935 - acc: 0.63 - ETA: 20s - loss: 6.1062 - acc: 0.62 - ETA: 14s - loss: 6.0761 - acc: 0.62 - ETA: 7s - loss: 6.0631 - acc: 0.6239 - ETA: 1s - loss: 6.0292 - acc: 0.626 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 320/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 5.5997 - acc: 0.65 - ETA: 26s - loss: 5.5619 - acc: 0.65 - ETA: 20s - loss: 5.8375 - acc: 0.63 - ETA: 14s - loss: 5.8641 - acc: 0.63 - ETA: 7s - loss: 5.9489 - acc: 0.6309 - ETA: 1s - loss: 6.0138 - acc: 0.626 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 321/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 5.7928 - acc: 0.64 - ETA: 27s - loss: 6.0026 - acc: 0.62 - ETA: 20s - loss: 5.9327 - acc: 0.63 - ETA: 14s - loss: 6.1349 - acc: 0.61 - ETA: 7s - loss: 6.1756 - acc: 0.6169 - ETA: 1s - loss: 5.9635 - acc: 0.630 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 322/500\n",
      "800/800 [==============================] - ETA: 31s - loss: 5.6836 - acc: 0.64 - ETA: 26s - loss: 5.6501 - acc: 0.64 - ETA: 20s - loss: 6.0054 - acc: 0.62 - ETA: 14s - loss: 5.9754 - acc: 0.62 - ETA: 7s - loss: 6.0161 - acc: 0.6268 - ETA: 1s - loss: 6.0222 - acc: 0.626 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 323/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 6.5483 - acc: 0.59 - ETA: 26s - loss: 6.1957 - acc: 0.61 - ETA: 20s - loss: 6.1146 - acc: 0.62 - ETA: 14s - loss: 6.0677 - acc: 0.62 - ETA: 7s - loss: 6.0816 - acc: 0.6227 - ETA: 1s - loss: 6.0110 - acc: 0.627 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 324/500\n",
      "800/800 [==============================] - ETA: 35s - loss: 6.2377 - acc: 0.61 - ETA: 27s - loss: 5.9691 - acc: 0.62 - ETA: 20s - loss: 6.0110 - acc: 0.62 - ETA: 14s - loss: 6.0152 - acc: 0.62 - ETA: 7s - loss: 6.1017 - acc: 0.6215 - ETA: 1s - loss: 6.0516 - acc: 0.624 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 325/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.2125 - acc: 0.61 - ETA: 27s - loss: 6.2503 - acc: 0.61 - ETA: 20s - loss: 5.9215 - acc: 0.63 - ETA: 14s - loss: 6.0677 - acc: 0.62 - ETA: 7s - loss: 6.0849 - acc: 0.6225 - ETA: 1s - loss: 6.0138 - acc: 0.626 - 44s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 326/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - ETA: 32s - loss: 6.0026 - acc: 0.62 - ETA: 26s - loss: 5.9523 - acc: 0.63 - ETA: 20s - loss: 6.0054 - acc: 0.62 - ETA: 14s - loss: 6.0677 - acc: 0.62 - ETA: 7s - loss: 5.9859 - acc: 0.6286 - ETA: 1s - loss: 6.0180 - acc: 0.626 - 43s 53ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 327/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 6.2881 - acc: 0.60 - ETA: 27s - loss: 5.8138 - acc: 0.63 - ETA: 20s - loss: 5.8851 - acc: 0.63 - ETA: 14s - loss: 5.9460 - acc: 0.63 - ETA: 7s - loss: 5.9842 - acc: 0.6288 - ETA: 1s - loss: 6.0166 - acc: 0.626 - 43s 53ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 328/500\n",
      "800/800 [==============================] - ETA: 31s - loss: 5.8264 - acc: 0.63 - ETA: 27s - loss: 5.9523 - acc: 0.63 - ETA: 20s - loss: 6.1761 - acc: 0.61 - ETA: 14s - loss: 6.0992 - acc: 0.62 - ETA: 8s - loss: 6.1269 - acc: 0.6199 - ETA: 1s - loss: 5.9705 - acc: 0.629 - 45s 57ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 329/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.9271 - acc: 0.63 - ETA: 26s - loss: 6.1412 - acc: 0.61 - ETA: 20s - loss: 6.0026 - acc: 0.62 - ETA: 14s - loss: 6.0236 - acc: 0.62 - ETA: 7s - loss: 5.9959 - acc: 0.6280 - ETA: 1s - loss: 6.0292 - acc: 0.626 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 330/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 6.2713 - acc: 0.61 - ETA: 26s - loss: 6.1286 - acc: 0.61 - ETA: 21s - loss: 6.0922 - acc: 0.62 - ETA: 14s - loss: 5.8410 - acc: 0.63 - ETA: 8s - loss: 5.9170 - acc: 0.6329 - ETA: 1s - loss: 5.9775 - acc: 0.629 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 331/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 5.8180 - acc: 0.63 - ETA: 27s - loss: 6.1412 - acc: 0.61 - ETA: 20s - loss: 5.9747 - acc: 0.62 - ETA: 14s - loss: 6.1055 - acc: 0.62 - ETA: 8s - loss: 6.0010 - acc: 0.6277 - ETA: 1s - loss: 6.0152 - acc: 0.626 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 332/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 6.1621 - acc: 0.61 - ETA: 27s - loss: 6.1160 - acc: 0.62 - ETA: 20s - loss: 5.8963 - acc: 0.63 - ETA: 14s - loss: 5.9942 - acc: 0.62 - ETA: 8s - loss: 5.9775 - acc: 0.6292 - ETA: 1s - loss: 6.0418 - acc: 0.625 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 333/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.2461 - acc: 0.61 - ETA: 26s - loss: 6.2461 - acc: 0.61 - ETA: 20s - loss: 6.2629 - acc: 0.61 - ETA: 14s - loss: 6.1391 - acc: 0.61 - ETA: 7s - loss: 5.9942 - acc: 0.6281 - ETA: 1s - loss: 5.9691 - acc: 0.629 - 42s 53ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 334/500\n",
      "800/800 [==============================] - ETA: 35s - loss: 5.9355 - acc: 0.63 - ETA: 28s - loss: 5.9061 - acc: 0.63 - ETA: 21s - loss: 6.1929 - acc: 0.61 - ETA: 14s - loss: 6.0950 - acc: 0.62 - ETA: 8s - loss: 6.0866 - acc: 0.6224 - ETA: 1s - loss: 5.9761 - acc: 0.629 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 335/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 5.6081 - acc: 0.65 - ETA: 27s - loss: 5.6543 - acc: 0.64 - ETA: 20s - loss: 5.6277 - acc: 0.65 - ETA: 14s - loss: 5.7676 - acc: 0.64 - ETA: 7s - loss: 5.8196 - acc: 0.6390 - ETA: 1s - loss: 5.9691 - acc: 0.629 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 336/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.8431 - acc: 0.63 - ETA: 26s - loss: 5.9859 - acc: 0.62 - ETA: 20s - loss: 5.9523 - acc: 0.63 - ETA: 13s - loss: 6.1517 - acc: 0.61 - ETA: 7s - loss: 6.0161 - acc: 0.6268 - ETA: 1s - loss: 5.9467 - acc: 0.631 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 337/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 5.9355 - acc: 0.63 - ETA: 27s - loss: 5.7508 - acc: 0.64 - ETA: 20s - loss: 5.7788 - acc: 0.64 - ETA: 14s - loss: 5.9187 - acc: 0.63 - ETA: 7s - loss: 6.0010 - acc: 0.6277 - ETA: 1s - loss: 5.9831 - acc: 0.628 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 338/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.3972 - acc: 0.60 - ETA: 26s - loss: 6.2125 - acc: 0.61 - ETA: 20s - loss: 5.8823 - acc: 0.63 - ETA: 14s - loss: 5.9187 - acc: 0.63 - ETA: 8s - loss: 5.9120 - acc: 0.6332 - ETA: 1s - loss: 5.9579 - acc: 0.630 - 44s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 339/500\n",
      "800/800 [==============================] - ETA: 31s - loss: 6.5483 - acc: 0.59 - ETA: 26s - loss: 6.0740 - acc: 0.62 - ETA: 20s - loss: 6.1537 - acc: 0.61 - ETA: 14s - loss: 6.1978 - acc: 0.61 - ETA: 7s - loss: 6.0597 - acc: 0.6241 - ETA: 1s - loss: 6.0124 - acc: 0.627 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 340/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 5.7760 - acc: 0.64 - ETA: 26s - loss: 5.7550 - acc: 0.64 - ETA: 21s - loss: 5.8851 - acc: 0.63 - ETA: 14s - loss: 5.9040 - acc: 0.63 - ETA: 7s - loss: 5.9086 - acc: 0.6334 - ETA: 1s - loss: 5.9998 - acc: 0.627 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 341/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 5.8767 - acc: 0.63 - ETA: 27s - loss: 5.8683 - acc: 0.63 - ETA: 20s - loss: 5.9887 - acc: 0.62 - ETA: 14s - loss: 5.9963 - acc: 0.62 - ETA: 7s - loss: 5.9321 - acc: 0.6320 - ETA: 1s - loss: 5.9733 - acc: 0.629 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 342/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.8431 - acc: 0.63 - ETA: 27s - loss: 6.0404 - acc: 0.62 - ETA: 20s - loss: 5.7816 - acc: 0.64 - ETA: 14s - loss: 5.8809 - acc: 0.63 - ETA: 7s - loss: 5.9103 - acc: 0.6333 - ETA: 1s - loss: 5.9607 - acc: 0.630 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 343/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 6.3972 - acc: 0.60 - ETA: 26s - loss: 6.0236 - acc: 0.62 - ETA: 20s - loss: 5.9831 - acc: 0.62 - ETA: 14s - loss: 5.9984 - acc: 0.62 - ETA: 7s - loss: 5.9691 - acc: 0.6297 - ETA: 1s - loss: 6.0376 - acc: 0.625 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 344/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 6.6742 - acc: 0.58 - ETA: 27s - loss: 6.2545 - acc: 0.61 - ETA: 20s - loss: 6.1286 - acc: 0.61 - ETA: 14s - loss: 6.0950 - acc: 0.62 - ETA: 7s - loss: 6.1386 - acc: 0.6192 - ETA: 1s - loss: 6.0348 - acc: 0.625 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 345/500\n",
      "800/800 [==============================] - ETA: 31s - loss: 5.9607 - acc: 0.63 - ETA: 27s - loss: 5.9397 - acc: 0.63 - ETA: 20s - loss: 6.0866 - acc: 0.62 - ETA: 14s - loss: 6.0782 - acc: 0.62 - ETA: 8s - loss: 6.0816 - acc: 0.6227 - ETA: 1s - loss: 6.0180 - acc: 0.626 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 346/500\n",
      "800/800 [==============================] - ETA: 30s - loss: 6.3888 - acc: 0.60 - ETA: 26s - loss: 5.9145 - acc: 0.63 - ETA: 20s - loss: 6.0810 - acc: 0.62 - ETA: 14s - loss: 5.9775 - acc: 0.62 - ETA: 8s - loss: 5.9355 - acc: 0.6318 - ETA: 1s - loss: 6.0278 - acc: 0.626 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 347/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.9019 - acc: 0.63 - ETA: 26s - loss: 6.0026 - acc: 0.62 - ETA: 20s - loss: 5.8152 - acc: 0.63 - ETA: 14s - loss: 6.0488 - acc: 0.62 - ETA: 7s - loss: 6.0329 - acc: 0.6257 - ETA: 1s - loss: 6.0320 - acc: 0.625 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 348/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - ETA: 34s - loss: 6.4056 - acc: 0.60 - ETA: 27s - loss: 6.2881 - acc: 0.60 - ETA: 20s - loss: 6.0390 - acc: 0.62 - ETA: 14s - loss: 5.9628 - acc: 0.63 - ETA: 7s - loss: 5.9674 - acc: 0.6298 - ETA: 1s - loss: 6.0222 - acc: 0.626 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 349/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.6920 - acc: 0.64 - ETA: 26s - loss: 6.0278 - acc: 0.62 - ETA: 20s - loss: 6.0166 - acc: 0.62 - ETA: 14s - loss: 5.8389 - acc: 0.63 - ETA: 7s - loss: 5.9288 - acc: 0.6322 - ETA: 1s - loss: 5.9593 - acc: 0.630 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 350/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 5.7760 - acc: 0.64 - ETA: 26s - loss: 6.1118 - acc: 0.62 - ETA: 20s - loss: 6.0222 - acc: 0.62 - ETA: 14s - loss: 5.9271 - acc: 0.63 - ETA: 8s - loss: 5.9472 - acc: 0.6310 - ETA: 1s - loss: 6.0012 - acc: 0.627 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 351/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 6.0278 - acc: 0.62 - ETA: 27s - loss: 5.8641 - acc: 0.63 - ETA: 20s - loss: 6.0474 - acc: 0.62 - ETA: 14s - loss: 6.0257 - acc: 0.62 - ETA: 7s - loss: 6.0715 - acc: 0.6233 - ETA: 1s - loss: 6.0026 - acc: 0.627 - 44s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 352/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 6.7918 - acc: 0.57 - ETA: 27s - loss: 6.1621 - acc: 0.61 - ETA: 20s - loss: 6.1761 - acc: 0.61 - ETA: 14s - loss: 6.1684 - acc: 0.61 - ETA: 8s - loss: 6.0732 - acc: 0.6232 - ETA: 1s - loss: 6.0376 - acc: 0.625 - 44s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 353/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 6.1118 - acc: 0.62 - ETA: 27s - loss: 6.0446 - acc: 0.62 - ETA: 20s - loss: 6.0362 - acc: 0.62 - ETA: 14s - loss: 5.9796 - acc: 0.62 - ETA: 8s - loss: 6.0345 - acc: 0.6256 - ETA: 1s - loss: 6.0040 - acc: 0.627 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 354/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.5661 - acc: 0.65 - ETA: 26s - loss: 6.0740 - acc: 0.62 - ETA: 20s - loss: 6.3468 - acc: 0.60 - ETA: 14s - loss: 6.1558 - acc: 0.61 - ETA: 7s - loss: 6.0916 - acc: 0.6221 - ETA: 1s - loss: 6.0446 - acc: 0.625 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 355/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 6.5483 - acc: 0.59 - ETA: 26s - loss: 6.1579 - acc: 0.61 - ETA: 20s - loss: 6.0194 - acc: 0.62 - ETA: 14s - loss: 5.8830 - acc: 0.63 - ETA: 7s - loss: 6.0329 - acc: 0.6257 - ETA: 1s - loss: 5.9970 - acc: 0.628 - 43s 53ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 356/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.8180 - acc: 0.63 - ETA: 27s - loss: 5.8851 - acc: 0.63 - ETA: 20s - loss: 6.0950 - acc: 0.62 - ETA: 14s - loss: 5.9796 - acc: 0.62 - ETA: 8s - loss: 5.9540 - acc: 0.6306 - ETA: 1s - loss: 5.9761 - acc: 0.629 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 357/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.9439 - acc: 0.63 - ETA: 25s - loss: 6.0908 - acc: 0.62 - ETA: 20s - loss: 6.0586 - acc: 0.62 - ETA: 14s - loss: 5.9942 - acc: 0.62 - ETA: 7s - loss: 6.0278 - acc: 0.6260 - ETA: 1s - loss: 6.0502 - acc: 0.624 - 43s 53ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 358/500\n",
      "800/800 [==============================] - ETA: 35s - loss: 6.0614 - acc: 0.62 - ETA: 27s - loss: 6.2629 - acc: 0.61 - ETA: 20s - loss: 6.1426 - acc: 0.61 - ETA: 14s - loss: 6.1454 - acc: 0.61 - ETA: 7s - loss: 6.0127 - acc: 0.6270 - ETA: 1s - loss: 6.0558 - acc: 0.624 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 359/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 6.0614 - acc: 0.62 - ETA: 29s - loss: 6.2209 - acc: 0.61 - ETA: 22s - loss: 6.1761 - acc: 0.61 - ETA: 15s - loss: 6.1537 - acc: 0.61 - ETA: 8s - loss: 6.1386 - acc: 0.6192 - ETA: 1s - loss: 6.0348 - acc: 0.625 - 44s 56ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 360/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.1873 - acc: 0.61 - ETA: 26s - loss: 5.9229 - acc: 0.63 - ETA: 20s - loss: 6.1314 - acc: 0.61 - ETA: 14s - loss: 6.0152 - acc: 0.62 - ETA: 7s - loss: 5.9926 - acc: 0.6282 - ETA: 1s - loss: 5.9761 - acc: 0.629 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 361/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 5.8012 - acc: 0.64 - ETA: 27s - loss: 5.8515 - acc: 0.63 - ETA: 21s - loss: 5.9103 - acc: 0.63 - ETA: 14s - loss: 5.9628 - acc: 0.63 - ETA: 8s - loss: 5.9942 - acc: 0.6281 - ETA: 1s - loss: 6.0180 - acc: 0.626 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 362/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.5745 - acc: 0.65 - ETA: 27s - loss: 6.1915 - acc: 0.61 - ETA: 20s - loss: 5.9942 - acc: 0.62 - ETA: 14s - loss: 6.0593 - acc: 0.62 - ETA: 7s - loss: 5.9791 - acc: 0.6291 - ETA: 1s - loss: 6.0250 - acc: 0.626 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 363/500\n",
      "800/800 [==============================] - ETA: 31s - loss: 6.1202 - acc: 0.62 - ETA: 26s - loss: 6.1957 - acc: 0.61 - ETA: 21s - loss: 6.0698 - acc: 0.62 - ETA: 14s - loss: 6.0887 - acc: 0.62 - ETA: 8s - loss: 6.0429 - acc: 0.6251 - ETA: 1s - loss: 6.0250 - acc: 0.626 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 364/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.2461 - acc: 0.61 - ETA: 26s - loss: 5.9942 - acc: 0.62 - ETA: 20s - loss: 6.2181 - acc: 0.61 - ETA: 14s - loss: 5.9838 - acc: 0.62 - ETA: 7s - loss: 6.0933 - acc: 0.6220 - ETA: 1s - loss: 6.0782 - acc: 0.622 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 365/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 6.0698 - acc: 0.62 - ETA: 27s - loss: 6.0992 - acc: 0.62 - ETA: 20s - loss: 6.1454 - acc: 0.61 - ETA: 14s - loss: 6.0614 - acc: 0.62 - ETA: 7s - loss: 6.0580 - acc: 0.6242 - ETA: 1s - loss: 6.0068 - acc: 0.627 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 366/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.0110 - acc: 0.62 - ETA: 26s - loss: 6.2377 - acc: 0.61 - ETA: 20s - loss: 5.9775 - acc: 0.62 - ETA: 14s - loss: 5.9984 - acc: 0.62 - ETA: 8s - loss: 6.0413 - acc: 0.6252 - ETA: 1s - loss: 6.0082 - acc: 0.627 - 44s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 367/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.5735 - acc: 0.59 - ETA: 25s - loss: 6.4644 - acc: 0.59 - ETA: 20s - loss: 6.3468 - acc: 0.60 - ETA: 14s - loss: 6.2419 - acc: 0.61 - ETA: 7s - loss: 6.0564 - acc: 0.6243 - ETA: 1s - loss: 5.9789 - acc: 0.629 - 43s 53ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 368/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 5.9271 - acc: 0.63 - ETA: 27s - loss: 6.0320 - acc: 0.62 - ETA: 20s - loss: 5.8823 - acc: 0.63 - ETA: 14s - loss: 6.0341 - acc: 0.62 - ETA: 7s - loss: 5.9775 - acc: 0.6292 - ETA: 1s - loss: 6.0026 - acc: 0.627 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 369/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.3720 - acc: 0.60 - ETA: 26s - loss: 6.5105 - acc: 0.59 - ETA: 20s - loss: 6.2601 - acc: 0.61 - ETA: 14s - loss: 6.1684 - acc: 0.61 - ETA: 7s - loss: 6.1403 - acc: 0.6191 - ETA: 1s - loss: 6.0348 - acc: 0.625 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 370/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - ETA: 31s - loss: 5.9691 - acc: 0.62 - ETA: 26s - loss: 5.7130 - acc: 0.64 - ETA: 20s - loss: 5.9019 - acc: 0.63 - ETA: 14s - loss: 5.9103 - acc: 0.63 - ETA: 7s - loss: 5.9607 - acc: 0.6302 - ETA: 1s - loss: 5.9803 - acc: 0.629 - 42s 53ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 371/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 5.9775 - acc: 0.62 - ETA: 27s - loss: 5.9229 - acc: 0.63 - ETA: 20s - loss: 5.8543 - acc: 0.63 - ETA: 14s - loss: 5.9229 - acc: 0.63 - ETA: 7s - loss: 5.9304 - acc: 0.6321 - ETA: 1s - loss: 6.0138 - acc: 0.626 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 372/500\n",
      "800/800 [==============================] - ETA: 31s - loss: 6.3720 - acc: 0.60 - ETA: 26s - loss: 6.3804 - acc: 0.60 - ETA: 20s - loss: 6.1593 - acc: 0.61 - ETA: 14s - loss: 6.0698 - acc: 0.62 - ETA: 7s - loss: 6.0043 - acc: 0.6275 - ETA: 1s - loss: 6.0012 - acc: 0.627 - 43s 53ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 373/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 6.1454 - acc: 0.61 - ETA: 26s - loss: 6.2167 - acc: 0.61 - ETA: 21s - loss: 6.0978 - acc: 0.62 - ETA: 14s - loss: 6.0425 - acc: 0.62 - ETA: 8s - loss: 6.0127 - acc: 0.6270 - ETA: 1s - loss: 6.0474 - acc: 0.624 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 374/500\n",
      "800/800 [==============================] - ETA: 35s - loss: 5.7676 - acc: 0.64 - ETA: 27s - loss: 5.6752 - acc: 0.64 - ETA: 21s - loss: 5.9075 - acc: 0.63 - ETA: 14s - loss: 5.9880 - acc: 0.62 - ETA: 8s - loss: 5.8985 - acc: 0.6341 - ETA: 1s - loss: 5.9425 - acc: 0.631 - 45s 57ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 375/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 6.0530 - acc: 0.62 - ETA: 27s - loss: 6.1873 - acc: 0.61 - ETA: 20s - loss: 6.0502 - acc: 0.62 - ETA: 14s - loss: 5.9963 - acc: 0.62 - ETA: 8s - loss: 6.0026 - acc: 0.6276 - ETA: 1s - loss: 6.0082 - acc: 0.627 - 44s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 376/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 6.0278 - acc: 0.62 - ETA: 27s - loss: 5.9187 - acc: 0.63 - ETA: 20s - loss: 5.9803 - acc: 0.62 - ETA: 14s - loss: 6.0551 - acc: 0.62 - ETA: 7s - loss: 6.0631 - acc: 0.6239 - ETA: 1s - loss: 5.9914 - acc: 0.628 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 377/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 6.1537 - acc: 0.61 - ETA: 27s - loss: 6.1621 - acc: 0.61 - ETA: 20s - loss: 6.0530 - acc: 0.62 - ETA: 14s - loss: 6.1223 - acc: 0.62 - ETA: 7s - loss: 6.0648 - acc: 0.6237 - ETA: 1s - loss: 6.0334 - acc: 0.625 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 378/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 5.6333 - acc: 0.65 - ETA: 27s - loss: 5.9439 - acc: 0.63 - ETA: 20s - loss: 5.9719 - acc: 0.62 - ETA: 14s - loss: 6.0698 - acc: 0.62 - ETA: 8s - loss: 6.0916 - acc: 0.6221 - ETA: 1s - loss: 6.0236 - acc: 0.626 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 379/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.6836 - acc: 0.64 - ETA: 26s - loss: 5.9733 - acc: 0.62 - ETA: 20s - loss: 6.0838 - acc: 0.62 - ETA: 14s - loss: 6.0803 - acc: 0.62 - ETA: 7s - loss: 6.1084 - acc: 0.6210 - ETA: 1s - loss: 6.0138 - acc: 0.626 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 380/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.3552 - acc: 0.60 - ETA: 27s - loss: 6.0698 - acc: 0.62 - ETA: 20s - loss: 6.2013 - acc: 0.61 - ETA: 14s - loss: 6.0530 - acc: 0.62 - ETA: 8s - loss: 5.9422 - acc: 0.6314 - ETA: 1s - loss: 6.0376 - acc: 0.625 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 381/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.3468 - acc: 0.60 - ETA: 26s - loss: 6.2713 - acc: 0.61 - ETA: 20s - loss: 6.1537 - acc: 0.61 - ETA: 14s - loss: 5.9712 - acc: 0.62 - ETA: 7s - loss: 6.0060 - acc: 0.6274 - ETA: 1s - loss: 5.9271 - acc: 0.632 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 382/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.3478 - acc: 0.66 - ETA: 26s - loss: 5.9691 - acc: 0.62 - ETA: 20s - loss: 6.0138 - acc: 0.62 - ETA: 14s - loss: 6.1118 - acc: 0.62 - ETA: 8s - loss: 6.1353 - acc: 0.6194 - ETA: 1s - loss: 6.0250 - acc: 0.626 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 383/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.9103 - acc: 0.63 - ETA: 26s - loss: 5.9145 - acc: 0.63 - ETA: 20s - loss: 5.7368 - acc: 0.64 - ETA: 13s - loss: 5.9313 - acc: 0.63 - ETA: 7s - loss: 5.8801 - acc: 0.6352 - ETA: 1s - loss: 5.8529 - acc: 0.636 - 43s 53ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 384/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 6.0950 - acc: 0.62 - ETA: 27s - loss: 6.0362 - acc: 0.62 - ETA: 21s - loss: 5.9803 - acc: 0.62 - ETA: 14s - loss: 6.0194 - acc: 0.62 - ETA: 8s - loss: 6.0211 - acc: 0.6265 - ETA: 1s - loss: 6.0166 - acc: 0.626 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 385/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 5.8683 - acc: 0.63 - ETA: 27s - loss: 5.9900 - acc: 0.62 - ETA: 20s - loss: 5.9327 - acc: 0.63 - ETA: 14s - loss: 5.9250 - acc: 0.63 - ETA: 8s - loss: 6.0127 - acc: 0.6270 - ETA: 1s - loss: 6.0208 - acc: 0.626 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 386/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.3982 - acc: 0.66 - ETA: 27s - loss: 5.8809 - acc: 0.63 - ETA: 20s - loss: 5.9411 - acc: 0.63 - ETA: 14s - loss: 6.1286 - acc: 0.61 - ETA: 7s - loss: 6.1185 - acc: 0.6204 - ETA: 1s - loss: 5.9984 - acc: 0.627 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 387/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 5.3143 - acc: 0.67 - ETA: 28s - loss: 5.5871 - acc: 0.65 - ETA: 21s - loss: 5.6613 - acc: 0.64 - ETA: 14s - loss: 5.8956 - acc: 0.63 - ETA: 8s - loss: 5.9338 - acc: 0.6319 - ETA: 1s - loss: 6.0068 - acc: 0.627 - 45s 56ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 388/500\n",
      "800/800 [==============================] - ETA: 36s - loss: 6.3300 - acc: 0.60 - ETA: 28s - loss: 6.3468 - acc: 0.60 - ETA: 21s - loss: 6.1286 - acc: 0.61 - ETA: 14s - loss: 6.1684 - acc: 0.61 - ETA: 8s - loss: 6.0933 - acc: 0.6220 - ETA: 1s - loss: 6.0026 - acc: 0.627 - 45s 56ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 389/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 6.0530 - acc: 0.62 - ETA: 26s - loss: 6.0656 - acc: 0.62 - ETA: 20s - loss: 6.1873 - acc: 0.61 - ETA: 14s - loss: 6.1999 - acc: 0.61 - ETA: 7s - loss: 6.0547 - acc: 0.6244 - ETA: 1s - loss: 6.0208 - acc: 0.626 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 390/500\n",
      "800/800 [==============================] - ETA: 31s - loss: 5.9103 - acc: 0.63 - ETA: 26s - loss: 5.8473 - acc: 0.63 - ETA: 20s - loss: 5.8795 - acc: 0.63 - ETA: 14s - loss: 6.0089 - acc: 0.62 - ETA: 7s - loss: 6.1302 - acc: 0.6197 - ETA: 1s - loss: 6.0250 - acc: 0.626 - 43s 53ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 391/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 5.5829 - acc: 0.65 - ETA: 26s - loss: 5.8180 - acc: 0.63 - ETA: 20s - loss: 5.9523 - acc: 0.63 - ETA: 14s - loss: 5.8326 - acc: 0.63 - ETA: 7s - loss: 5.9489 - acc: 0.6309 - ETA: 1s - loss: 5.9481 - acc: 0.631 - 43s 53ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 392/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - ETA: 33s - loss: 5.6417 - acc: 0.65 - ETA: 26s - loss: 6.0992 - acc: 0.62 - ETA: 21s - loss: 5.9887 - acc: 0.62 - ETA: 14s - loss: 5.9376 - acc: 0.63 - ETA: 8s - loss: 5.9892 - acc: 0.6284 - ETA: 1s - loss: 6.0152 - acc: 0.626 - 44s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 393/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.8096 - acc: 0.63 - ETA: 27s - loss: 5.8389 - acc: 0.63 - ETA: 20s - loss: 6.0278 - acc: 0.62 - ETA: 14s - loss: 5.9607 - acc: 0.63 - ETA: 7s - loss: 6.0161 - acc: 0.6268 - ETA: 1s - loss: 6.0264 - acc: 0.626 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 394/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 6.1202 - acc: 0.62 - ETA: 27s - loss: 6.2965 - acc: 0.60 - ETA: 20s - loss: 6.2321 - acc: 0.61 - ETA: 14s - loss: 6.1349 - acc: 0.61 - ETA: 8s - loss: 6.0379 - acc: 0.6254 - ETA: 1s - loss: 6.0460 - acc: 0.624 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 395/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 6.5231 - acc: 0.59 - ETA: 27s - loss: 6.0656 - acc: 0.62 - ETA: 21s - loss: 5.9970 - acc: 0.62 - ETA: 14s - loss: 6.0173 - acc: 0.62 - ETA: 7s - loss: 6.0765 - acc: 0.6230 - ETA: 1s - loss: 6.0362 - acc: 0.625 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 396/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.0446 - acc: 0.62 - ETA: 26s - loss: 6.3468 - acc: 0.60 - ETA: 20s - loss: 6.1062 - acc: 0.62 - ETA: 14s - loss: 6.1013 - acc: 0.62 - ETA: 7s - loss: 6.0664 - acc: 0.6236 - ETA: 1s - loss: 6.0628 - acc: 0.623 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 397/500\n",
      "800/800 [==============================] - ETA: 31s - loss: 5.6081 - acc: 0.65 - ETA: 26s - loss: 6.2335 - acc: 0.61 - ETA: 20s - loss: 5.9886 - acc: 0.62 - ETA: 14s - loss: 5.9607 - acc: 0.63 - ETA: 7s - loss: 5.9942 - acc: 0.6281 - ETA: 1s - loss: 5.9859 - acc: 0.628 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 398/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 5.7760 - acc: 0.64 - ETA: 26s - loss: 6.1705 - acc: 0.61 - ETA: 20s - loss: 6.0558 - acc: 0.62 - ETA: 14s - loss: 6.0026 - acc: 0.62 - ETA: 7s - loss: 6.0245 - acc: 0.6262 - ETA: 1s - loss: 6.0138 - acc: 0.626 - 43s 53ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 399/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.8599 - acc: 0.63 - ETA: 26s - loss: 5.8557 - acc: 0.63 - ETA: 20s - loss: 6.0614 - acc: 0.62 - ETA: 14s - loss: 6.1181 - acc: 0.62 - ETA: 7s - loss: 6.1302 - acc: 0.6197 - ETA: 1s - loss: 6.0110 - acc: 0.627 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 400/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.8767 - acc: 0.63 - ETA: 26s - loss: 6.1034 - acc: 0.62 - ETA: 20s - loss: 5.9831 - acc: 0.62 - ETA: 13s - loss: 6.0467 - acc: 0.62 - ETA: 7s - loss: 6.0547 - acc: 0.6244 - ETA: 1s - loss: 5.9495 - acc: 0.630 - 43s 53ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 401/500\n",
      "800/800 [==============================] - ETA: 35s - loss: 5.9355 - acc: 0.63 - ETA: 27s - loss: 5.6878 - acc: 0.64 - ETA: 21s - loss: 5.7928 - acc: 0.64 - ETA: 14s - loss: 5.8180 - acc: 0.63 - ETA: 7s - loss: 5.9623 - acc: 0.6301 - ETA: 1s - loss: 6.0222 - acc: 0.626 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 402/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 5.8935 - acc: 0.63 - ETA: 27s - loss: 5.9900 - acc: 0.62 - ETA: 20s - loss: 5.7088 - acc: 0.64 - ETA: 14s - loss: 5.8914 - acc: 0.63 - ETA: 8s - loss: 5.9137 - acc: 0.6331 - ETA: 1s - loss: 5.9942 - acc: 0.628 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 403/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.0026 - acc: 0.62 - ETA: 26s - loss: 5.8096 - acc: 0.63 - ETA: 20s - loss: 5.9775 - acc: 0.62 - ETA: 14s - loss: 5.9460 - acc: 0.63 - ETA: 7s - loss: 5.9590 - acc: 0.6303 - ETA: 1s - loss: 6.0124 - acc: 0.627 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 404/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 5.8683 - acc: 0.63 - ETA: 27s - loss: 5.6752 - acc: 0.64 - ETA: 20s - loss: 5.7564 - acc: 0.64 - ETA: 14s - loss: 5.8725 - acc: 0.63 - ETA: 7s - loss: 5.9791 - acc: 0.6291 - ETA: 1s - loss: 5.9928 - acc: 0.628 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 405/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 6.3636 - acc: 0.60 - ETA: 27s - loss: 5.8263 - acc: 0.63 - ETA: 20s - loss: 5.8152 - acc: 0.63 - ETA: 14s - loss: 5.8557 - acc: 0.63 - ETA: 8s - loss: 5.9758 - acc: 0.6293 - ETA: 1s - loss: 6.0208 - acc: 0.626 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 406/500\n",
      "800/800 [==============================] - ETA: 31s - loss: 6.3049 - acc: 0.60 - ETA: 27s - loss: 6.0908 - acc: 0.62 - ETA: 20s - loss: 6.2153 - acc: 0.61 - ETA: 14s - loss: 6.0971 - acc: 0.62 - ETA: 8s - loss: 6.0278 - acc: 0.6260 - ETA: 1s - loss: 6.0404 - acc: 0.625 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 407/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.9691 - acc: 0.62 - ETA: 26s - loss: 5.8641 - acc: 0.63 - ETA: 20s - loss: 5.8991 - acc: 0.63 - ETA: 14s - loss: 5.9900 - acc: 0.62 - ETA: 7s - loss: 5.9926 - acc: 0.6282 - ETA: 1s - loss: 5.9775 - acc: 0.629 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 408/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 6.0614 - acc: 0.62 - ETA: 26s - loss: 6.2041 - acc: 0.61 - ETA: 20s - loss: 6.1426 - acc: 0.61 - ETA: 14s - loss: 6.0551 - acc: 0.62 - ETA: 8s - loss: 6.1135 - acc: 0.6207 - ETA: 1s - loss: 6.0544 - acc: 0.624 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 409/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 6.0194 - acc: 0.62 - ETA: 27s - loss: 6.2755 - acc: 0.61 - ETA: 20s - loss: 6.0474 - acc: 0.62 - ETA: 14s - loss: 5.9271 - acc: 0.63 - ETA: 8s - loss: 5.9540 - acc: 0.6306 - ETA: 1s - loss: 6.0082 - acc: 0.627 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 410/500\n",
      "800/800 [==============================] - ETA: 31s - loss: 5.8683 - acc: 0.63 - ETA: 26s - loss: 6.1370 - acc: 0.61 - ETA: 20s - loss: 6.1705 - acc: 0.61 - ETA: 14s - loss: 6.1579 - acc: 0.61 - ETA: 7s - loss: 6.0899 - acc: 0.6222 - ETA: 1s - loss: 6.0376 - acc: 0.625 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 411/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 5.3898 - acc: 0.66 - ETA: 26s - loss: 5.6249 - acc: 0.65 - ETA: 20s - loss: 5.7760 - acc: 0.64 - ETA: 14s - loss: 5.9208 - acc: 0.63 - ETA: 8s - loss: 6.0127 - acc: 0.6270 - ETA: 1s - loss: 5.9984 - acc: 0.627 - 45s 56ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 412/500\n",
      "800/800 [==============================] - ETA: 35s - loss: 6.6826 - acc: 0.58 - ETA: 27s - loss: 6.4434 - acc: 0.60 - ETA: 21s - loss: 6.2265 - acc: 0.61 - ETA: 14s - loss: 6.0908 - acc: 0.62 - ETA: 7s - loss: 5.9355 - acc: 0.6318 - ETA: 1s - loss: 6.0208 - acc: 0.626 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 413/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 6.2797 - acc: 0.61 - ETA: 27s - loss: 5.8054 - acc: 0.63 - ETA: 20s - loss: 5.9327 - acc: 0.63 - ETA: 14s - loss: 5.9628 - acc: 0.63 - ETA: 8s - loss: 5.8583 - acc: 0.6366 - ETA: 1s - loss: 6.0250 - acc: 0.626 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 414/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - ETA: 32s - loss: 6.0698 - acc: 0.62 - ETA: 27s - loss: 5.9397 - acc: 0.63 - ETA: 20s - loss: 5.9579 - acc: 0.63 - ETA: 14s - loss: 5.9166 - acc: 0.63 - ETA: 7s - loss: 5.9724 - acc: 0.6295 - ETA: 1s - loss: 5.9845 - acc: 0.628 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 415/500\n",
      "800/800 [==============================] - ETA: 31s - loss: 5.8935 - acc: 0.63 - ETA: 26s - loss: 5.9607 - acc: 0.63 - ETA: 20s - loss: 5.8263 - acc: 0.63 - ETA: 14s - loss: 5.9649 - acc: 0.62 - ETA: 7s - loss: 6.0295 - acc: 0.6259 - ETA: 1s - loss: 6.0068 - acc: 0.627 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 416/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 6.0614 - acc: 0.62 - ETA: 27s - loss: 6.1537 - acc: 0.61 - ETA: 20s - loss: 6.2097 - acc: 0.61 - ETA: 14s - loss: 6.0299 - acc: 0.62 - ETA: 7s - loss: 6.0178 - acc: 0.6267 - ETA: 1s - loss: 6.0040 - acc: 0.627 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 417/500\n",
      "800/800 [==============================] - ETA: 31s - loss: 6.0026 - acc: 0.62 - ETA: 27s - loss: 5.8347 - acc: 0.63 - ETA: 20s - loss: 5.8459 - acc: 0.63 - ETA: 14s - loss: 6.0110 - acc: 0.62 - ETA: 8s - loss: 5.9590 - acc: 0.6303 - ETA: 1s - loss: 5.9663 - acc: 0.629 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 418/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.0614 - acc: 0.62 - ETA: 26s - loss: 6.0530 - acc: 0.62 - ETA: 20s - loss: 5.9075 - acc: 0.63 - ETA: 14s - loss: 5.9565 - acc: 0.63 - ETA: 7s - loss: 6.0144 - acc: 0.6269 - ETA: 1s - loss: 6.0222 - acc: 0.626 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 419/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 6.1286 - acc: 0.61 - ETA: 26s - loss: 5.8851 - acc: 0.63 - ETA: 20s - loss: 6.0614 - acc: 0.62 - ETA: 14s - loss: 6.0824 - acc: 0.62 - ETA: 7s - loss: 6.1084 - acc: 0.6210 - ETA: 1s - loss: 5.9928 - acc: 0.628 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 420/500\n",
      "800/800 [==============================] - ETA: 36s - loss: 6.0446 - acc: 0.62 - ETA: 28s - loss: 6.1076 - acc: 0.62 - ETA: 21s - loss: 6.1034 - acc: 0.62 - ETA: 14s - loss: 6.0110 - acc: 0.62 - ETA: 8s - loss: 5.9959 - acc: 0.6280 - ETA: 1s - loss: 5.9859 - acc: 0.628 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 421/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.5147 - acc: 0.59 - ETA: 26s - loss: 6.0362 - acc: 0.62 - ETA: 20s - loss: 6.0726 - acc: 0.62 - ETA: 14s - loss: 6.0383 - acc: 0.62 - ETA: 8s - loss: 6.0463 - acc: 0.6249 - ETA: 1s - loss: 6.0502 - acc: 0.624 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 422/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 6.2629 - acc: 0.61 - ETA: 27s - loss: 6.1747 - acc: 0.61 - ETA: 21s - loss: 6.0390 - acc: 0.62 - ETA: 14s - loss: 6.0992 - acc: 0.62 - ETA: 7s - loss: 5.9842 - acc: 0.6287 - ETA: 1s - loss: 6.0222 - acc: 0.626 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 423/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 6.1286 - acc: 0.61 - ETA: 27s - loss: 6.1202 - acc: 0.62 - ETA: 20s - loss: 5.9775 - acc: 0.62 - ETA: 14s - loss: 6.0068 - acc: 0.62 - ETA: 7s - loss: 6.0043 - acc: 0.6275 - ETA: 1s - loss: 5.9621 - acc: 0.630 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 424/500\n",
      "800/800 [==============================] - ETA: 31s - loss: 6.2545 - acc: 0.61 - ETA: 26s - loss: 5.8222 - acc: 0.63 - ETA: 20s - loss: 5.7788 - acc: 0.64 - ETA: 14s - loss: 5.8977 - acc: 0.63 - ETA: 7s - loss: 5.9674 - acc: 0.6298 - ETA: 1s - loss: 6.0054 - acc: 0.627 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 425/500\n",
      "800/800 [==============================] - ETA: 31s - loss: 5.5997 - acc: 0.65 - ETA: 26s - loss: 5.7550 - acc: 0.64 - ETA: 20s - loss: 5.7900 - acc: 0.64 - ETA: 14s - loss: 5.8851 - acc: 0.63 - ETA: 7s - loss: 5.8263 - acc: 0.6385 - ETA: 1s - loss: 5.9495 - acc: 0.630 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 426/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 5.7256 - acc: 0.64 - ETA: 26s - loss: 5.9019 - acc: 0.63 - ETA: 20s - loss: 5.8347 - acc: 0.63 - ETA: 14s - loss: 5.9124 - acc: 0.63 - ETA: 7s - loss: 5.9674 - acc: 0.6298 - ETA: 1s - loss: 5.9649 - acc: 0.629 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 427/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 5.8347 - acc: 0.63 - ETA: 27s - loss: 5.9271 - acc: 0.63 - ETA: 20s - loss: 5.9271 - acc: 0.63 - ETA: 14s - loss: 6.0026 - acc: 0.62 - ETA: 7s - loss: 6.0144 - acc: 0.6269 - ETA: 1s - loss: 5.9900 - acc: 0.628 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 428/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 5.8935 - acc: 0.63 - ETA: 26s - loss: 5.9817 - acc: 0.62 - ETA: 20s - loss: 5.9747 - acc: 0.62 - ETA: 14s - loss: 5.8851 - acc: 0.63 - ETA: 8s - loss: 5.8583 - acc: 0.6366 - ETA: 1s - loss: 5.9789 - acc: 0.629 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 429/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 5.7004 - acc: 0.64 - ETA: 26s - loss: 5.8809 - acc: 0.63 - ETA: 20s - loss: 6.0026 - acc: 0.62 - ETA: 14s - loss: 5.9942 - acc: 0.62 - ETA: 7s - loss: 6.0043 - acc: 0.6275 - ETA: 1s - loss: 5.9900 - acc: 0.628 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 430/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 5.9775 - acc: 0.62 - ETA: 27s - loss: 5.9859 - acc: 0.62 - ETA: 20s - loss: 6.0026 - acc: 0.62 - ETA: 14s - loss: 5.9313 - acc: 0.63 - ETA: 8s - loss: 5.8734 - acc: 0.6356 - ETA: 1s - loss: 6.0054 - acc: 0.627 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 431/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.1034 - acc: 0.62 - ETA: 27s - loss: 6.1537 - acc: 0.61 - ETA: 20s - loss: 6.2097 - acc: 0.61 - ETA: 14s - loss: 5.9984 - acc: 0.62 - ETA: 7s - loss: 6.0245 - acc: 0.6263 - ETA: 1s - loss: 5.9859 - acc: 0.628 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 432/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.2209 - acc: 0.61 - ETA: 27s - loss: 6.1747 - acc: 0.61 - ETA: 20s - loss: 6.1761 - acc: 0.61 - ETA: 14s - loss: 6.2503 - acc: 0.61 - ETA: 7s - loss: 6.1252 - acc: 0.6200 - ETA: 1s - loss: 6.0194 - acc: 0.626 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 433/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 6.0866 - acc: 0.62 - ETA: 27s - loss: 6.1034 - acc: 0.62 - ETA: 20s - loss: 6.1202 - acc: 0.62 - ETA: 14s - loss: 6.0299 - acc: 0.62 - ETA: 7s - loss: 5.9523 - acc: 0.6307 - ETA: 1s - loss: 5.9747 - acc: 0.629 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 434/500\n",
      "800/800 [==============================] - ETA: 31s - loss: 5.7172 - acc: 0.64 - ETA: 26s - loss: 5.7424 - acc: 0.64 - ETA: 20s - loss: 5.8571 - acc: 0.63 - ETA: 14s - loss: 5.9187 - acc: 0.63 - ETA: 7s - loss: 5.9808 - acc: 0.6290 - ETA: 1s - loss: 6.0208 - acc: 0.626 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 435/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 6.0278 - acc: 0.62 - ETA: 27s - loss: 6.4392 - acc: 0.60 - ETA: 20s - loss: 6.0530 - acc: 0.62 - ETA: 14s - loss: 6.1034 - acc: 0.62 - ETA: 8s - loss: 5.9707 - acc: 0.6296 - ETA: 1s - loss: 5.9887 - acc: 0.628 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 436/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - ETA: 35s - loss: 5.9103 - acc: 0.63 - ETA: 27s - loss: 6.0320 - acc: 0.62 - ETA: 21s - loss: 5.9691 - acc: 0.62 - ETA: 14s - loss: 5.9775 - acc: 0.62 - ETA: 8s - loss: 5.9456 - acc: 0.6311 - ETA: 1s - loss: 5.9803 - acc: 0.629 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 437/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 5.9355 - acc: 0.63 - ETA: 26s - loss: 5.9775 - acc: 0.62 - ETA: 20s - loss: 5.9383 - acc: 0.63 - ETA: 14s - loss: 5.9796 - acc: 0.62 - ETA: 7s - loss: 5.8465 - acc: 0.6373 - ETA: 1s - loss: 5.9845 - acc: 0.628 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 438/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 5.8264 - acc: 0.63 - ETA: 26s - loss: 5.8683 - acc: 0.63 - ETA: 20s - loss: 5.9859 - acc: 0.62 - ETA: 14s - loss: 5.9775 - acc: 0.62 - ETA: 7s - loss: 6.0530 - acc: 0.6245 - ETA: 1s - loss: 5.9873 - acc: 0.628 - 43s 53ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 439/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 6.8421 - acc: 0.57 - ETA: 27s - loss: 6.3468 - acc: 0.60 - ETA: 21s - loss: 6.2937 - acc: 0.60 - ETA: 14s - loss: 6.2503 - acc: 0.61 - ETA: 8s - loss: 6.1000 - acc: 0.6216 - ETA: 1s - loss: 6.0376 - acc: 0.625 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 440/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 5.4570 - acc: 0.66 - ETA: 26s - loss: 5.8012 - acc: 0.64 - ETA: 20s - loss: 5.9075 - acc: 0.63 - ETA: 14s - loss: 6.0929 - acc: 0.62 - ETA: 7s - loss: 6.0261 - acc: 0.6261 - ETA: 1s - loss: 6.0572 - acc: 0.624 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 441/500\n",
      "800/800 [==============================] - ETA: 31s - loss: 6.2377 - acc: 0.61 - ETA: 26s - loss: 5.8515 - acc: 0.63 - ETA: 20s - loss: 6.0670 - acc: 0.62 - ETA: 13s - loss: 5.9984 - acc: 0.62 - ETA: 7s - loss: 5.9976 - acc: 0.6279 - ETA: 1s - loss: 5.9914 - acc: 0.628 - 42s 53ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 442/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 6.2377 - acc: 0.61 - ETA: 26s - loss: 6.1999 - acc: 0.61 - ETA: 21s - loss: 6.0054 - acc: 0.62 - ETA: 14s - loss: 5.9376 - acc: 0.63 - ETA: 8s - loss: 5.9321 - acc: 0.6320 - ETA: 1s - loss: 6.0222 - acc: 0.626 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 443/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 5.9942 - acc: 0.62 - ETA: 26s - loss: 5.8473 - acc: 0.63 - ETA: 20s - loss: 6.0558 - acc: 0.62 - ETA: 14s - loss: 6.0152 - acc: 0.62 - ETA: 7s - loss: 5.9254 - acc: 0.6324 - ETA: 1s - loss: 6.0320 - acc: 0.625 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 444/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.8505 - acc: 0.57 - ETA: 27s - loss: 6.6155 - acc: 0.58 - ETA: 20s - loss: 6.1482 - acc: 0.61 - ETA: 14s - loss: 6.0782 - acc: 0.62 - ETA: 7s - loss: 6.0664 - acc: 0.6236 - ETA: 1s - loss: 6.0124 - acc: 0.627 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 445/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 6.2965 - acc: 0.60 - ETA: 27s - loss: 6.0740 - acc: 0.62 - ETA: 21s - loss: 6.0866 - acc: 0.62 - ETA: 14s - loss: 6.1223 - acc: 0.62 - ETA: 7s - loss: 6.1202 - acc: 0.6203 - ETA: 1s - loss: 5.9942 - acc: 0.628 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 446/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 6.1118 - acc: 0.62 - ETA: 26s - loss: 5.8473 - acc: 0.63 - ETA: 20s - loss: 5.9411 - acc: 0.63 - ETA: 14s - loss: 6.0278 - acc: 0.62 - ETA: 7s - loss: 6.0782 - acc: 0.6229 - ETA: 1s - loss: 6.0334 - acc: 0.625 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 447/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.3049 - acc: 0.60 - ETA: 27s - loss: 5.8054 - acc: 0.63 - ETA: 20s - loss: 5.9187 - acc: 0.63 - ETA: 14s - loss: 6.0425 - acc: 0.62 - ETA: 7s - loss: 6.0950 - acc: 0.6219 - ETA: 1s - loss: 6.0334 - acc: 0.625 - 44s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 448/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.2545 - acc: 0.61 - ETA: 27s - loss: 6.0740 - acc: 0.62 - ETA: 20s - loss: 5.9439 - acc: 0.63 - ETA: 14s - loss: 5.9355 - acc: 0.63 - ETA: 7s - loss: 5.9422 - acc: 0.6314 - ETA: 1s - loss: 6.0054 - acc: 0.627 - 43s 53ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 449/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 6.0446 - acc: 0.62 - ETA: 27s - loss: 5.8893 - acc: 0.63 - ETA: 20s - loss: 5.9859 - acc: 0.62 - ETA: 14s - loss: 5.8473 - acc: 0.63 - ETA: 8s - loss: 5.9489 - acc: 0.6309 - ETA: 1s - loss: 5.9663 - acc: 0.629 - 44s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 450/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 6.0278 - acc: 0.62 - ETA: 27s - loss: 6.0782 - acc: 0.62 - ETA: 20s - loss: 6.0362 - acc: 0.62 - ETA: 14s - loss: 5.9586 - acc: 0.63 - ETA: 8s - loss: 6.0026 - acc: 0.6276 - ETA: 1s - loss: 6.0152 - acc: 0.626 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 451/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.7424 - acc: 0.64 - ETA: 27s - loss: 5.9439 - acc: 0.63 - ETA: 20s - loss: 5.9607 - acc: 0.63 - ETA: 14s - loss: 5.8096 - acc: 0.63 - ETA: 7s - loss: 5.8868 - acc: 0.6348 - ETA: 1s - loss: 5.9845 - acc: 0.628 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 452/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 5.7508 - acc: 0.64 - ETA: 26s - loss: 6.0530 - acc: 0.62 - ETA: 20s - loss: 5.9411 - acc: 0.63 - ETA: 14s - loss: 5.9984 - acc: 0.62 - ETA: 7s - loss: 5.9842 - acc: 0.6288 - ETA: 1s - loss: 6.0068 - acc: 0.627 - 43s 53ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 453/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 5.9523 - acc: 0.63 - ETA: 26s - loss: 5.9523 - acc: 0.63 - ETA: 20s - loss: 5.8431 - acc: 0.63 - ETA: 14s - loss: 5.8851 - acc: 0.63 - ETA: 7s - loss: 5.9808 - acc: 0.6290 - ETA: 1s - loss: 5.9998 - acc: 0.627 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 454/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.1370 - acc: 0.61 - ETA: 26s - loss: 5.6878 - acc: 0.64 - ETA: 20s - loss: 5.6976 - acc: 0.64 - ETA: 14s - loss: 5.6249 - acc: 0.65 - ETA: 7s - loss: 5.8180 - acc: 0.6391 - ETA: 1s - loss: 5.9565 - acc: 0.630 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 455/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 5.9607 - acc: 0.63 - ETA: 26s - loss: 5.9313 - acc: 0.63 - ETA: 20s - loss: 6.0166 - acc: 0.62 - ETA: 14s - loss: 5.9775 - acc: 0.62 - ETA: 8s - loss: 5.9221 - acc: 0.6326 - ETA: 1s - loss: 5.9355 - acc: 0.631 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 456/500\n",
      "800/800 [==============================] - ETA: 35s - loss: 5.7508 - acc: 0.64 - ETA: 28s - loss: 5.8725 - acc: 0.63 - ETA: 21s - loss: 5.8319 - acc: 0.63 - ETA: 15s - loss: 5.9733 - acc: 0.62 - ETA: 8s - loss: 6.0211 - acc: 0.6265 - ETA: 1s - loss: 6.0124 - acc: 0.627 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 457/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 5.8431 - acc: 0.63 - ETA: 27s - loss: 5.6333 - acc: 0.65 - ETA: 20s - loss: 5.8851 - acc: 0.63 - ETA: 14s - loss: 5.9628 - acc: 0.63 - ETA: 7s - loss: 5.9506 - acc: 0.6308 - ETA: 1s - loss: 6.0474 - acc: 0.624 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 458/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - ETA: 32s - loss: 6.4308 - acc: 0.60 - ETA: 27s - loss: 5.8389 - acc: 0.63 - ETA: 20s - loss: 5.7816 - acc: 0.64 - ETA: 14s - loss: 5.7466 - acc: 0.64 - ETA: 8s - loss: 5.9556 - acc: 0.6305 - ETA: 1s - loss: 6.0334 - acc: 0.625 - 45s 56ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 459/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.5399 - acc: 0.59 - ETA: 26s - loss: 5.8054 - acc: 0.63 - ETA: 20s - loss: 5.7956 - acc: 0.64 - ETA: 14s - loss: 5.9586 - acc: 0.63 - ETA: 7s - loss: 5.9623 - acc: 0.6301 - ETA: 1s - loss: 5.9998 - acc: 0.627 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 460/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 5.6333 - acc: 0.65 - ETA: 26s - loss: 5.8305 - acc: 0.63 - ETA: 20s - loss: 5.8263 - acc: 0.63 - ETA: 14s - loss: 5.8788 - acc: 0.63 - ETA: 7s - loss: 6.0278 - acc: 0.6260 - ETA: 1s - loss: 6.0320 - acc: 0.625 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 461/500\n",
      "800/800 [==============================] - ETA: 31s - loss: 6.1118 - acc: 0.62 - ETA: 27s - loss: 5.8977 - acc: 0.63 - ETA: 20s - loss: 5.7900 - acc: 0.64 - ETA: 14s - loss: 5.9124 - acc: 0.63 - ETA: 7s - loss: 5.9942 - acc: 0.6281 - ETA: 1s - loss: 5.9789 - acc: 0.629 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 462/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 6.0782 - acc: 0.62 - ETA: 27s - loss: 5.9607 - acc: 0.63 - ETA: 20s - loss: 5.7704 - acc: 0.64 - ETA: 14s - loss: 5.8830 - acc: 0.63 - ETA: 7s - loss: 5.9338 - acc: 0.6319 - ETA: 1s - loss: 6.0250 - acc: 0.626 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 463/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 5.5493 - acc: 0.65 - ETA: 27s - loss: 5.6878 - acc: 0.64 - ETA: 20s - loss: 5.9942 - acc: 0.62 - ETA: 14s - loss: 6.0950 - acc: 0.62 - ETA: 7s - loss: 5.9808 - acc: 0.6290 - ETA: 1s - loss: 5.9621 - acc: 0.630 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 464/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 5.4486 - acc: 0.66 - ETA: 28s - loss: 5.6585 - acc: 0.64 - ETA: 21s - loss: 5.9747 - acc: 0.62 - ETA: 15s - loss: 5.9775 - acc: 0.62 - ETA: 8s - loss: 5.9338 - acc: 0.6319 - ETA: 1s - loss: 6.0110 - acc: 0.627 - 45s 57ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 465/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.2125 - acc: 0.61 - ETA: 27s - loss: 5.9649 - acc: 0.62 - ETA: 20s - loss: 5.9411 - acc: 0.63 - ETA: 14s - loss: 5.9984 - acc: 0.62 - ETA: 7s - loss: 6.0513 - acc: 0.6246 - ETA: 1s - loss: 6.0082 - acc: 0.627 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 466/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.2293 - acc: 0.61 - ETA: 26s - loss: 5.9607 - acc: 0.63 - ETA: 20s - loss: 5.9047 - acc: 0.63 - ETA: 14s - loss: 5.9544 - acc: 0.63 - ETA: 7s - loss: 5.8969 - acc: 0.6342 - ETA: 1s - loss: 6.0432 - acc: 0.625 - 43s 53ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 467/500\n",
      "800/800 [==============================] - ETA: 36s - loss: 6.7750 - acc: 0.57 - ETA: 28s - loss: 6.3594 - acc: 0.60 - ETA: 21s - loss: 6.0782 - acc: 0.62 - ETA: 14s - loss: 6.1370 - acc: 0.61 - ETA: 8s - loss: 5.9859 - acc: 0.6286 - ETA: 1s - loss: 5.9313 - acc: 0.632 - 44s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 468/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.1537 - acc: 0.61 - ETA: 27s - loss: 6.3258 - acc: 0.60 - ETA: 20s - loss: 5.9607 - acc: 0.63 - ETA: 14s - loss: 6.1328 - acc: 0.61 - ETA: 8s - loss: 6.1151 - acc: 0.6206 - ETA: 1s - loss: 6.0474 - acc: 0.624 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 469/500\n",
      "800/800 [==============================] - ETA: 31s - loss: 5.5409 - acc: 0.65 - ETA: 26s - loss: 5.8096 - acc: 0.63 - ETA: 20s - loss: 6.0642 - acc: 0.62 - ETA: 14s - loss: 6.1349 - acc: 0.61 - ETA: 7s - loss: 6.1336 - acc: 0.6195 - ETA: 1s - loss: 6.0418 - acc: 0.625 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 470/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 6.4308 - acc: 0.60 - ETA: 28s - loss: 6.4266 - acc: 0.60 - ETA: 22s - loss: 6.1146 - acc: 0.62 - ETA: 15s - loss: 6.1684 - acc: 0.61 - ETA: 8s - loss: 6.1067 - acc: 0.6211 - ETA: 1s - loss: 6.0614 - acc: 0.624 - 44s 56ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 471/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 5.2471 - acc: 0.67 - ETA: 27s - loss: 5.6585 - acc: 0.64 - ETA: 21s - loss: 5.8012 - acc: 0.64 - ETA: 14s - loss: 5.9900 - acc: 0.62 - ETA: 8s - loss: 6.0614 - acc: 0.6240 - ETA: 1s - loss: 5.9831 - acc: 0.628 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 472/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.2545 - acc: 0.61 - ETA: 27s - loss: 6.0572 - acc: 0.62 - ETA: 20s - loss: 6.0082 - acc: 0.62 - ETA: 14s - loss: 6.1013 - acc: 0.62 - ETA: 8s - loss: 6.0614 - acc: 0.6240 - ETA: 1s - loss: 6.0040 - acc: 0.627 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 473/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.4811 - acc: 0.59 - ETA: 26s - loss: 6.0698 - acc: 0.62 - ETA: 20s - loss: 6.0362 - acc: 0.62 - ETA: 14s - loss: 5.9040 - acc: 0.63 - ETA: 7s - loss: 6.0345 - acc: 0.6256 - ETA: 1s - loss: 6.0404 - acc: 0.625 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 474/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 6.2965 - acc: 0.60 - ETA: 27s - loss: 6.1244 - acc: 0.62 - ETA: 20s - loss: 5.9467 - acc: 0.63 - ETA: 14s - loss: 5.8809 - acc: 0.63 - ETA: 7s - loss: 5.8918 - acc: 0.6345 - ETA: 1s - loss: 5.9285 - acc: 0.632 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 475/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.1454 - acc: 0.61 - ETA: 26s - loss: 6.2671 - acc: 0.61 - ETA: 20s - loss: 6.1845 - acc: 0.61 - ETA: 14s - loss: 6.0824 - acc: 0.62 - ETA: 8s - loss: 6.0547 - acc: 0.6244 - ETA: 1s - loss: 6.0152 - acc: 0.626 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 476/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.9691 - acc: 0.62 - ETA: 26s - loss: 6.1831 - acc: 0.61 - ETA: 20s - loss: 6.1649 - acc: 0.61 - ETA: 14s - loss: 6.0530 - acc: 0.62 - ETA: 7s - loss: 6.0211 - acc: 0.6265 - ETA: 1s - loss: 6.0208 - acc: 0.626 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 477/500\n",
      "800/800 [==============================] - ETA: 35s - loss: 6.3804 - acc: 0.60 - ETA: 27s - loss: 6.4811 - acc: 0.59 - ETA: 20s - loss: 6.3468 - acc: 0.60 - ETA: 14s - loss: 6.2587 - acc: 0.61 - ETA: 7s - loss: 6.0463 - acc: 0.6249 - ETA: 1s - loss: 6.0068 - acc: 0.627 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 478/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.6406 - acc: 0.58 - ETA: 27s - loss: 6.1789 - acc: 0.61 - ETA: 20s - loss: 6.1230 - acc: 0.62 - ETA: 14s - loss: 6.0005 - acc: 0.62 - ETA: 7s - loss: 6.0480 - acc: 0.6248 - ETA: 1s - loss: 6.0180 - acc: 0.626 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 479/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.1286 - acc: 0.61 - ETA: 27s - loss: 6.0152 - acc: 0.62 - ETA: 20s - loss: 6.1565 - acc: 0.61 - ETA: 14s - loss: 6.0740 - acc: 0.62 - ETA: 7s - loss: 5.9674 - acc: 0.6298 - ETA: 1s - loss: 6.0250 - acc: 0.626 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 480/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - ETA: 31s - loss: 5.7340 - acc: 0.64 - ETA: 25s - loss: 5.5535 - acc: 0.65 - ETA: 20s - loss: 5.6696 - acc: 0.64 - ETA: 13s - loss: 5.9649 - acc: 0.62 - ETA: 7s - loss: 5.8902 - acc: 0.6346 - ETA: 1s - loss: 5.9117 - acc: 0.633 - 42s 53ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 481/500\n",
      "800/800 [==============================] - ETA: 36s - loss: 5.9355 - acc: 0.63 - ETA: 28s - loss: 5.7550 - acc: 0.64 - ETA: 21s - loss: 5.8375 - acc: 0.63 - ETA: 14s - loss: 5.8620 - acc: 0.63 - ETA: 8s - loss: 5.9758 - acc: 0.6293 - ETA: 1s - loss: 5.9998 - acc: 0.627 - 44s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 482/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 5.8012 - acc: 0.64 - ETA: 27s - loss: 5.9691 - acc: 0.62 - ETA: 20s - loss: 5.9579 - acc: 0.63 - ETA: 14s - loss: 6.0110 - acc: 0.62 - ETA: 7s - loss: 6.1000 - acc: 0.6216 - ETA: 1s - loss: 5.9887 - acc: 0.628 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 483/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.7340 - acc: 0.64 - ETA: 26s - loss: 5.8977 - acc: 0.63 - ETA: 20s - loss: 6.0390 - acc: 0.62 - ETA: 14s - loss: 6.1013 - acc: 0.62 - ETA: 7s - loss: 6.0950 - acc: 0.6219 - ETA: 1s - loss: 6.0040 - acc: 0.627 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 484/500\n",
      "800/800 [==============================] - ETA: 37s - loss: 5.7928 - acc: 0.64 - ETA: 30s - loss: 5.9649 - acc: 0.62 - ETA: 23s - loss: 6.0558 - acc: 0.62 - ETA: 16s - loss: 6.0215 - acc: 0.62 - ETA: 8s - loss: 6.0026 - acc: 0.6276 - ETA: 1s - loss: 5.9845 - acc: 0.628 - 47s 59ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 485/500\n",
      "800/800 [==============================] - ETA: 35s - loss: 6.4895 - acc: 0.59 - ETA: 27s - loss: 6.3384 - acc: 0.60 - ETA: 20s - loss: 6.2265 - acc: 0.61 - ETA: 14s - loss: 6.0425 - acc: 0.62 - ETA: 7s - loss: 6.0010 - acc: 0.6277 - ETA: 1s - loss: 5.9998 - acc: 0.627 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 486/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.2209 - acc: 0.61 - ETA: 27s - loss: 5.9229 - acc: 0.63 - ETA: 20s - loss: 5.9439 - acc: 0.63 - ETA: 14s - loss: 5.9880 - acc: 0.62 - ETA: 8s - loss: 6.0345 - acc: 0.6256 - ETA: 1s - loss: 5.9900 - acc: 0.628 - 44s 56ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 487/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.8096 - acc: 0.63 - ETA: 26s - loss: 5.7298 - acc: 0.64 - ETA: 20s - loss: 5.7592 - acc: 0.64 - ETA: 14s - loss: 5.8746 - acc: 0.63 - ETA: 7s - loss: 6.0211 - acc: 0.6265 - ETA: 1s - loss: 5.9845 - acc: 0.628 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 488/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 5.9523 - acc: 0.63 - ETA: 26s - loss: 6.1915 - acc: 0.61 - ETA: 20s - loss: 6.0026 - acc: 0.62 - ETA: 14s - loss: 5.9334 - acc: 0.63 - ETA: 7s - loss: 5.9187 - acc: 0.6328 - ETA: 1s - loss: 6.0180 - acc: 0.626 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 489/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 5.9691 - acc: 0.62 - ETA: 27s - loss: 5.9481 - acc: 0.63 - ETA: 20s - loss: 6.0026 - acc: 0.62 - ETA: 14s - loss: 5.9712 - acc: 0.62 - ETA: 7s - loss: 5.9657 - acc: 0.6299 - ETA: 1s - loss: 5.9649 - acc: 0.629 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 490/500\n",
      "800/800 [==============================] - ETA: 31s - loss: 5.9271 - acc: 0.63 - ETA: 25s - loss: 5.6920 - acc: 0.64 - ETA: 20s - loss: 5.7424 - acc: 0.64 - ETA: 14s - loss: 5.7844 - acc: 0.64 - ETA: 7s - loss: 5.9422 - acc: 0.6314 - ETA: 1s - loss: 6.0264 - acc: 0.626 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 491/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 6.1202 - acc: 0.62 - ETA: 26s - loss: 6.0698 - acc: 0.62 - ETA: 20s - loss: 6.2573 - acc: 0.61 - ETA: 14s - loss: 6.2209 - acc: 0.61 - ETA: 7s - loss: 6.1554 - acc: 0.6181 - ETA: 1s - loss: 6.0516 - acc: 0.624 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 492/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 5.8851 - acc: 0.63 - ETA: 27s - loss: 6.3468 - acc: 0.60 - ETA: 20s - loss: 6.0810 - acc: 0.62 - ETA: 14s - loss: 6.2062 - acc: 0.61 - ETA: 8s - loss: 6.1420 - acc: 0.6190 - ETA: 1s - loss: 6.0110 - acc: 0.627 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 493/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.1454 - acc: 0.61 - ETA: 27s - loss: 6.0278 - acc: 0.62 - ETA: 20s - loss: 6.0586 - acc: 0.62 - ETA: 14s - loss: 6.0068 - acc: 0.62 - ETA: 8s - loss: 5.9002 - acc: 0.6340 - ETA: 1s - loss: 5.9565 - acc: 0.630 - 44s 55ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 494/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 5.6501 - acc: 0.64 - ETA: 26s - loss: 5.5955 - acc: 0.65 - ETA: 20s - loss: 6.0866 - acc: 0.62 - ETA: 14s - loss: 6.0362 - acc: 0.62 - ETA: 7s - loss: 6.0245 - acc: 0.6263 - ETA: 1s - loss: 6.0278 - acc: 0.626 - 45s 56ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 495/500\n",
      "800/800 [==============================] - ETA: 35s - loss: 5.8767 - acc: 0.63 - ETA: 27s - loss: 6.0068 - acc: 0.62 - ETA: 20s - loss: 6.1426 - acc: 0.61 - ETA: 14s - loss: 6.0593 - acc: 0.62 - ETA: 7s - loss: 5.9825 - acc: 0.6289 - ETA: 1s - loss: 6.0040 - acc: 0.627 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 496/500\n",
      "800/800 [==============================] - ETA: 35s - loss: 5.7424 - acc: 0.64 - ETA: 28s - loss: 5.9900 - acc: 0.62 - ETA: 21s - loss: 5.8767 - acc: 0.63 - ETA: 15s - loss: 6.0299 - acc: 0.62 - ETA: 8s - loss: 6.0278 - acc: 0.6260 - ETA: 1s - loss: 6.0278 - acc: 0.626 - 45s 56ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 497/500\n",
      "800/800 [==============================] - ETA: 32s - loss: 6.0278 - acc: 0.62 - ETA: 26s - loss: 5.8683 - acc: 0.63 - ETA: 20s - loss: 5.7760 - acc: 0.64 - ETA: 14s - loss: 5.8830 - acc: 0.63 - ETA: 8s - loss: 5.9137 - acc: 0.6331 - ETA: 1s - loss: 5.9817 - acc: 0.628 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 498/500\n",
      "800/800 [==============================] - ETA: 33s - loss: 6.2713 - acc: 0.61 - ETA: 27s - loss: 6.3972 - acc: 0.60 - ETA: 20s - loss: 6.4476 - acc: 0.60 - ETA: 14s - loss: 6.3070 - acc: 0.60 - ETA: 7s - loss: 6.1034 - acc: 0.6214 - ETA: 1s - loss: 6.0124 - acc: 0.627 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 499/500\n",
      "800/800 [==============================] - ETA: 34s - loss: 5.6752 - acc: 0.64 - ETA: 27s - loss: 5.6081 - acc: 0.65 - ETA: 20s - loss: 5.9131 - acc: 0.63 - ETA: 14s - loss: 6.0236 - acc: 0.62 - ETA: 7s - loss: 5.9086 - acc: 0.6334 - ETA: 1s - loss: 5.9467 - acc: 0.631 - 43s 54ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n",
      "Epoch 500/500\n",
      "800/800 [==============================] - ETA: 35s - loss: 6.3552 - acc: 0.60 - ETA: 30s - loss: 6.1537 - acc: 0.61 - ETA: 23s - loss: 6.2965 - acc: 0.60 - ETA: 16s - loss: 6.0803 - acc: 0.62 - ETA: 8s - loss: 5.9993 - acc: 0.6278 - ETA: 1s - loss: 6.0474 - acc: 0.624 - 48s 60ms/step - loss: 6.0030 - acc: 0.6276 - val_loss: 5.9264 - val_acc: 0.6323\n"
     ]
    }
   ],
   "source": [
    "    #Variation - 3 Hidden LSTM Layer and 2 Dense ouput layer with higher Dropout Rate\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(english_vocab_size, embedding_vector_length,weights=[encoder_embedding_matrix], trainable= False,\n",
    "                    input_shape= input_shape[1:]))\n",
    "model.add(Bidirectional(LSTM(hidden_dim, return_sequences=True)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Bidirectional(LSTM(hidden_dim, return_sequences=True)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Bidirectional(LSTM(hidden_dim, return_sequences=True)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(french_vocab_size))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(french_vocab_size))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss=sparse_categorical_crossentropy,\n",
    "              optimizer=Adam(learning_rate),\n",
    "              metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "model.fit(tmp_x, preproc_french_sentences, batch_size=128, epochs=500, validation_split=0.2)\n",
    "\n",
    "model.save('./saved_models/final_rnn_model6_embed.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "In this section, you will experiment with various neural network architectures.\n",
    "You will begin by training four relatively simple architectures.\n",
    "- Model 1 is a simple RNN\n",
    "- Model 2 is a RNN with Embedding\n",
    "- Model 3 is a Bidirectional RNN\n",
    "- Model 4 is an optional Encoder-Decoder RNN\n",
    "\n",
    "After experimenting with the four simple architectures, you will construct a deeper architecture that is designed to outperform all four models.\n",
    "### Ids Back to Text\n",
    "The neural network will be translating the input to words ids, which isn't the final form we want.  We want the French translation.  The function `logits_to_text` will bridge the gab between the logits from the neural network to the French translation.  You'll be using this function to better understand the output of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`logits_to_text` function loaded.\n"
     ]
    }
   ],
   "source": [
    "def logits_to_text(logits, tokenizer):\n",
    "    \"\"\"\n",
    "    Turn logits from a neural network into text using the tokenizer\n",
    "    :param logits: Logits from a neural network\n",
    "    :param tokenizer: Keras Tokenizer fit on the labels\n",
    "    :return: String that represents the text of the logits\n",
    "    \"\"\"\n",
    "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
    "    index_to_words[0] = '<PAD>'\n",
    "\n",
    "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n",
    "\n",
    "print('`logits_to_text` function loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: RNN (IMPLEMENTATION)\n",
    "![RNN](images/rnn.png)\n",
    "A basic RNN model is a good baseline for sequence data.  In this model, you'll build a RNN that translates English to French."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_test(raw_word, name, english_tokenizer):\n",
    "    l1 = ['wont','won\\'t','wouldnt','wouldn\\'t','m', 're', 've', 'll', 's','d', 'nt', '\\'m', '\\'re', '\\'ve', '\\'ll', '\\'s', '\\'d', 'can\\'t', 'n\\'t', 'B: ', 'A: ', ',', ';', '.', '?', '!', ':', '. ?', ',   .', '. ,', 'EOS', 'BOS', 'eos', 'bos']\n",
    "    l2 = ['will not','will not','would not','would not',' am', ' are', ' have', ' will', ' is', ' had', ' not', ' am', ' are', ' have', ' will', ' is', ' had', 'can not', ' not', '', '', ' ,', ' ;', ' .', ' ?', ' !', ' :', '? ', '.', ',', '', '', '', '']\n",
    "    l3 = ['-', '_', ' *', ' /', '* ', '/ ', '\\\"', ' \\\\\"', '\\\\ ', '--', '...', '. . .']\n",
    "    l4 = ['jeffrey','fred','benjamin','paula','walter','rachel','andy','helen','harrington','kathy','ronnie','carl','annie','cole','ike','milo','cole','rick','johnny','loretta','cornelius','claire','romeo','casey','johnson','rudy','stanzi','cosgrove','wolfi','kevin','paulie','cindy','paulie','enzo','mikey','i\\97','davis','jeffrey','norman','johnson','dolores','tom','brian','bruce','john','laurie','stella','dignan','elaine','jack','christ','george','frank','mary','amon','david','tom','joe','paul','sam','charlie','bob','marry','walter','james','jimmy','michael','rose','jim','peter','nick','eddie','johnny','jake','ted','mike','billy','louis','ed','jerry','alex','charles','tommy','bobby','betty','sid','dave','jeffrey','jeff','marty','richard','otis','gale','fred','bill','jones','smith','mickey']    \n",
    "\n",
    "    raw_word = raw_word.lower()\n",
    "    raw_word = raw_word.replace(', ' + name_of_computer, '')\n",
    "    raw_word = raw_word.replace(name_of_computer + ' ,', '')\n",
    "\n",
    "    for j, term in enumerate(l1):\n",
    "        raw_word = raw_word.replace(term,l2[j])\n",
    "        \n",
    "    for term in l3:\n",
    "        raw_word = raw_word.replace(term,' ')\n",
    "    \n",
    "    for term in l4:\n",
    "        raw_word = raw_word.replace(', ' + term, ', ' + name)\n",
    "        raw_word = raw_word.replace(' ' + term + ' ,' ,' ' + name + ' ,')\n",
    "        raw_word = raw_word.replace('i am ' + term, 'i am ' + name_of_computer)\n",
    "        raw_word = raw_word.replace('my name is' + term, 'my name is ' + name_of_computer)\n",
    "    \n",
    "    for j in range(30):\n",
    "        raw_word = raw_word.replace('. .', '')\n",
    "        raw_word = raw_word.replace('.  .', '')\n",
    "        raw_word = raw_word.replace('..', '')\n",
    "       \n",
    "    for j in range(5):\n",
    "        raw_word = raw_word.replace('  ', ' ')\n",
    "        \n",
    "    if raw_word[-1] !=  '!' and raw_word[-1] != '?' and raw_word[-1] != '.' and raw_word[-2:] !=  '! ' and raw_word[-2:] != '? ' and raw_word[-2:] != '. ':\n",
    "        raw_word = raw_word + ' .'\n",
    "    \n",
    "    if raw_word == ' !' or raw_word == ' ?' or raw_word == ' .' or raw_word == ' ! ' or raw_word == ' ? ' or raw_word == ' . ':\n",
    "        raw_word = 'what ?'\n",
    "       \n",
    "    \n",
    "    if raw_word == '  .' or raw_word == ' .' or raw_word == '  . ':\n",
    "        raw_word = 'i do not want to talk about it .'\n",
    "    \n",
    "    \n",
    "    raw_new_string = []\n",
    "    \n",
    "    for word in nltk.tokenize.word_tokenize(raw_word):\n",
    "        if word not in english_tokenizer.word_index.keys():\n",
    "            word = name\n",
    "            raw_new_string.append(word)\n",
    "        else:\n",
    "            raw_new_string.append(word)\n",
    "            \n",
    "    \n",
    "    \n",
    "            #for dict_word in english_tokenizer.word_index.keys():\n",
    "            #    nlp_words = nlp(word)\n",
    "            #    dict_tokens = nlp(dict_word)\n",
    "            #    nlp_words.similarity(dict_tokens)\n",
    "    \n",
    "    \n",
    "    return ' '.join(raw_new_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computer: hi ! please type your name.\n",
      "\n",
      "user: shyam\n",
      "computer: hi , shyam ! My name is egg.\n",
      "\n",
      "how are you\n",
      "Sample 1:\n",
      "konnichihuahua! <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Did you hear about the Native American man that drank 200 cups of tea?\n",
      "Sample 1:\n",
      "He pulled impeached <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "What's the best anti diarrheal prescription?\n",
      "Sample 1:\n",
      "mitosis. <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "How do you call it when an egg is on point?\n",
      "Sample 1:\n",
      "konnichihuahua! <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "exit\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "que = ''\n",
    "last_query  = ' '\n",
    "last_last_query = ''\n",
    "text = ' '\n",
    "last_text = ''\n",
    "name_of_computer = 'egg'\n",
    "\n",
    "def final_predictions(model, x, y, x_tk, y_tk):\n",
    "    \"\"\"\n",
    "    Gets predictions using the final model\n",
    "    :param x: Preprocessed English data\n",
    "    :param y: Preprocessed French data\n",
    "    :param x_tk: English tokenizer\n",
    "    :param y_tk: French tokenizer\n",
    "    \"\"\"\n",
    "    \n",
    "    #model = load_model('saved_models/final_rnn_model6.hdf5')\n",
    "    #model.summary()\n",
    "    \n",
    "    ## DON'T EDIT ANYTHING BELOW THIS LINE\n",
    "    y_id_to_word = {value: key for key, value in y_tk.word_index.items()}\n",
    "    y_id_to_word[0] = '<PAD>'\n",
    "    \n",
    "\n",
    "    que = ''\n",
    "\n",
    "    print('computer: hi ! please type your name.\\n')\n",
    "    name = input('user: ')\n",
    "    print('computer: hi , ' + name +' ! My name is ' + name_of_computer + '.\\n') \n",
    "\n",
    "    \n",
    "    while(True):\n",
    "        \n",
    "        que = input()#'saw a old yellow'\n",
    "        if que =='exit':\n",
    "            break\n",
    "        else:\n",
    "            que = preprocess_test(que, name_of_computer, x_tk)\n",
    "        \n",
    "        sentence =que\n",
    "        \n",
    "        sentence = [x_tk.word_index[word] for word in sentence.split()]\n",
    "        \n",
    "\n",
    "        sentence = pad_sequences([sentence], maxlen=x.shape[-1], padding='post')\n",
    "        sentences = np.array([sentence[0], x[0]])\n",
    "\n",
    "        tmp_sentences = pad(sentences, y.shape[1])\n",
    "        tmp_sentences = tmp_sentences.reshape((-1, y.shape[-2]))\n",
    "\n",
    "        predictions = model.predict(tmp_sentences, len(tmp_sentences))\n",
    "\n",
    "        print('Sample 1:')\n",
    "        print(' '.join([y_id_to_word[np.argmax(x)] for x in predictions[0]]))\n",
    "        #print('Il a vu un vieux camion jaune')\n",
    "        #print('Sample 2:')\n",
    "        #print(' '.join([y_id_to_word[np.argmax(x)] for x in predictions[1]]))\n",
    "        #print(' '.join([y_id_to_word[np.argmax(x)] for x in y[0]]))\n",
    "\n",
    "final_predictions(loaded_model,preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.preprocessing.text.Tokenizer at 0x91129b0>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 15, 128)           346496    \n",
      "_________________________________________________________________\n",
      "bidirectional_7 (Bidirection (None, 15, 64)            41216     \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 15, 64)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_8 (Bidirection (None, 15, 64)            24832     \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 15, 64)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_9 (Bidirection (None, 15, 64)            24832     \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 15, 64)            0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 15, 2764)          179660    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 15, 2764)          0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 15, 2764)          0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 15, 2764)          7642460   \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 15, 2764)          0         \n",
      "=================================================================\n",
      "Total params: 8,259,496\n",
      "Trainable params: 8,259,496\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "loaded_model = load_model('saved_models/final_rnn_model6.hdf5')\n",
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15,)\n",
      "15\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You called `set_weights(weights)` on layer \"embedding_6\" with a  weight list of length 2707, but the layer was expecting 1 weights. Provided weights: [[ 0.          0.          0.         ...,  0.    ...",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-c0f6351a636e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[0mpreproc_french_sentences\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menglish_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     len(french_tokenizer.word_index)+1)\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[0mfinal_rnn_model6\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreproc_french_sentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-53-c0f6351a636e>\u001b[0m in \u001b[0;36mmodel_final6\u001b[1;34m(input_shape, output_sequence_length, english_vocab_size, french_vocab_size)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     model.add(Embedding(english_vocab_size, embedding_vector_length, weights=encoder_embedding_matrix,trainable= False,\n\u001b[1;32m---> 23\u001b[1;33m                         input_shape= input_shape[1:]))\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBidirectional\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36madd\u001b[1;34m(self, layer)\u001b[0m\n\u001b[0;32m    440\u001b[0m                 \u001b[1;31m# and create the node connecting the current layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m                 \u001b[1;31m# to the input layer we just created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 442\u001b[1;33m                 \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    443\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minbound_nodes\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\topology.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    581\u001b[0m                 \u001b[1;31m# Load weights that were specified at layer instantiation.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initial_weights\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initial_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m             \u001b[1;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\topology.py\u001b[0m in \u001b[0;36mset_weights\u001b[1;34m(self, weights)\u001b[0m\n\u001b[0;32m   1189\u001b[0m                              \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m                              \u001b[1;34m' weights. Provided weights: '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1191\u001b[1;33m                              str(weights)[:50] + '...')\n\u001b[0m\u001b[0;32m   1192\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1193\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: You called `set_weights(weights)` on layer \"embedding_6\" with a  weight list of length 2707, but the layer was expecting 1 weights. Provided weights: [[ 0.          0.          0.         ...,  0.    ..."
     ]
    }
   ],
   "source": [
    "\n",
    "def model_final6(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a model that incorporates embedding, encoder-decoder, and bidirectional RNN on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    print(input_shape[1:])\n",
    "    print(output_sequence_length)\n",
    "    \n",
    "    learning_rate = 0.01\n",
    "    hidden_dim = 256\n",
    "    embedding_vector_length = 100\n",
    "    \n",
    "    #Variation - 3 Hidden LSTM Layer and 2 Dense ouput layer with higher Dropout Rate\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(english_vocab_size, embedding_vector_length, weights=encoder_embedding_matrix,trainable= False,\n",
    "                        input_shape= input_shape[1:]))\n",
    "    model.add(Bidirectional(LSTM(hidden_dim, return_sequences=True)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Bidirectional(LSTM(hidden_dim, return_sequences=True)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Bidirectional(LSTM(hidden_dim, return_sequences=True)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(french_vocab_size))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(french_vocab_size))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "\n",
    "#tests.test_model_final(model_final)\n",
    "\n",
    "\n",
    "# Reshaping the input to work with a basic RNN\n",
    "tmp_x = pad(preproc_english_sentences, preproc_french_sentences.shape[1])\n",
    "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2]))\n",
    "\n",
    "# Train the neural network\n",
    "final_rnn_model6 = model_final6(\n",
    "    tmp_x.shape,\n",
    "    preproc_french_sentences.shape[1],\n",
    "    len(english_tokenizer.word_index)+1,\n",
    "    len(french_tokenizer.word_index)+1)\n",
    "final_rnn_model6.fit(tmp_x, preproc_french_sentences, batch_size=128, epochs=500, validation_split=0.2)\n",
    "\n",
    "final_rnn_model6.save('./saved_models/final_rnn_model6_embed.hdf5')\n",
    "\n",
    "# Print prediction(s)\n",
    "print(logits_to_text(final_rnn_model6.predict(tmp_x[:1])[0], french_tokenizer))\n",
    "\n",
    "print('Final Model Loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-9a7881f870d4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "print(logits_to_text(model.predict(tmp_x[25:30])[0], french_tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#preproc_french_sentences[6]\n",
    "\n",
    "len(french_tokenizer.word_index)+1\n",
    "#print(logits_to_text(final_rnn_model6.predict(tmp_x[20]), french_tokenizer))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction (IMPLEMENTATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "When you are ready to submit your project, do the following steps:\n",
    "1. Ensure you pass all points on the [rubric](https://review.udacity.com/#!/rubrics/1004/view).\n",
    "2. Submit the following in a zip file.\n",
    "  - `helper.py`\n",
    "  - `machine_translation.ipynb`\n",
    "  - `machine_translation.html`\n",
    "    - You can export the notebook by navigating to **File -> Download as -> HTML (.html)**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
