{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Resume Chatbot\n",
    "\n",
    "## Web Analytics Final project\n",
    "\n",
    "This is part of the final project on 620 Web Analytics course. Objective of this project is to create a resume chatbot for Shyam BV. This will provide most of the information about me.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we will build a deep neural network with that functions as part of an end-to-end machine translation pipeline which also uses text processing libraries. A question will be asked, an answer will be provided by bot. Also bot's ability to respond to new questions.\n",
    "\n",
    "- **Dataset Creation** - Here we will create a dataset with various text inputs\n",
    "- **Preprocess** - You'll convert text to sequence of integers with various preprocessing.\n",
    "- **Models** - A Deep neural network model will be created. By creating a word embedding, this model will be able to learn new words.\n",
    "- **Prediction** - Run the model and generate responses from bot.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "To perform this deep learning chatbot, we need to train the model with lots of data. Unfortunately my resume is not in format of QA(question answering type). So we need to generate lot of data with some existing data for this model.\n",
    " \n",
    "Here is a sample data which [I found](\n",
    "https://raw.githubusercontent.com/bvshyam/make_yourself_a_bot/master/workspace-watson.json). This will be me base to create a dataset.\n",
    "\n",
    "Over the period, due to many interactions, we will gather more data and our bot will get better over time.\n",
    "\n",
    "## Dataset creation\n",
    "\n",
    "As mentioned earlier, need create the initial dataset. That will be performed in  data_creation.ipynb\n",
    "\n",
    "## Neural Network\n",
    "\n",
    "As the model will be created using deep neural networks, we will be using Keras library with Tensorflow backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import GRU, Input, Dense, TimeDistributed, SimpleRNN, LSTM\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Dropout, Bidirectional, RepeatVector\n",
    "from keras.layers.embeddings import Embedding\n",
    "import collections\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer,text_to_word_sequence\n",
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "from autocorrect import spell\n",
    "from scipy.spatial import distance\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dataset has been created with dataset_creation.ipynb and some manual effort to correct it.Now we will load it and perform further pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tell me 5 positive things about you</td>\n",
       "      <td>Tricky question detecting. Waiting for next qu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tell me your strengths</td>\n",
       "      <td>Tricky question detecting. Waiting for next qu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tell us Unique Selling Points</td>\n",
       "      <td>Tricky question detecting. Waiting for next qu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are you good at ?</td>\n",
       "      <td>Tricky question detecting. Waiting for next qu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are your professional strengths ?</td>\n",
       "      <td>Tricky question detecting. Waiting for next qu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Question  \\\n",
       "0     Tell me 5 positive things about you   \n",
       "1                  Tell me your strengths   \n",
       "2           Tell us Unique Selling Points   \n",
       "3                  What are you good at ?   \n",
       "4  What are your professional strengths ?   \n",
       "\n",
       "                                              Answer  \n",
       "0  Tricky question detecting. Waiting for next qu...  \n",
       "1  Tricky question detecting. Waiting for next qu...  \n",
       "2  Tricky question detecting. Waiting for next qu...  \n",
       "3  Tricky question detecting. Waiting for next qu...  \n",
       "4  Tricky question detecting. Waiting for next qu...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a pandas dataframe for the input data\n",
    "\n",
    "df = pd.read_csv('./data/final_qa_data.csv',sep=',')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loaded\n"
     ]
    }
   ],
   "source": [
    "# Loading dataset to numpy array\n",
    "\n",
    "question_sentences = df.Question.values\n",
    "answer_sentences = df.Answer.values\n",
    "\n",
    "print('Dataset Loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files\n",
    "\n",
    "In each line, the first part is question and then the next part is answers. Lets see sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question Line 1:  Tell me 5 positive things about you\n",
      "Answer Line 1:  Tricky question detecting. Waiting for next question...\n",
      "Question Line 2:  Tell me your strengths\n",
      "Answer Line 2:  Tricky question detecting. Waiting for next question...\n"
     ]
    }
   ],
   "source": [
    "for sample_i in range(2):\n",
    "    print('Question Line {}:  {}'.format(sample_i + 1, question_sentences[sample_i]))\n",
    "    print('Answer Line {}:  {}'.format(sample_i + 1, answer_sentences[sample_i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From looking at the sentences, we need to perform some pre-processing. Below are some of them.\n",
    "\n",
    "- Complete string lower case. Parent vocab has only lower case characters.\n",
    "- Separate symbols and text. Else it will be considered as unique word.\n",
    "- Preserve the line ending.\n",
    "- Also replace some of the junk characters.\n",
    "- Replace won't, wouldn't etc into proper words.\n",
    "\n",
    "\n",
    "### Vocabulary\n",
    "\n",
    "Also need to build a word vocablary for the questions and answers. This will be our knowledge base. All the new questions will be stored and the vocabulary will be updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_cleaning(texts):\n",
    "    \n",
    "    text_cleaned = []\n",
    "    \n",
    "#    for text in texts:\n",
    "#        for raw_word in text_to_word_sequence(text,filters='', lower=True):\n",
    "            #\n",
    "#           l1 = ['won’t','won\\'t','wouldn’t','wouldn\\'t','’m', '’re', '’ve', '’ll', '’s','’d', 'n’t', '\\'m', '\\'re', '\\'ve', '\\'ll', '\\'s', '\\'d', 'can\\'t', 'n\\'t', 'B: ', 'A: ', ',', ';', '.', '?', '!', ':', '. ?', ',   .', '. ,', 'EOS', 'BOS', 'eos', 'bos']\n",
    "#            l2 = ['will not','will not','would not','would not',' am', ' are', ' have', ' will', ' is', ' had', ' not', ' am', ' are', ' have', ' will', ' is', ' had', 'can not', ' not', '', '', ' ,', ' ;', ' .', ' ?', ' !', ' :', '? ', '.', ',', '', '', '', '']\n",
    "\n",
    "#            raw_word = raw_word.lower()\n",
    "\n",
    "#            for j, term in enumerate(l1):\n",
    "#                raw_word = raw_word.replace(term,l2[j])\n",
    "\n",
    "#            for j in range(30):\n",
    "#                raw_word = raw_word.replace('. .', '')\n",
    "#                raw_word = raw_word.replace('.  .', '')\n",
    "#                raw_word = raw_word.replace('..', '')\n",
    "#                raw_word = raw_word.replace('...', '')\n",
    "\n",
    "#            for j in range(5):\n",
    "#                raw_word = raw_word.replace('  ', ' ')\n",
    "                \n",
    "#            text_cleaned.append(raw_word)\n",
    "            \n",
    "        #text_cleaned.append(raw_word)\n",
    "        #print(text_cleaned)\n",
    "        #Convert to lower and tokenize\n",
    "        \n",
    "        #text_cleaned.append(' '.join(text_to_word_sequence(raw_word,filters='', lower=True)))\n",
    "        \n",
    "    text_cleaned = [' '.join(nltk.tokenize.word_tokenize(word.lower(),preserve_line=True)) for word in texts]\n",
    "        \n",
    "        \n",
    "        \n",
    "    return text_cleaned\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Clean and store in the same variables.\n",
    "\n",
    "question_sentences = preprocess_cleaning(question_sentences)\n",
    "answer_sentences = preprocess_cleaning(answer_sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nex we will analyze the overall dataset and draw some inference out of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1067 words in questions.\n",
      "330 unique English words.\n",
      "10 Most common words in the questions dataset:\n",
      "\"you\" \"?\" \"are\" \"your\" \"do\" \"What\" \"what\" \"me\" \"about\" \"How\"\n",
      "\n",
      "2876 words in answers.\n",
      "319 unique English words.\n",
      "10 Most common words in the answers dataset:\n",
      "\"I\" \"to\" \"a\" \"you\" \"for\" \"my\" \"and\" \"I'm\" \"is\" \"me\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "question_words_counter = collections.Counter([word for sentence in question_sentences for word in sentence.split()])\n",
    "answer_words_counter = collections.Counter([word for sentence in answer_sentences for word in sentence.split()])\n",
    "\n",
    "print('{} words in questions.'.format(len([word for sentence in question_sentences for word in sentence.split()])))\n",
    "print('{} unique English words.'.format(len(question_words_counter)))\n",
    "print('10 Most common words in the questions dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*question_words_counter.most_common(10)))[0]) + '\"')\n",
    "print()\n",
    "print('{} words in answers.'.format(len([word for sentence in answer_sentences for word in sentence.split()])))\n",
    "print('{} unique English words.'.format(len(answer_words_counter)))\n",
    "print('10 Most common words in the answers dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*answer_words_counter.most_common(10)))[0]) + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('you', 101),\n",
       " ('?', 73),\n",
       " ('are', 34),\n",
       " ('your', 31),\n",
       " ('do', 31),\n",
       " ('What', 26),\n",
       " ('what', 26),\n",
       " ('me', 17),\n",
       " ('about', 17),\n",
       " ('How', 16),\n",
       " ('to', 16),\n",
       " ('is', 15),\n",
       " ('a', 14),\n",
       " ('I', 13),\n",
       " ('the', 12),\n",
       " ('how', 11),\n",
       " ('work', 11),\n",
       " ('can', 10),\n",
       " ('have', 10),\n",
       " ('tell', 9)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Most common top 20 word.\n",
    "\n",
    "question_words_counter.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess\n",
    "\n",
    "As a next step we will perform below ones.\n",
    "1. Tokenize the words into ids\n",
    "2. Add padding to make all the sequences the same length.\n",
    "\n",
    "\n",
    "### Tokenize (IMPLEMENTATION)\n",
    "For a neural network to predict on text data, it first has to be turned into data it can understand. \n",
    "\n",
    "A word level model uses word ids that generate text predictions for each word. Also we will use the pre-trained word embedding called as [Glove](https://nlp.stanford.edu/projects/glove/)\n",
    "\n",
    "Turn each sentence into a sequence of words ids using Keras's [`Tokenizer`](https://keras.io/preprocessing/text/#tokenizer) function. Using this function we will tokenize `question_sentences` and `answer_sentences` in the cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tell me 5 positive things about you' 'Tell me your strengths'\n",
      " 'Tell us Unique Selling Points'] \n",
      "\n",
      "{'tell': 1, 'me': 2, '5': 3, 'positive': 4, 'things': 5, 'about': 6, 'you': 7, 'your': 8, 'strengths': 9, 'us': 10, 'unique': 11, 'selling': 12, 'points': 13}\n",
      "\n",
      "Sequence 1 in x\n",
      "  Input:  Tell me 5 positive things about you\n",
      "  Output: [1, 2, 3, 4, 5, 6, 7]\n",
      "Sequence 2 in x\n",
      "  Input:  Tell me your strengths\n",
      "  Output: [1, 2, 8, 9]\n",
      "Sequence 3 in x\n",
      "  Input:  Tell us Unique Selling Points\n",
      "  Output: [1, 10, 11, 12, 13]\n"
     ]
    }
   ],
   "source": [
    "def tokenize(x):\n",
    "    \"\"\"\n",
    "    Tokenize x\n",
    "    param x: List of sentences/strings to be tokenized\n",
    "    return: Tuple of (tokenized x data, tokenizer used to tokenize x)\n",
    "    \"\"\"\n",
    "    # convert to nltk tokenizer to preserve the symbols and tokenize\n",
    "    x = [' '.join(nltk.tokenize.word_tokenize(word.lower(),preserve_line=True)) for word in x]\n",
    "\n",
    "    #Use tokenizer from Keras\n",
    "    tokenizer = Tokenizer(num_words=None, filters=\"\", lower=True, split=\" \")\n",
    "    tokenizer.fit_on_texts(x)\n",
    "    return tokenizer.texts_to_sequences(x), tokenizer\n",
    "\n",
    "\n",
    "# Tokenize sample output\n",
    "\n",
    "text_sentences = question_sentences[:3]\n",
    "text_tokenized, text_tokenizer = tokenize(text_sentences)\n",
    "print(text_sentences,\"\\n\")\n",
    "print(text_tokenizer.word_index)\n",
    "print()\n",
    "\n",
    "for sample_i, (sent, token_sent) in enumerate(zip(text_sentences, text_tokenized)):\n",
    "    print('Sequence {} in x'.format(sample_i + 1))\n",
    "    print('  Input:  {}'.format(sent))\n",
    "    print('  Output: {}'.format(token_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding\n",
    "\n",
    "When batching the sequence of word ids together, each sequence needs to be the same length.  Since sentences are dynamic in length, we can add padding to the end of the sequences to make them the same length.\n",
    "\n",
    "Make sure all the English sequences have the same length and all the French sequences have the same length by adding padding to the **end** of each sequence using Keras's [`pad_sequences`](https://keras.io/preprocessing/sequence/#pad_sequences) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 1 in x\n",
      "  Input:  [1 2 3 4 5 6 7]\n",
      "  Output: [1 2 3 4 5 6 7]\n",
      "Sequence 2 in x\n",
      "  Input:  [1 2 8 9]\n",
      "  Output: [1 2 8 9 0 0 0]\n",
      "Sequence 3 in x\n",
      "  Input:  [ 1 10 11 12 13]\n",
      "  Output: [ 1 10 11 12 13  0  0]\n"
     ]
    }
   ],
   "source": [
    "def pad(x, length=None):\n",
    "    \"\"\"\n",
    "    Pad x\n",
    "    param x: List of sequences.\n",
    "    param length: Length to pad the sequence to.  If None, use length of longest sequence in x.\n",
    "    return: Padded numpy array of sequences\n",
    "    \"\"\"\n",
    "    if length==None:\n",
    "        maxLenX = 0\n",
    "        for sequence in x:\n",
    "            if len(sequence) > maxLenX:\n",
    "                maxLenX = len(sequence)\n",
    "        \n",
    "        padded = pad_sequences(sequences=x,maxlen=maxLenX, dtype='int32', padding='post', truncating='post', value=0)\n",
    "\n",
    "    else:\n",
    "        padded = pad_sequences(sequences=x,maxlen=length, dtype='int32', padding='post', truncating='post', value=0)\n",
    "    return padded\n",
    "\n",
    "\n",
    "# Pad Tokenized output\n",
    "test_pad = pad(text_tokenized)\n",
    "for sample_i, (token_sent, pad_sent) in enumerate(zip(text_tokenized, test_pad)):\n",
    "    print('Sequence {} in x'.format(sample_i + 1))\n",
    "    print('  Input:  {}'.format(np.array(token_sent)))\n",
    "    print('  Output: {}'.format(pad_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Pipeline\n",
    "\n",
    "As a next step we will call the above functions for the questions and answers and store those tokenizer and outputs for further usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(261, 39)\n",
      "Data Preprocessed\n"
     ]
    }
   ],
   "source": [
    "def preprocess(x, y):\n",
    "    \"\"\"\n",
    "    Preprocess x and y\n",
    "    :param x: Feature List of sentences\n",
    "    :param y: Feature output List of sentences\n",
    "    :return: Tuple of (Preprocessed x, Preprocessed y, x tokenizer, y tokenizer)\n",
    "    \"\"\"\n",
    "    preprocess_x, x_tk = tokenize(x)\n",
    "    preprocess_y, y_tk = tokenize(y)\n",
    "\n",
    "    preprocess_x = pad(preprocess_x)\n",
    "    preprocess_y = pad(preprocess_y)\n",
    "    \n",
    "    print(preprocess_y.shape)\n",
    "    \n",
    "    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n",
    "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
    "\n",
    "    return preprocess_x, preprocess_y, x_tk, y_tk\n",
    "\n",
    "preproc_question_sentences, preproc_answer_sentences, question_tokenizer, answer_tokenizer =\\\n",
    "    preprocess(question_sentences, answer_sentences)\n",
    "\n",
    "print('Data Preprocessed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 10,  11,  74, ...,   0,   0,   0],\n",
       "       [ 10,  11,   6, ...,   0,   0,   0],\n",
       "       [ 10,  38, 134, ...,   0,   0,   0],\n",
       "       ..., \n",
       "       [  3,   9,  33, ...,   0,   0,   0],\n",
       "       [  3,   9,  33, ...,   0,   0,   0],\n",
       "       [  3,   9,  33, ...,   0,   0,   0]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preproc_question_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove Embedding \n",
    "\n",
    "As a next step we will try Glove word embedding. Here we want to convert the preprocessed padded word sentences to a word vector with dimentions. By performing this, we know the relationship between the words. \n",
    "\n",
    "For this project, we have used a simple word embedding which is of 100 dimensions. Means each word is converted to a 100 dimension vector. That will be one of the input to Embedding layer.\n",
    "\n",
    "We will assign the word embedding vector to each vocablary in the questions layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 400000 word vectors\n"
     ]
    }
   ],
   "source": [
    "#word embeddings from glove\n",
    "\n",
    "embeddings_index = dict()\n",
    "\n",
    "with open('./data/glove.6B.100d.txt',encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word]= coefs\n",
    "\n",
    "print('loaded %s word vectors' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create an encoder matrix with all the vocablary in questions.\n",
    "\n",
    "num_question_tokens = len(question_tokenizer.word_index)+1\n",
    "\n",
    "#Create a dummy matrix to store all the word embeddings\n",
    "encoder_embedding_matrix = np.zeros((num_question_tokens, 100))\n",
    "\n",
    "# List to find nearest words for unknown words in current vocab list\n",
    "unknown_helper_list = []\n",
    "\n",
    "for word,i in question_tokenizer.word_index.items():\n",
    "    \n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    unknown_helper_list.append(word)\n",
    "\n",
    "    if embedding_vector is not None:\n",
    "        encoder_embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "#Final matrix is the collection of vectors for the complete questions vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Implementation\n",
    "\n",
    "As a next step, we will build a deep neural network model. This will be a series of different layers. We will specify some hyper parameters and then use it to train our model.\n",
    "\n",
    "Below are the different layers of deep neural network\n",
    "\n",
    "1. Embedding layer which acts as input layer and gets input from embeddings vector.\n",
    "2. Create a Bidirectional LSTM layer.\n",
    "3. Add some random dropout neurons. Repeat Step 2 and 3.\n",
    "4. Create an output layer with sigmoid activation function.\n",
    "5. Finally compile the model with optimizer and accuracy metrics.\n",
    "6. Fit the model with training questions and answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Learning rate for optimizer\n",
    "learning_rate = 0.01\n",
    "# Hidden layers for LSTM layers\n",
    "hidden_dim = 256\n",
    "# Word embedding vector length\n",
    "embedding_vector_length = 100\n",
    "# Number of epochs for complete training\n",
    "epochs =1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Reshaping the questions input to work with a basic RNN with output shape\n",
    "tmp_x = pad(preproc_question_sentences, preproc_answer_sentences.shape[1])\n",
    "tmp_x = tmp_x.reshape((-1, preproc_answer_sentences.shape[-2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Assign to the variables\n",
    "input_shape = tmp_x.shape\n",
    "output_sequence_length = preproc_answer_sentences.shape[1]\n",
    "question_vocab_size = len(question_tokenizer.word_index)+1\n",
    "answer_vocab_size= len(answer_tokenizer.word_index)+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 39, 100)           28900     \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 39, 512)           731136    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 39, 512)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 39, 512)           1574912   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 39, 512)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 39, 512)           1574912   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 39, 512)           0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 39, 293)           150309    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 39, 293)           0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 39, 293)           0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 39, 293)           86142     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 39, 293)           0         \n",
      "=================================================================\n",
      "Total params: 4,146,311\n",
      "Trainable params: 4,117,411\n",
      "Non-trainable params: 28,900\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 208 samples, validate on 53 samples\n",
      "Epoch 1/2\n",
      "208/208 [==============================] - 22s 108ms/step - loss: 5.3775 - acc: 0.0719 - val_loss: 1.3869 - val_acc: 0.8708\n",
      "Epoch 2/2\n",
      "208/208 [==============================] - 12s 56ms/step - loss: 3.8623 - acc: 0.6161 - val_loss: 1.1327 - val_acc: 0.8708\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x3cb1c4a8>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Variation - 3 Hidden LSTM Layer and 2 Dense ouput layer with higher Dropout Rate\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(question_vocab_size, embedding_vector_length,weights=[encoder_embedding_matrix], trainable= False,\n",
    "                    input_shape= input_shape[1:]))\n",
    "model.add(Bidirectional(LSTM(hidden_dim, return_sequences=True)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Bidirectional(LSTM(hidden_dim, return_sequences=True)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Bidirectional(LSTM(hidden_dim, return_sequences=True)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(answer_vocab_size))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(answer_vocab_size))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss=sparse_categorical_crossentropy,\n",
    "              optimizer=Adam(learning_rate),\n",
    "              metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "model.fit(tmp_x, preproc_answer_sentences, batch_size=128, epochs=epochs, validation_split=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Save the model for future purposes\n",
    "model.save('./saved_models/resume_chatbot_save_'+str(epochs)+'.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ids Back to Text\n",
    "\n",
    "The neural network will be translating the input to words ids, which isn't the final form we want.  We want the answers for that question.  Below functions `logits_to_text` will bridge the gab between the logits from the neural network to the answers. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`logits_to_text` function loaded.\n"
     ]
    }
   ],
   "source": [
    "def logits_to_text(logits, tokenizer):\n",
    "    \"\"\"\n",
    "    Turn logits from a neural network into text using the tokenizer\n",
    "    :param logits: Logits from a neural network\n",
    "    :param tokenizer: Keras Tokenizer fit on the labels\n",
    "    :return: String that represents the text of the logits\n",
    "    \"\"\"\n",
    "    #index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
    "    #index_to_words[0] = '<PAD>'\n",
    "\n",
    "    #return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n",
    "    \n",
    "    return False\n",
    "print('`logits_to_text` function loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def similar_word(unknown_word_dim):\n",
    "    \"\"\"\n",
    "    To find the similar if a typed word is not available in questions vocab. \n",
    "    Here we are finding the nearest word using euclidean distance \n",
    "    and perform the approximate word  which is similar to it.\n",
    "    :param unknown_word_dim: unknown word entered in the text\n",
    "    \"\"\"\n",
    "    all_distance = []\n",
    "\n",
    "    for known_word in encoder_embedding_matrix:\n",
    "        all_distance.append(distance.euclidean(unknown_word_dim,known_word))\n",
    "    \n",
    "    #Get the minimum distance using argsort\n",
    "    return(unknown_helper_list[np.array(all_distance).argsort()[:1][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_test(raw_word, question_tokenizer):\n",
    "    \"\"\"\n",
    "    Preprocess the text which is entered by user. We need to remove and clean \n",
    "    the text before we predict the answer.\n",
    "    :param raw_word: Raw sentence entered by the user.\n",
    "    :param question_tokenizer: Question tokenizer for vocab\n",
    "    \"\"\"\n",
    "    \n",
    "    # Cleaning the text\n",
    "    l1 = ['won’t','won\\'t','wouldn’t','wouldn\\'t','’m', '’re', '’ve', '’ll', '’s','’d', 'n’t', '\\'m', '\\'re', '\\'ve', '\\'ll', '\\'s', '\\'d', 'can\\'t', 'n\\'t', 'B: ', 'A: ', ',', ';', '.', '?', '!', ':', '. ?', ',   .', '. ,', 'EOS', 'BOS', 'eos', 'bos']\n",
    "    l2 = ['will not','will not','would not','would not',' am', ' are', ' have', ' will', ' is', ' had', ' not', ' am', ' are', ' have', ' will', ' is', ' had', 'can not', ' not', '', '', ' ,', ' ;', ' .', ' ?', ' !', ' :', '? ', '.', ',', '', '', '', '']\n",
    "\n",
    "    raw_word = raw_word.lower()\n",
    "\n",
    "    for j, term in enumerate(l1):\n",
    "        raw_word = raw_word.replace(term,l2[j])\n",
    "       \n",
    "    for j in range(30):\n",
    "        raw_word = raw_word.replace('. .', '')\n",
    "        raw_word = raw_word.replace('.  .', '')\n",
    "        raw_word = raw_word.replace('..', '')\n",
    "        raw_word = raw_word.replace('...', '')\n",
    "        \n",
    "    for j in range(5):\n",
    "        raw_word = raw_word.replace('  ', ' ')\n",
    " \n",
    "    #Spell checker and call similar words function\n",
    "\n",
    "    final_corrected_words  = []\n",
    "    \n",
    "    for text in nltk.tokenize.word_tokenize(raw_word.lower(),preserve_line=True):\n",
    "        text = text.lower()\n",
    "        #Spell checker changes the symbols. So passing only strings.\n",
    "        if text not in '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n':\n",
    "            #Spell checker\n",
    "            text = spell(text).lower()\n",
    "            \n",
    "            # Finding unknown similar words from the vocab\n",
    "            if text not in question_tokenizer.word_index.keys():\n",
    "                \n",
    "                try:\n",
    "                    final_corrected_words.append(similar_word(embeddings_index[text]))\n",
    "                except:\n",
    "                    final_corrected_words.append('?')\n",
    "            else:\n",
    "                final_corrected_words.append(text)\n",
    "        else:\n",
    "           \n",
    "            if text not in question_tokenizer.word_index.keys():\n",
    "                final_corrected_words.append(similar_word(embeddings_index[text]))\n",
    "            else:\n",
    "                final_corrected_words.append(text)\n",
    "            \n",
    "    #print(' '.join(final_corrected_words))\n",
    "    return ' '.join(final_corrected_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Hi ! please type your name.\n",
      "\n",
      "user: SHyam\n",
      "Chatbot: hi , SHyam ! My name is Jammy AI.\n",
      "\n",
      "what can you do for me?\n",
      "think of me as shyam at an ask me `` interview '' related questions. what books i read , what have i doing lately or to tell you a joke .        \n",
      "\n",
      "tell me a joke\n",
      "ok.. here it is. roses are \\ # ff0000 / violets are \\ # 0000ff / all my base / belong to you .               \n",
      "\n",
      "exit\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "que = ''\n",
    "last_query  = ' '\n",
    "last_last_query = ''\n",
    "text = ' '\n",
    "last_text = ''\n",
    "name_of_computer = 'Jammy AI'\n",
    "\n",
    "# Open files to save the conversation for further training:\n",
    "\n",
    "name = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "qf = open('./session_data/'+name+'.txt', 'w')\n",
    "\n",
    "\n",
    "def final_predictions(model, x, y, x_tk, y_tk):\n",
    "    \"\"\"\n",
    "    Gets predictions using the final model\n",
    "    :param x: Preprocessed English data\n",
    "    :param y: Preprocessed French data\n",
    "    :param x_tk: Questions tokenizer\n",
    "    :param y_tk: Answers tokenizer\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Create a answer dictionary\n",
    "    y_id_to_word = {value: key for key, value in y_tk.word_index.items()}\n",
    "    y_id_to_word[0] = '<PAD>'\n",
    "    \n",
    "\n",
    "    print('Chatbot: Hi ! please type your name.\\n')\n",
    "    name = input('user: ')\n",
    "    print('Chatbot: hi , ' + name +' ! My name is ' + name_of_computer + '.\\n') \n",
    "\n",
    "    \n",
    "    while(True):\n",
    "        \n",
    "        que = input()\n",
    "        \n",
    "        qf.write(\"Question typed:\" + que + '\\n')\n",
    "        \n",
    "        if que =='exit':\n",
    "            break\n",
    "        else:\n",
    "            #Preprocess the text\n",
    "            que = preprocess_test(que,x_tk)\n",
    "            #print(que)\n",
    "        sentence =que\n",
    "        \n",
    "        qf.write(\"Question interpreted:\" + que + '\\n')\n",
    "        \n",
    "        #sentence = [x_tk.word_index[word] for word in text_to_word_sequence(sentence,filters='')]\n",
    "        \n",
    "        sentence = [x_tk.word_index[word] for word in nltk.tokenize.word_tokenize(sentence.lower())]\n",
    "        \n",
    "        #Convert to padded sequence\n",
    "        sentence = pad_sequences([sentence], maxlen=x.shape[-1], padding='post')\n",
    "        sentences = np.array([sentence[0], x[0]])\n",
    "        \n",
    "        #print(sentences.shape)\n",
    "        \n",
    "        tmp_sentences = pad(sentences, y.shape[1])\n",
    "        tmp_sentences = tmp_sentences.reshape((-1, y.shape[-2]))\n",
    "\n",
    "        #print(tmp_sentences)\n",
    "        predictions = model.predict(tmp_sentences, len(tmp_sentences))\n",
    "\n",
    "        #print('Sample 1:')\n",
    "        prediction_text = ' '.join([y_id_to_word[np.argmax(x)] for x in predictions[0]]).replace('<PAD>','')\n",
    "        \n",
    "        qf.write(\"Answer by bot:\" + prediction_text + '\\n')\n",
    "        print(prediction_text+'\\n')\n",
    "\n",
    "    qf.close()\n",
    "\n",
    "final_predictions(loaded_model,preproc_question_sentences, preproc_answer_sentences, question_tokenizer, answer_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 39, 100)           28900     \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 39, 512)           731136    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 39, 512)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 39, 512)           1574912   \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 39, 512)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 39, 512)           1574912   \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 39, 512)           0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 39, 293)           150309    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 39, 293)           0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 39, 293)           0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 39, 293)           86142     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 39, 293)           0         \n",
      "=================================================================\n",
      "Total params: 4,146,311\n",
      "Trainable params: 4,117,411\n",
      "Non-trainable params: 28,900\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "#loaded_model = load_model('saved_models/final_rnn_model6.hdf5')\n",
    "loaded_model = load_model('saved_models/resume_chatbot_save_2000.hdf5')\n",
    "\n",
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Variation - 3 Hidden LSTM Layer and 2 Dense ouput layer with higher Dropout Rate\n",
    "\n",
    "# Reshaping the input to work with a basic RNN\n",
    "tmp_x = pad(preproc_question_sentences, preproc_answer_sentences.shape[1])\n",
    "tmp_x = tmp_x.reshape((-1, preproc_answer_sentences.shape[-2]))\n",
    "\n",
    "\n",
    "input_shape =tmp_x.shape\n",
    "output_sequence_length =preproc_answer_sentences.shape[1]\n",
    "question_vocab_size =len(question_tokenizer.word_index)+1\n",
    "answer_vocab_size=len(answer_tokenizer.word_index)+1\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(question_vocab_size, embedding_vector_length,weights=[encoder_embedding_matrix], trainable= False,\n",
    "                    input_shape= input_shape[1:]))\n",
    "model.add(Bidirectional(LSTM(hidden_dim, return_sequences=True)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Bidirectional(LSTM(hidden_dim, return_sequences=True)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Bidirectional(LSTM(hidden_dim, return_sequences=True)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(answer_vocab_size))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(answer_vocab_size))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss=sparse_categorical_crossentropy,\n",
    "              optimizer=Adam(learning_rate),\n",
    "              metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "epochs =1000\n",
    "\n",
    "model.fit(tmp_x, preproc_answer_sentences, batch_size=128, epochs=epochs, validation_split=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different try Seq2Seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# New code \n",
    "question_context = Input(shape=(max_encoder_seq_length,), dtype='int32')\n",
    "encoder_Embedding = Embedding(output_dim=input_dimenstions, input_dim=num_encoder_tokens,weights=[encoder_embedding_matrix], trainable= False)#, input_length=max_encoder_seq_length)\n",
    "LSTM_encoder = LSTM(hidden_dim, kernel_initializer= 'lecun_uniform', return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "answer_context = Input(shape=(max_decoder_seq_length,), dtype='int32')\n",
    "LSTM_decoder = LSTM(hidden_dim, kernel_initializer= 'lecun_uniform', return_state=True)\n",
    "decoder_Embedding = Embedding(output_dim=input_dimenstions, input_dim=num_decoder_tokens, weights=[encoder_embedding_matrix], trainable= False, input_length=max_decoder_seq_length)\n",
    "\n",
    "#Shared_Embedding = Embedding(output_dim=input_dimenstions, input_dim=dictionary_size, weights=[embedding_matrix], input_length=maxlen_input, name='Shared')\n",
    "\n",
    "word_embedding_context = encoder_Embedding(input_context)\n",
    "context_embedding = LSTM_encoder(word_embedding_context)\n",
    "\n",
    "word_embedding_answer = decoder_Embedding(input_answer)\n",
    "answer_embedding = LSTM_decoder(word_embedding_answer)\n",
    "\n",
    "merge_layer = merge([context_embedding, answer_embedding], mode='concat', concat_axis=1)\n",
    "out = Dense(input_dimenstions, activation=\"relu\")(merge_layer)\n",
    "out = Dense(input_dimenstions, activation=\"softmax\")(out)\n",
    "\n",
    "model = Model(input=[input_context, input_answer], output = [out])\n",
    "ad = Adam(lr=0.00005) \n",
    "model.compile(loss='categorical_crossentropy', optimizer=ad)\n",
    "\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size=batch_size, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Inputs \n",
    "#tmp_x, preproc_answer_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15,)\n",
      "15\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You called `set_weights(weights)` on layer \"embedding_6\" with a  weight list of length 2707, but the layer was expecting 1 weights. Provided weights: [[ 0.          0.          0.         ...,  0.    ...",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-c0f6351a636e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[0mpreproc_french_sentences\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menglish_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     len(french_tokenizer.word_index)+1)\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[0mfinal_rnn_model6\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreproc_french_sentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-53-c0f6351a636e>\u001b[0m in \u001b[0;36mmodel_final6\u001b[1;34m(input_shape, output_sequence_length, english_vocab_size, french_vocab_size)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     model.add(Embedding(english_vocab_size, embedding_vector_length, weights=encoder_embedding_matrix,trainable= False,\n\u001b[1;32m---> 23\u001b[1;33m                         input_shape= input_shape[1:]))\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBidirectional\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36madd\u001b[1;34m(self, layer)\u001b[0m\n\u001b[0;32m    440\u001b[0m                 \u001b[1;31m# and create the node connecting the current layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m                 \u001b[1;31m# to the input layer we just created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 442\u001b[1;33m                 \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    443\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minbound_nodes\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\topology.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    581\u001b[0m                 \u001b[1;31m# Load weights that were specified at layer instantiation.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initial_weights\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initial_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m             \u001b[1;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\topology.py\u001b[0m in \u001b[0;36mset_weights\u001b[1;34m(self, weights)\u001b[0m\n\u001b[0;32m   1189\u001b[0m                              \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m                              \u001b[1;34m' weights. Provided weights: '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1191\u001b[1;33m                              str(weights)[:50] + '...')\n\u001b[0m\u001b[0;32m   1192\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1193\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: You called `set_weights(weights)` on layer \"embedding_6\" with a  weight list of length 2707, but the layer was expecting 1 weights. Provided weights: [[ 0.          0.          0.         ...,  0.    ..."
     ]
    }
   ],
   "source": [
    "\n",
    "def model_final6(input_shape, output_sequence_length, question_vocab_size, answer_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a model that incorporates embedding, encoder-decoder, and bidirectional RNN on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param question_vocab_size: Number of unique English words in the dataset\n",
    "    :param answer_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    print(input_shape[1:])\n",
    "    print(output_sequence_length)\n",
    "    \n",
    "    learning_rate = 0.01\n",
    "    hidden_dim = 256\n",
    "    embedding_vector_length = 100\n",
    "    \n",
    "    #Variation - 3 Hidden LSTM Layer and 2 Dense ouput layer with higher Dropout Rate\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(question_vocab_size, embedding_vector_length, weights=encoder_embedding_matrix,trainable= False,\n",
    "                        input_shape= input_shape[1:]))\n",
    "    model.add(Bidirectional(LSTM(hidden_dim, return_sequences=True)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Bidirectional(LSTM(hidden_dim, return_sequences=True)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Bidirectional(LSTM(hidden_dim, return_sequences=True)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(answer_vocab_size))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(answer_vocab_size))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "\n",
    "#tests.test_model_final(model_final)\n",
    "\n",
    "\n",
    "# Reshaping the input to work with a basic RNN\n",
    "tmp_x = pad(preproc_question_sentences, preproc_answer_sentences.shape[1])\n",
    "tmp_x = tmp_x.reshape((-1, preproc_answer_sentences.shape[-2]))\n",
    "\n",
    "# Train the neural network\n",
    "final_rnn_model6 = model_final6(\n",
    "    tmp_x.shape,\n",
    "    preproc_answer_sentences.shape[1],\n",
    "    len(question_tokenizer.word_index)+1,\n",
    "    len(answer_tokenizer.word_index)+1)\n",
    "final_rnn_model6.fit(tmp_x, preproc_answer_sentences, batch_size=128, epochs=500, validation_split=0.2)\n",
    "\n",
    "final_rnn_model6.save('./saved_models/final_rnn_model6_embed.hdf5')\n",
    "\n",
    "# Print prediction(s)\n",
    "print(logits_to_text(final_rnn_model6.predict(tmp_x[:1])[0], answer_tokenizer))\n",
    "\n",
    "print('Final Model Loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(logits_to_text(loaded_model.predict(tmp_x[:1])[0], answer_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-9a7881f870d4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "print(logits_to_text(model.predict(tmp_x[25:30])[0], answer_tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#preproc_answer_sentences[6]\n",
    "\n",
    "len(answer_tokenizer.word_index)+1\n",
    "#print(logits_to_text(final_rnn_model6.predict(tmp_x[20]), answer_tokenizer))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction (IMPLEMENTATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_test(raw_word, name, question_tokenizer):\n",
    "    l1 = ['won’t','won\\'t','wouldn’t','wouldn\\'t','’m', '’re', '’ve', '’ll', '’s','’d', 'n’t', '\\'m', '\\'re', '\\'ve', '\\'ll', '\\'s', '\\'d', 'can\\'t', 'n\\'t', 'B: ', 'A: ', ',', ';', '.', '?', '!', ':', '. ?', ',   .', '. ,', 'EOS', 'BOS', 'eos', 'bos']\n",
    "    l2 = ['will not','will not','would not','would not',' am', ' are', ' have', ' will', ' is', ' had', ' not', ' am', ' are', ' have', ' will', ' is', ' had', 'can not', ' not', '', '', ' ,', ' ;', ' .', ' ?', ' !', ' :', '? ', '.', ',', '', '', '', '']\n",
    "#   l3 = ['-', '_', ' *', ' /', '* ', '/ ', '\\\"', ' \\\\\"', '\\\\ ', '--', '...', '. . .']\n",
    "#    l4 = ['jeffrey','fred','benjamin','paula','walter','rachel','andy','helen','harrington','kathy','ronnie','carl','annie','cole','ike','milo','cole','rick','johnny','loretta','cornelius','claire','romeo','casey','johnson','rudy','stanzi','cosgrove','wolfi','kevin','paulie','cindy','paulie','enzo','mikey','i\\97','davis','jeffrey','norman','johnson','dolores','tom','brian','bruce','john','laurie','stella','dignan','elaine','jack','christ','george','frank','mary','amon','david','tom','joe','paul','sam','charlie','bob','marry','walter','james','jimmy','michael','rose','jim','peter','nick','eddie','johnny','jake','ted','mike','billy','louis','ed','jerry','alex','charles','tommy','bobby','betty','sid','dave','jeffrey','jeff','marty','richard','otis','gale','fred','bill','jones','smith','mickey']    \n",
    "\n",
    "    raw_word = raw_word.lower()\n",
    "    raw_word = raw_word.replace(', ' + name_of_computer, '')\n",
    "    raw_word = raw_word.replace(name_of_computer + ' ,', '')\n",
    "\n",
    "    for j, term in enumerate(l1):\n",
    "        raw_word = raw_word.replace(term,l2[j])\n",
    "        \n",
    " #   for term in l3:\n",
    " #       raw_word = raw_word.replace(term,' ')\n",
    "       \n",
    "    for j in range(30):\n",
    "        raw_word = raw_word.replace('. .', '')\n",
    "        raw_word = raw_word.replace('.  .', '')\n",
    "        raw_word = raw_word.replace('..', '')\n",
    "       \n",
    "    for j in range(5):\n",
    "        raw_word = raw_word.replace('  ', ' ')\n",
    "        \n",
    "    #if raw_word[-1] !=  '!' and raw_word[-1] != '?' and raw_word[-1] != '.' and raw_word[-2:] !=  '! ' and raw_word[-2:] != '? ' and raw_word[-2:] != '. ':\n",
    "    #    raw_word = raw_word + ' .'\n",
    "    \n",
    "    #if raw_word == ' !' or raw_word == ' ?' or raw_word == ' .' or raw_word == ' ! ' or raw_word == ' ? ' or raw_word == ' . ':\n",
    "    #    raw_word = 'what ?'\n",
    "       \n",
    "    \n",
    "    #if raw_word == '  .' or raw_word == ' .' or raw_word == '  . ':\n",
    "    #    raw_word = 'i do not want to talk about it .'\n",
    "    \n",
    "    \n",
    "    #raw_new_string = []\n",
    "    \n",
    "    #for word in nltk.tokenize.word_tokenize(raw_word):\n",
    "    #    if word not in question_tokenizer.word_index.keys():\n",
    "    #        word = name\n",
    "    #        raw_new_string.append(word)\n",
    "    #    else:\n",
    "    #        raw_new_string.append(word)\n",
    "            \n",
    "    \n",
    "    \n",
    "            #for dict_word in question_tokenizer.word_index.keys():\n",
    "            #    nlp_words = nlp(word)\n",
    "            #    dict_tokens = nlp(dict_word)\n",
    "            #    nlp_words.similarity(dict_tokens)\n",
    "    \n",
    "    \n",
    "    return raw_word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "When you are ready to submit your project, do the following steps:\n",
    "1. Ensure you pass all points on the [rubric](https://review.udacity.com/#!/rubrics/1004/view).\n",
    "2. Submit the following in a zip file.\n",
    "  - `helper.py`\n",
    "  - `machine_translation.ipynb`\n",
    "  - `machine_translation.html`\n",
    "    - You can export the notebook by navigating to **File -> Download as -> HTML (.html)**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
