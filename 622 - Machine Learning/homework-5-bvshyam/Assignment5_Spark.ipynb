{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using spark on housing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset:\n",
    "\n",
    "For this assignment, youâ€™ll make use of the California Housing data set (http://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html). This assignment is to implement the usage of pyspark and try MlLib to build a machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going use spark using two different methods\n",
    "\n",
    "1. Using spark MlLib library and using pyspark modules.\n",
    "2. Using spark-sklearn library. Build a pipeline in sklearn and run using spark gridsearch.\n",
    "\n",
    "## Option 1:\n",
    "In option 1, we are going to use pyspark native implementation functionalities. Also use MlLib library for a machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/usr/local/spark'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "   .master(\"local\") \\\n",
    "   .appName(\"Linear Regression Model\") \\\n",
    "   .getOrCreate()\n",
    "   \n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.17.0.2:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Linear Regression Model</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=Linear Regression Model>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the data\n",
    "rdd = sc.textFile('file:////data/cadata.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['  4.5260000000000000e+005  8.3252000000000006e+000  4.1000000000000000e+001  8.8000000000000000e+002  1.2900000000000000e+002  3.2200000000000000e+002  1.2600000000000000e+002  3.7880000000000003e+001 -1.2223000000000000e+002']"
      ]
     },
     "execution_count": 544,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a RDD by splitting the values and convert it to float\n",
    "\n",
    "rdd1 = rdd.map(lambda line: line.split(\"  \")[1:])\\\n",
    ".map(lambda line: (line[:7],line[7].split(' '))) \\\n",
    ".map(lambda num: [float(item) for sublist in num for item in sublist])#.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[452600.0, 8.3252, 41.0, 880.0, 129.0, 322.0, 126.0, 37.88, -122.23],\n",
       " [358500.0, 8.3014, 21.0, 7099.0, 1106.0, 2401.0, 1138.0, 37.86, -122.22],\n",
       " [352100.0, 7.2574, 52.0, 1467.0, 190.0, 496.0, 177.0, 37.85, -122.24],\n",
       " [341300.0, 5.6431, 52.0, 1274.0, 235.0, 558.0, 219.0, 37.85, -122.25],\n",
       " [342200.0, 3.8462, 52.0, 1627.0, 280.0, 565.0, 259.0, 37.85, -122.25],\n",
       " [269700.0, 4.0368, 52.0, 919.0, 213.0, 413.0, 193.0, 37.85, -122.25],\n",
       " [299200.0, 3.6591, 52.0, 2535.0, 489.0, 1094.0, 514.0, 37.84, -122.25],\n",
       " [241400.0, 3.12, 52.0, 3104.0, 687.0, 1157.0, 647.0, 37.84, -122.25],\n",
       " [226700.0, 2.0804, 42.0, 2555.0, 665.0, 1206.0, 595.0, 37.84, -122.26],\n",
       " [261100.0, 3.6912, 52.0, 3549.0, 707.0, 1551.0, 714.0, 37.84, -122.25]]"
      ]
     },
     "execution_count": 546,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the RDD to a DF and create column names\n",
    "\n",
    "df = rdd1.map(lambda line: Row(medianHouseValue=line[0], \n",
    "                              medianIncome=line[1], \n",
    "                              housingMedianAge=line[2],\n",
    "                              totalRooms=line[3],\n",
    "                              totalBedRooms=line[4],\n",
    "                              population=line[5], \n",
    "                              households=line[6],\n",
    "                              latitude=line[7],\n",
    "                              longitude=line[8])).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+--------+---------+----------------+------------+----------+-------------+----------+\n",
      "|households|housingMedianAge|latitude|longitude|medianHouseValue|medianIncome|population|totalBedRooms|totalRooms|\n",
      "+----------+----------------+--------+---------+----------------+------------+----------+-------------+----------+\n",
      "|     126.0|            41.0|   37.88|  -122.23|        452600.0|      8.3252|     322.0|        129.0|     880.0|\n",
      "|    1138.0|            21.0|   37.86|  -122.22|        358500.0|      8.3014|    2401.0|       1106.0|    7099.0|\n",
      "|     177.0|            52.0|   37.85|  -122.24|        352100.0|      7.2574|     496.0|        190.0|    1467.0|\n",
      "|     219.0|            52.0|   37.85|  -122.25|        341300.0|      5.6431|     558.0|        235.0|    1274.0|\n",
      "|     259.0|            52.0|   37.85|  -122.25|        342200.0|      3.8462|     565.0|        280.0|    1627.0|\n",
      "+----------+----------------+--------+---------+----------------+------------+----------+-------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- households: double (nullable = true)\n",
      " |-- housingMedianAge: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- medianHouseValue: double (nullable = true)\n",
      " |-- medianIncome: double (nullable = true)\n",
      " |-- population: double (nullable = true)\n",
      " |-- totalBedRooms: double (nullable = true)\n",
      " |-- totalRooms: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional Part:\n",
    "Here we are converting the dataframe columns to float. But it is already converted to float using previous commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- households: float (nullable = true)\n",
      " |-- housingMedianAge: float (nullable = true)\n",
      " |-- latitude: float (nullable = true)\n",
      " |-- longitude: float (nullable = true)\n",
      " |-- medianHouseValue: float (nullable = true)\n",
      " |-- medianIncome: float (nullable = true)\n",
      " |-- population: float (nullable = true)\n",
      " |-- totalBedRooms: float (nullable = true)\n",
      " |-- totalRooms: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Optional part\n",
    "\n",
    "# Import all from `sql.types`\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Write a custom function to convert the data type of DataFrame columns\n",
    "def convertColumn(df, names, newType):\n",
    "    for name in names: \n",
    "        df = df.withColumn(name, df[name].cast(newType))\n",
    "    return df \n",
    "\n",
    "# Assign all column names to `columns`\n",
    "columns = ['households', 'housingMedianAge', 'latitude', 'longitude', 'medianHouseValue', 'medianIncome', 'population', 'totalBedRooms', 'totalRooms']\n",
    "\n",
    "# Conver the `df` columns to `FloatType()`\n",
    "df = convertColumn(df, columns, FloatType())\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+\n",
      "|population|totalBedRooms|\n",
      "+----------+-------------+\n",
      "|     322.0|        129.0|\n",
      "|    2401.0|       1106.0|\n",
      "|     496.0|        190.0|\n",
      "|     558.0|        235.0|\n",
      "|     565.0|        280.0|\n",
      "|     413.0|        213.0|\n",
      "|    1094.0|        489.0|\n",
      "|    1157.0|        687.0|\n",
      "|    1206.0|        665.0|\n",
      "|    1551.0|        707.0|\n",
      "+----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show 10 population and totalBedRooms\n",
    "df.select('population','totalBedRooms').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+\n",
      "|housingMedianAge|count|\n",
      "+----------------+-----+\n",
      "|            52.0| 1273|\n",
      "|            51.0|   48|\n",
      "|            50.0|  136|\n",
      "|            49.0|  134|\n",
      "|            48.0|  177|\n",
      "|            47.0|  198|\n",
      "|            46.0|  245|\n",
      "|            45.0|  294|\n",
      "|            44.0|  356|\n",
      "|            43.0|  353|\n",
      "|            42.0|  368|\n",
      "|            41.0|  296|\n",
      "|            40.0|  304|\n",
      "|            39.0|  369|\n",
      "|            38.0|  394|\n",
      "|            37.0|  537|\n",
      "|            36.0|  862|\n",
      "|            35.0|  824|\n",
      "|            34.0|  689|\n",
      "|            33.0|  615|\n",
      "+----------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Groupby housingMedianAge and get the top 20 count\n",
    "\n",
    "df.groupBy(\"housingMedianAge\").count().sort(\"housingMedianAge\",ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|  medianHouseValue|\n",
      "+-------+------------------+\n",
      "|  count|             20640|\n",
      "|   mean|206855.81690891474|\n",
      "| stddev|115395.61587441359|\n",
      "|    min|           14999.0|\n",
      "|    max|          500001.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Info of medianHouseValue\n",
    "df.select('medianHouseValue').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(households=126.0, housingMedianAge=41.0, latitude=37.880001068115234, longitude=-122.2300033569336, medianHouseValue=4.526, medianIncome=8.325200080871582, population=322.0, totalBedRooms=129.0, totalRooms=880.0),\n",
       " Row(households=1138.0, housingMedianAge=21.0, latitude=37.86000061035156, longitude=-122.22000122070312, medianHouseValue=3.585, medianIncome=8.301400184631348, population=2401.0, totalBedRooms=1106.0, totalRooms=7099.0)]"
      ]
     },
     "execution_count": 554,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Change the depended variable\n",
    "# Import all from `sql.functions` \n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Adjust the values of `medianHouseValue`\n",
    "df = df.withColumn(\"medianHouseValue\", col(\"medianHouseValue\")/100000)\n",
    "\n",
    "# Show the first 2 lines of `df`\n",
    "df.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all from `sql.functions` if you haven't yet\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Divide `totalRooms` by `households`\n",
    "roomsPerHousehold = df.select(col(\"totalRooms\")/col(\"households\"))\n",
    "\n",
    "# Divide `population` by `households`\n",
    "populationPerHousehold = df.select(col(\"population\")/col(\"households\"))\n",
    "\n",
    "# Divide `totalBedRooms` by `totalRooms`\n",
    "bedroomsPerRoom = df.select(col(\"totalBedRooms\")/col(\"totalRooms\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(households=126.0, housingMedianAge=41.0, latitude=37.880001068115234, longitude=-122.2300033569336, medianHouseValue=4.526, medianIncome=8.325200080871582, population=322.0, totalBedRooms=129.0, totalRooms=880.0, roomsPerHousehold=6.984126984126984, populationPerHousehold=2.5555555555555554, bedroomsPerRoom=0.14659090909090908)"
      ]
     },
     "execution_count": 556,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the new columns to `df`\n",
    "df = df.withColumn(\"roomsPerHousehold\", col(\"totalRooms\")/col(\"households\")) \\\n",
    "   .withColumn(\"populationPerHousehold\", col(\"population\")/col(\"households\")) \\\n",
    "   .withColumn(\"bedroomsPerRoom\", col(\"totalBedRooms\")/col(\"totalRooms\"))\n",
    "   \n",
    "# Inspect the result\n",
    "df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-order and select columns\n",
    "df = df.select(\"medianHouseValue\", \n",
    "              \"totalBedRooms\", \n",
    "              \"population\", \n",
    "              \"households\", \n",
    "              \"medianIncome\", \n",
    "              \"roomsPerHousehold\", \n",
    "              \"populationPerHousehold\", \n",
    "              \"bedroomsPerRoom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import `DenseVector`\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "\n",
    "# Define the `input_data` \n",
    "input_data = df.rdd.map(lambda x: (x[0], DenseVector(x[1:])))\n",
    "\n",
    "# Replace `df` with the new DataFrame\n",
    "df1 = spark.createDataFrame(input_data, [\"label\", \"features\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+----------+----------+------------+-----------------+----------------------+-------------------+\n",
      "|medianHouseValue|totalBedRooms|population|households|medianIncome|roomsPerHousehold|populationPerHousehold|    bedroomsPerRoom|\n",
      "+----------------+-------------+----------+----------+------------+-----------------+----------------------+-------------------+\n",
      "|           4.526|        129.0|     322.0|     126.0|      8.3252|6.984126984126984|    2.5555555555555554|0.14659090909090908|\n",
      "|           3.585|       1106.0|    2401.0|    1138.0|      8.3014|6.238137082601054|     2.109841827768014|0.15579659106916466|\n",
      "+----------------+-------------+----------+----------+------------+-----------------+----------------------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label=4.526, features=DenseVector([129.0, 322.0, 126.0, 8.3252, 6.9841, 2.5556, 0.1466]), features_scaled=DenseVector([0.3062, 0.2843, 0.3296, 4.3821, 2.8228, 0.2461, 2.5264])),\n",
       " Row(label=3.585, features=DenseVector([1106.0, 2401.0, 1138.0, 8.3014, 6.2381, 2.1098, 0.1558]), features_scaled=DenseVector([2.6255, 2.1202, 2.9765, 4.3696, 2.5213, 0.2031, 2.6851]))]"
      ]
     },
     "execution_count": 560,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import `StandardScaler` \n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "# Initialize the `standardScaler`\n",
    "standardScaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\")\n",
    "\n",
    "# Fit the DataFrame to the scaler\n",
    "scaler = standardScaler.fit(df1)\n",
    "\n",
    "# Transform the data in `df` with the scaler\n",
    "scaled_df = scaler.transform(df1)\n",
    "\n",
    "# Inspect the result\n",
    "scaled_df.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "train_data, test_data = scaled_df.randomSplit([.8,.2],seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import `LinearRegression`\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Initialize `lr`\n",
    "lr = LinearRegression(labelCol=\"label\", maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the data to the model\n",
    "linearModel = lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "predicted = linearModel.transform(test_data)\n",
    "\n",
    "# Extract the predictions and the \"known\" correct labels\n",
    "predictions = predicted.select(\"prediction\").rdd.map(lambda x: x[0])\n",
    "labels = predicted.select(\"label\").rdd.map(lambda x: x[0])\n",
    "\n",
    "# Zip `predictions` and `labels` into a list\n",
    "predictionAndLabel = predictions.zip(labels).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.1340115638008952, 0.14999),\n",
       " (1.4485018834650096, 0.14999),\n",
       " (1.5713396046425587, 0.14999),\n",
       " (1.7496542762527307, 0.283),\n",
       " (1.2438468929500472, 0.366),\n",
       " (1.277345544168797, 0.367),\n",
       " (1.5895709536087121, 0.375),\n",
       " (1.5270755381601813, 0.388),\n",
       " (1.6941214255677641, 0.4),\n",
       " (1.9389021305505283, 0.404)]"
      ]
     },
     "execution_count": 564,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print out first 5 instances of `predictionAndLabel` \n",
    "predictionAndLabel[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([0.0, 0.0, 0.0, 0.2796, 0.0, 0.0, 0.0])"
      ]
     },
     "execution_count": 565,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Coefficients for the model\n",
    "linearModel.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9841344205626824"
      ]
     },
     "execution_count": 566,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Intercept for the model\n",
    "linearModel.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8765335684459216"
      ]
     },
     "execution_count": 567,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the RMSE\n",
    "linearModel.summary.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42282227755911483"
      ]
     },
     "execution_count": 568,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the R2\n",
    "linearModel.summary.r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Option 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second option, we will integrate sklearn and spark usig spark_sklearn package. This package provides grid search option which can be executed parallely in spark instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "    1. Create a sklearn pipeline\n",
    "    2. Perfrom data cleaning and add new features\n",
    "    3. Create a gridsearchcv object from spark_sklearn\n",
    "    4. Fit a model using gridsearchcv, so the models will get executed in spark cluster\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV as GV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from spark_sklearn import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the data\n",
    "pd_df = pd.read_csv('./data/cadata.txt',sep=\" \", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>452600.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.3252</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>880.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>129.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>322.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>126.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>358500.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.3014</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7099.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1106.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>352100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.2574</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1467.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>190.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>496.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>177.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>341300.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.6431</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1274.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>235.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>558.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>219.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>342200.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.8462</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1627.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>280.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>565.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>259.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0   1         2   3       4   5     6   7       8   9       10  11      12  \\\n",
       "0 NaN NaN  452600.0 NaN  8.3252 NaN  41.0 NaN   880.0 NaN   129.0 NaN   322.0   \n",
       "1 NaN NaN  358500.0 NaN  8.3014 NaN  21.0 NaN  7099.0 NaN  1106.0 NaN  2401.0   \n",
       "2 NaN NaN  352100.0 NaN  7.2574 NaN  52.0 NaN  1467.0 NaN   190.0 NaN   496.0   \n",
       "3 NaN NaN  341300.0 NaN  5.6431 NaN  52.0 NaN  1274.0 NaN   235.0 NaN   558.0   \n",
       "4 NaN NaN  342200.0 NaN  3.8462 NaN  52.0 NaN  1627.0 NaN   280.0 NaN   565.0   \n",
       "\n",
       "   13      14  15     16      17  \n",
       "0 NaN   126.0 NaN  37.88 -122.23  \n",
       "1 NaN  1138.0 NaN  37.86 -122.22  \n",
       "2 NaN   177.0 NaN  37.85 -122.24  \n",
       "3 NaN   219.0 NaN  37.85 -122.25  \n",
       "4 NaN   259.0 NaN  37.85 -122.25  "
      ]
     },
     "execution_count": 585,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Cleaning\n",
    "pd_df.dropna(axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add the column names\n",
    "pd_df.columns=['medianHouseValue', 'medianIncome', 'housingMedianAge', 'totalRooms', 'totalBedRooms', 'population','households', 'latitude', 'longitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_df[\"medianHouseValue\"] = pd_df[\"medianHouseValue\"]/100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split train and test dataset\n",
    "train_X,test_X,train_y,test_y = train_test_split(pd_df.drop('medianHouseValue',axis=1),\n",
    "                                                pd_df[['medianHouseValue']],\n",
    "                                                test_size =0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a class for item selector\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"This class selects the features which are mentioned in the pipeline\"\"\"\n",
    "    def __init__(self,cols):\n",
    "        self.cols = cols\n",
    "        \n",
    "    def fit(self, x, y=None):\n",
    "        #print(\"infit\")\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        #print(\"intransform\")\n",
    "        col_list = []\n",
    "        for c in self.cols:\n",
    "            col_list.append(X[:, c:c+1])\n",
    "\n",
    "        return np.concatenate(col_list,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function transformer object to add new features\n",
    "\n",
    "def roomsperhousehold(X):\n",
    "    \"\"\"This function is used in pipeline. \"\"\"\n",
    "    X['roomsPerHousehold'] = X['totalRooms']/X['households']\n",
    "    X['populationPerHousehold'] = X['population']/X['households']\n",
    "    X['bedroomsPerRoom'] = X['totalBedRooms']/X['totalRooms']\n",
    "\n",
    "    #print(X.head())\n",
    "    return(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function transformer function\n",
    "roomsperhousehold_ft = FunctionTransformer(roomsperhousehold, validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [],
   "source": [
    "#steps for pipeline\n",
    "steps = [('roomsperhousehold_nm',roomsperhousehold_ft),\n",
    "         ('scalar',StandardScaler()),\n",
    "         ('selection',ItemSelector(cols =(0,3,4,5,8,9,10))),\n",
    "         ('model',LinearRegression())\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add all the models \n",
    "param = [{'model':[LinearRegression()],'model__normalize':[True,False]},\n",
    "         {'model':[GradientBoostingRegressor()],'model__learning_rate':[0.01,0.001],\n",
    "         'model__n_estimators':[100,200],'model__verbose':[1]}\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a pipeline object\n",
    "pl = Pipeline(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spark version of Gridsearch object\n",
    "clf = GridSearchCV(sc,pl,param_grid=param,cv=5,n_jobs=-1)\n",
    "\n",
    "#python version of Gridsearch object\n",
    "#clf = GV(pl,param_grid=param,cv=5,n_jobs=-1) # 50 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py:526: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3175            2.64s\n",
      "         2           1.3036            2.61s\n",
      "         3           1.2900            2.60s\n",
      "         4           1.2766            2.56s\n",
      "         5           1.2636            2.58s\n",
      "         6           1.2507            2.56s\n",
      "         7           1.2381            2.54s\n",
      "         8           1.2258            2.52s\n",
      "         9           1.2138            2.49s\n",
      "        10           1.2017            2.48s\n",
      "        20           1.0929            2.32s\n",
      "        30           1.0014            2.16s\n",
      "        40           0.9251            2.01s\n",
      "        50           0.8611            1.91s\n",
      "        60           0.8075            1.79s\n",
      "        70           0.7624            1.69s\n",
      "        80           0.7243            1.58s\n",
      "        90           0.6921            1.44s\n",
      "       100           0.6650            1.30s\n",
      "       200           0.5333            0.00s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=Pipeline(steps=[('roomsperhousehold_nm', FunctionTransformer(accept_sparse=False,\n",
       "          func=<function roomsperhousehold at 0x7fe6c3fb4048>,\n",
       "          inv_kw_args=None, inverse_func=None, kw_args=None, pass_y=False,\n",
       "          validate=False)), ('scalar', StandardScaler(copy=True, with_mean=True, with_std=True)), ('selection', ItemSelector(cols=(0, 3, 4, 5, 8, 9, 10))), ('model', LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False))]),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid=[{'model': [LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)], 'model__normalize': [True, False]}, {'model': [GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
       "  ...], 'model__learning_rate': [0.01, 0.001], 'model__n_estimators': [100, 200], 'model__verbose': [1]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       sc=<SparkContext master=local appName=Linear Regression Model>,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 597,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(train_X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict on test dataset\n",
    "predict_y = clf.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.60180947036351884"
      ]
     },
     "execution_count": 599,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#R2 score of test dataset\n",
    "r2_score(test_y,predict_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.01, loss='ls', max_depth=3, max_features=None,\n",
       "              max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=200,\n",
       "              presort='auto', random_state=None, subsample=1.0, verbose=1,\n",
       "              warm_start=False),\n",
       " 'model__learning_rate': 0.01,\n",
       " 'model__n_estimators': 200,\n",
       " 'model__verbose': 1}"
      ]
     },
     "execution_count": 600,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Below mentioned is the best classifier and its model parameters\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary:\n",
    "\n",
    "Below are the findings of this assignment.\n",
    "\n",
    "1. There are multiple options to use spark as part of execution.\n",
    "2. Using native spark and its MlLib(Option 1 in this assignment) is the fastest of other options. For this assignment the whole execution took around 15 sec. \n",
    "3. But in option 1, we have not performed Gridsearch or cross validation. And the result which we got is not the best result.\n",
    "4. In option 2, we created an sklearn-pipeline object which is a very nice way to organize the code.\n",
    "5. Gridsearch option provided by spark-sklearn is used to run multiple models parallely in spark without MlLib.\n",
    "6. Gridsearch using spark-sklearn provided results in 45 sec. Gridsearch from sklearn provided results in 50 sec. \n",
    "7. But option 2, provided higher results than option 1. Improvement was aroudn 15%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
